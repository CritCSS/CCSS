[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Critical Computational Social Sciences",
    "section": "",
    "text": "Quantitative analysis can be a very powerful tool to understand systemic phenomena. By translating similar experiences shared by large groups of people into data, we can identify repeating patterns. This allows us to contextualize our personal experiences and address issues from a systemic perspective, leading to more effective solutions and positive change for ourselves and the communities we are a part of.\nHowever, data are not neutral or objective. They are the products of unequal social relations, and understanding this context is essential for conducting accurate and ethical analysis.\nIt is important to critically analyze data in order to avoid replicating the biases that may be inherent in the data. Standard practices in data science may serve to reinforce these existing inequalities (D’ignazio and Klein 2023).\n\n\n\nThe demographics of data science, as well as related occupations such as software engineering and artificial intelligence research, are not representative of the population as a whole. There is an over-representation of privileged global north-based White men.\nIt is important to keep in mind that science is shaped by the unique perspectives of individual scientists, and therefore researchers’ subjectivities and biases can influence the way data sets are built and later analyzed. Therefore, it is necessary to be transparent about the context in which research is conducted as well as potential sources of bias, thus acknowledging the partiality of our perspectives. It is also important to promote a more representative workforce in data science.\n\n\n\nFor the above, we believe that a critical perspective is key to tackle systemic biases that hinder the tools of data analysis to be tools for positive societal change. For this reason, these materials aim to develop a critical mindset toward the tools as they are presented. For developing this goal, our materials mix a critical analysis of the potential harms associated with the tools introduced in each material, together with the potentialities they bring for the study of inequalities, and wherever possible for the pedagogy of the courses, the use of real world data sets.\n\n\n\nThe materials consists of 20 building blocks, grouped into six categories\n\n\n\n\nEach block consists of four files that aim to develop the theoretical and practical content of the course\n\n\n\n\n\nDepending on the goals of the course, the students’ profile, and the number of sessions, different syllabuses could be built.\nEach block can be taught in one or two sessions. The suggested division is between the theoretical and practical parts, although this might not apply to every block.\nBelow are some examples of possible courses, but many combinations could work!",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#why-should-data-science-be-critical",
    "href": "index.html#why-should-data-science-be-critical",
    "title": "Critical Computational Social Sciences",
    "section": "",
    "text": "Quantitative analysis can be a very powerful tool to understand systemic phenomena. By translating similar experiences shared by large groups of people into data, we can identify repeating patterns. This allows us to contextualize our personal experiences and address issues from a systemic perspective, leading to more effective solutions and positive change for ourselves and the communities we are a part of.\nHowever, data are not neutral or objective. They are the products of unequal social relations, and understanding this context is essential for conducting accurate and ethical analysis.\nIt is important to critically analyze data in order to avoid replicating the biases that may be inherent in the data. Standard practices in data science may serve to reinforce these existing inequalities (D’ignazio and Klein 2023).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#biases-and-objectivity",
    "href": "index.html#biases-and-objectivity",
    "title": "Critical Computational Social Sciences",
    "section": "",
    "text": "The demographics of data science, as well as related occupations such as software engineering and artificial intelligence research, are not representative of the population as a whole. There is an over-representation of privileged global north-based White men.\nIt is important to keep in mind that science is shaped by the unique perspectives of individual scientists, and therefore researchers’ subjectivities and biases can influence the way data sets are built and later analyzed. Therefore, it is necessary to be transparent about the context in which research is conducted as well as potential sources of bias, thus acknowledging the partiality of our perspectives. It is also important to promote a more representative workforce in data science.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#critical-pedagogies-for-critical-thinkers",
    "href": "index.html#critical-pedagogies-for-critical-thinkers",
    "title": "Critical Computational Social Sciences",
    "section": "",
    "text": "For the above, we believe that a critical perspective is key to tackle systemic biases that hinder the tools of data analysis to be tools for positive societal change. For this reason, these materials aim to develop a critical mindset toward the tools as they are presented. For developing this goal, our materials mix a critical analysis of the potential harms associated with the tools introduced in each material, together with the potentialities they bring for the study of inequalities, and wherever possible for the pedagogy of the courses, the use of real world data sets.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#building-blocks",
    "href": "index.html#building-blocks",
    "title": "Critical Computational Social Sciences",
    "section": "",
    "text": "The materials consists of 20 building blocks, grouped into six categories",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#materials-structure",
    "href": "index.html#materials-structure",
    "title": "Critical Computational Social Sciences",
    "section": "",
    "text": "Each block consists of four files that aim to develop the theoretical and practical content of the course",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#flexible-syllabus",
    "href": "index.html#flexible-syllabus",
    "title": "Critical Computational Social Sciences",
    "section": "",
    "text": "Depending on the goals of the course, the students’ profile, and the number of sessions, different syllabuses could be built.\nEach block can be taught in one or two sessions. The suggested division is between the theoretical and practical parts, although this might not apply to every block.\nBelow are some examples of possible courses, but many combinations could work!",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Setup.html",
    "href": "Setup.html",
    "title": "Setup",
    "section": "",
    "text": "To be able to work with these materials, you will need:\n\n\n\n\n\n\nCreate a github.com account\nsetup git in your computer\n\n\n\n\nAlso, the following packages are needed for many of the materials:\n\ninstall.packages('tidyverse')\n\nEach materials block use different packages, so be sure to have everything installed beforehand",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "Setup.html#class-notes",
    "href": "Setup.html#class-notes",
    "title": "Setup",
    "section": "class notes",
    "text": "class notes\nThe class notes are built in quarto markdown, which allows for quick changes in the type of outputs. this can be made through a quick modification of the YALM.\nEach class note comes with a YALM header as follows:\n---\ntitle: \"Introduction I\"\nrunningheader: \"class notes\" # only for pdf output\nsubtitle: \"class notes\" # only for html output\nformat: html\n#format: pdf\n# format: \n#   revealjs:\n#     scrollable: true\n#     smaller: true\neditor: visual\nbibliography: references.bib\n---\n\nby default, this creates an html output.\nIf you prefer to work with slides, you need to comment format: htmland un-comment the following:\n\n# format: \n#   revealjs:\n#     scrollable: true\n#     smaller: true\n\nTo build a pdf, which can be a good way to build a compilation of all materials for printing, un-comment the format: pdf instead.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Materials",
    "section": "",
    "text": "As the goal is to compile a selection of materials into a course, the most convenient is to bulk download all the materials from the repository and select the blocks from the folder.\nAlternatively, you can check the class notes directly, and download the materials of each block:",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#bulk-download",
    "href": "materials.html#bulk-download",
    "title": "Materials",
    "section": "",
    "text": "As the goal is to compile a selection of materials into a course, the most convenient is to bulk download all the materials from the repository and select the blocks from the folder.\nAlternatively, you can check the class notes directly, and download the materials of each block:",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#introduction-i",
    "href": "materials.html#introduction-i",
    "title": "Materials",
    "section": "Introduction (I)",
    "text": "Introduction (I)\ncontent: - Introduction to Critical Quant - Introduction to R\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#introduction-ii",
    "href": "materials.html#introduction-ii",
    "title": "Materials",
    "section": "Introduction (II)",
    "text": "Introduction (II)\ncontent: - Introduction to Critical Quant - Introduction to R - Version control and Documentation\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#version-control",
    "href": "materials.html#version-control",
    "title": "Materials",
    "section": "Version Control",
    "text": "Version Control\ncontent: - GIT - Rmarkdown\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#descriptive-statistics",
    "href": "materials.html#descriptive-statistics",
    "title": "Materials",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\ncontent: - Probability and Statistics overview - Tests, correlations, confidence intervals and p-values\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#linear-regression",
    "href": "materials.html#linear-regression",
    "title": "Materials",
    "section": "Linear Regression",
    "text": "Linear Regression\ncontent: - Simple and multiple linear regression - Model assumptions and coefficient interpretation\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#logistic-regression",
    "href": "materials.html#logistic-regression",
    "title": "Materials",
    "section": "Logistic Regression",
    "text": "Logistic Regression\ncontent: - logistic function - generalization: multinomial logistic regression - model assumptions\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#shiny-apps",
    "href": "materials.html#shiny-apps",
    "title": "Materials",
    "section": "Shiny Apps",
    "text": "Shiny Apps\ncontent: - server and UI - interactive visualizations with ggplotly - note: this should be tought after 8 (tidyverse) and 9 (data storytelling)\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#tidyverse",
    "href": "materials.html#tidyverse",
    "title": "Materials",
    "section": "Tidyverse",
    "text": "Tidyverse\ncontent: - data wrangling - joins - Pivots - functions\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#data-storytelling",
    "href": "materials.html#data-storytelling",
    "title": "Materials",
    "section": "Data Storytelling",
    "text": "Data Storytelling\ncontent: - grammar of graphics - types of figures - multiple aesthetic attributes\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#tidy-dataviz",
    "href": "materials.html#tidy-dataviz",
    "title": "Materials",
    "section": "Tidy-dataviz",
    "text": "Tidy-dataviz\ncontent: - This material is a mix of 8 (tidyverse) and 9 (dataviz), for a fast-paced course\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#web-scraping",
    "href": "materials.html#web-scraping",
    "title": "Materials",
    "section": "Web Scraping",
    "text": "Web Scraping\ncontent: - html - Extracting data - API\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#preprocessing",
    "href": "materials.html#preprocessing",
    "title": "Materials",
    "section": "Preprocessing",
    "text": "Preprocessing\n“normalization imputation. Categorical, continous”\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#supervised-learning",
    "href": "materials.html#supervised-learning",
    "title": "Materials",
    "section": "Supervised Learning",
    "text": "Supervised Learning\nNon-parametric classification methods. KNN.\nParametric methods. Bayes theorem for classification\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#unsupervised-learning",
    "href": "materials.html#unsupervised-learning",
    "title": "Materials",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\nClustering: K-means, hierarchical clustering\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#evaluation",
    "href": "materials.html#evaluation",
    "title": "Materials",
    "section": "Evaluation",
    "text": "Evaluation\nTrain test overfitting metrics: precision, recall, F1\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#text-mining",
    "href": "materials.html#text-mining",
    "title": "Materials",
    "section": "Text Mining",
    "text": "Text Mining\n“regex text processing Quantitative representation of text”\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#topic-modeling",
    "href": "materials.html#topic-modeling",
    "title": "Materials",
    "section": "Topic Modeling",
    "text": "Topic Modeling\nTopic modeling, LDA, BERTOPIC\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#embeddings",
    "href": "materials.html#embeddings",
    "title": "Materials",
    "section": "Embeddings",
    "text": "Embeddings\nWord Embeddings Pre-trained language models BERT\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#deep-learning",
    "href": "materials.html#deep-learning",
    "title": "Materials",
    "section": "Deep Learning",
    "text": "Deep Learning\nFCNN, Optimization, CNN\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials.html#social-network-analysis",
    "href": "materials.html#social-network-analysis",
    "title": "Materials",
    "section": "Social Network Analysis",
    "text": "Social Network Analysis\nElements of a network, types of networks, Networks representation an metrics, social networks\n\nClass notes\nFull Materials",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "materials/16_text_mining/1_class_notes.html",
    "href": "materials/16_text_mining/1_class_notes.html",
    "title": "Text Mining",
    "section": "",
    "text": "Data is often associated with rigid structures: neat rows and columns forming orderly tables. Each row represents an observation, while columns contain variables with information about these observations, encoded as either continuous or categorical data. Continuous variables take numerical form, while categorical ones present as textual data. Within this schema, textual data tends to be concise, often referencing closed or open categories.\nHowever, Artificial Intelligence (AI) houses a specialized field that transcends this conventional view, considering human language as data itself. Natural Language Processing (NLP) sits at the intersection of computer science and linguistics, aiming to imbue computers with the ability to mimic human comprehension of text. Its goal is to enable machines to grasp the essence of various textual forms such as news articles, social media posts, and more. The inherent challenge lies in the fact that such information primarily exists as unstructured data. While we effortlessly discern content and information through reading, a computer initially encounters it as a string of characters. NLP seeks out patterns within this unstructured data to facilitate text interpretation.\nSome examples of useful NLP applications are:\n\nSentiment analysis\nText translation\nChat bots\nText classification (e.g., detecting hate speech)"
  },
  {
    "objectID": "materials/16_text_mining/1_class_notes.html#zipfs-law",
    "href": "materials/16_text_mining/1_class_notes.html#zipfs-law",
    "title": "Text Mining",
    "section": "Zipf’s Law",
    "text": "Zipf’s Law\nThis previous visualization depicts the frequency of the 96 most common terms across documents. The sheer number of features is readily apparent, as evidenced by the high dimensionality of the matrices. Notably, certain words shine brighter due to their significantly higher frequency compared to others. This hints at Zipf’s Law from quantitative linguistics, where a word’s frequency inversely correlates with its ranking in the frequency table.\nFor example, given the word frequency and rank of the words in Jane Austen books:\n\n\n# A tibble: 40,379 × 4\n   book              word      n  rank\n   &lt;fct&gt;             &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n 1 Mansfield Park    the    6206     1\n 2 Mansfield Park    to     5475     2\n 3 Mansfield Park    and    5438     3\n 4 Emma              to     5239     4\n 5 Emma              the    5201     5\n 6 Emma              and    4896     6\n 7 Mansfield Park    of     4778     7\n 8 Pride & Prejudice the    4331     8\n 9 Emma              of     4291     9\n10 Pride & Prejudice to     4162    10\n# ℹ 40,369 more rows\n\n\nVisualizing Zipf’s Law involves plotting word rank on the x-axis and term frequency on the y-axis, both on logarithmic scales. This reveals an intriguing pattern: an inversely proportional relationship between word rank and frequency. In other words, words with higher ranks appear less frequently, while lower-ranked words show greater frequency."
  },
  {
    "objectID": "materials/16_text_mining/1_class_notes.html#stop-words",
    "href": "materials/16_text_mining/1_class_notes.html#stop-words",
    "title": "Text Mining",
    "section": "Stop words",
    "text": "Stop words\nZipf’s Law and frequency visualizations reveal a key concept: certain overly common words offer little meaningful information. These “stop words” are frequently removed during text preprocessing before analysis. Conversely, extremely infrequent words also hold limited value and often get excluded. This selective removal improves the accuracy and relevance of subsequent analyses.\nThere are three main types of stop words (Silge and Hvitfeldt, n.d.b):\n\nGlobal stop words: These have minimal meaning across most contexts and languages. Think of English words like “the”, “of” and “and.” These act as connectors, not content carriers.\nSubject-specific stop words: These lack meaning within a specific domain. Analyzing U.S. education, words like “high”, “school”, or “student” might not help differentiate institutions. They could become stop words for such an analysis, often requiring manual curation.\nDocument-level stop words: These provide little to no information within a specific document. They’re challenging to identify and offer limited benefit for tasks like regression or classification, even if discovered."
  },
  {
    "objectID": "materials/16_text_mining/1_class_notes.html#stemming",
    "href": "materials/16_text_mining/1_class_notes.html#stemming",
    "title": "Text Mining",
    "section": "Stemming",
    "text": "Stemming\nOne challenge in text analysis is dealing with word variations. Consider the word “change”. It might appear as “change”, “changing”, “changed” or “changes”. While each gets counted as a separate feature in Bag of Words, we often want to treat them as equivalent. This is the idea behind stemming, the process of identifying the base word (or stem) for a data set of words (Silge and Hvitfeldt, n.d.c).\nDifferent stemming algorithms use various rules to remove prefixes and suffixes, grouping words with the same stem. In our “change” example, all four variants might be reduced to “chang”. It’s crucial to remember that stemming is a heuristic approach, not always producing a proper linguistic root but aiming to condense variations of a word to a common form."
  },
  {
    "objectID": "materials/16_text_mining/1_class_notes.html#lemmatization",
    "href": "materials/16_text_mining/1_class_notes.html#lemmatization",
    "title": "Text Mining",
    "section": "Lemmatization",
    "text": "Lemmatization\nAnother method for reducing words to their base form is lemmatization (“Stemming and Lemmatization,” n.d.). Unlike stemming, which applies heuristic rules, lemmatization considers the meaning and context of words.\nIn lemmatization, words are transformed into their lemma, which is the canonical or dictionary form. This process involves using vocabulary analysis and morphological analysis of words to return their base or dictionary form (known as the lemma) by considering the part of speech and the context of the word. For example, the words “changing”, “changed”, “changes” and “change” would all be transformed into “change”. This approach ensures that the transformed words belong to the language and carry real meaning. However, lemmatization requires detailed dictionaries2 and linguistic knowledge compared to stemming, making it more accurate but computationally more expensive due to the complexity of linguistic analysis involved.\nIt’s important to note that both lemmatization and stemming aim to reduce the feature space of the dataset, similar to preprocessing steps in machine learning. However, it’s not always necessary to perform stemming or lemmatization. These preprocessing steps should be carefully considered, taking into account domain knowledge of the problem being addressed."
  },
  {
    "objectID": "materials/16_text_mining/1_class_notes.html#footnotes",
    "href": "materials/16_text_mining/1_class_notes.html#footnotes",
    "title": "Text Mining",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn n-gram refers to a contiguous sequence of n items from a given sample of text or speech. These items can be characters, syllables, words, or even larger elements like phrases or sentences, depending on the context. The “n” in n-gram represents the number of items in the sequence.↩︎\nWordnet is a good dataset source to perform lemmatization of english words.↩︎\nNote that the numerator indicates how often two tokens are observed together, whereas the denominator indicates how often they are expected to co-occur assuming independence.↩︎"
  },
  {
    "objectID": "materials/18_embeddings/1_class_notes.html",
    "href": "materials/18_embeddings/1_class_notes.html",
    "title": "Word embeddings",
    "section": "",
    "text": "Textual information is a type of unstructured data. This means that we must give it some kind of structure to work with it.\n\n\nA classic approach to structure textual information is building a Bag of Words (BoW). For this we build a Term-Frequency Matrix (TFM), where:\n\neach row is a document from the corpus and each column is a term from the vocabulary\neach cell denotes the appearance of a word in a document. There are different types of counting: from a binary (it appears or not), the frequency, to the normalized frequency, and the Term-Frequency Inverse Document Frequency (TF-IDF), among others.\nWe can now think of each document as a vector over the vocabulary, and vice-versa each term is a vector over the documents of the corpus.\n\nThis classic approach for text mining allowed to use formal methods to automatically study large corpuses, but it has some limitations:\n\nIt does not preserve the order of language, the structure and sequential order of words and n-grams are lost\nit suffers from high dimensionality (the number of columns is the number of unique words in the vocabulary, a very high number, around ~10,000 for a medium sized corpus), and\nsince each text will only uses a subset of words, most cells in TFM are zero; this is called sparsity\nHigh dimensionality and sparsity have a high computational burden and reduce the quality of results for many automatic techniques."
  },
  {
    "objectID": "materials/18_embeddings/1_class_notes.html#bag-of-words-model",
    "href": "materials/18_embeddings/1_class_notes.html#bag-of-words-model",
    "title": "Word embeddings",
    "section": "",
    "text": "A classic approach to structure textual information is building a Bag of Words (BoW). For this we build a Term-Frequency Matrix (TFM), where:\n\neach row is a document from the corpus and each column is a term from the vocabulary\neach cell denotes the appearance of a word in a document. There are different types of counting: from a binary (it appears or not), the frequency, to the normalized frequency, and the Term-Frequency Inverse Document Frequency (TF-IDF), among others.\nWe can now think of each document as a vector over the vocabulary, and vice-versa each term is a vector over the documents of the corpus.\n\nThis classic approach for text mining allowed to use formal methods to automatically study large corpuses, but it has some limitations:\n\nIt does not preserve the order of language, the structure and sequential order of words and n-grams are lost\nit suffers from high dimensionality (the number of columns is the number of unique words in the vocabulary, a very high number, around ~10,000 for a medium sized corpus), and\nsince each text will only uses a subset of words, most cells in TFM are zero; this is called sparsity\nHigh dimensionality and sparsity have a high computational burden and reduce the quality of results for many automatic techniques."
  },
  {
    "objectID": "materials/18_embeddings/1_class_notes.html#word2vec",
    "href": "materials/18_embeddings/1_class_notes.html#word2vec",
    "title": "Word embeddings",
    "section": "Word2Vec",
    "text": "Word2Vec\nW2V is one of the first methods for word embeddings. It was developed by Google in 2013(Mikolov, Chen, et al. 2013) (Mikolov, Sutskever, et al. 2013).\nHow does word2vec works?\n\nEach word in the vocabulary is represented as a one-hot encoded vector3.\nIt uses a shallow neural network with either a continuous bag of words (CBOW) or skip-gram architecture.\n\nIn CBOW, the model predicts the target word based on the context words within a window around the target word.\nIn skip-gram, the model predicts context words given a target word.\n\nOnce the model is trained, the weights of the hidden layer (between the input and output layers) are used as word embeddings."
  },
  {
    "objectID": "materials/18_embeddings/1_class_notes.html#glove",
    "href": "materials/18_embeddings/1_class_notes.html#glove",
    "title": "Word embeddings",
    "section": "GloVe",
    "text": "GloVe\nGloVe(Pennington, Socher, and Manning 2014) differs from Word2Vec in its approach to learning word embeddings.\n\nThe main difference with word2vec is that it starts by constructing a co-occurrence matrix from the corpus.\nThis matrix captures how frequently words co-occur within a context window.\nThe context can be defined based on words within a fixed window size or using other contextual information like sentence boundaries."
  },
  {
    "objectID": "materials/18_embeddings/1_class_notes.html#fasttext",
    "href": "materials/18_embeddings/1_class_notes.html#fasttext",
    "title": "Word embeddings",
    "section": "FastText",
    "text": "FastText\nFastText (Bojanowski et al. 2017) (Joulin et al. 2016) is an extension of the Word2Vec model developed by Facebook AI Research. It shares similarities with Word2Vec but introduces several innovations, particularly in how it represents words and handles out-of-vocabulary words. In summary:\n\nFastText represents words as a bag of character n-grams (subword units). This allows FastText to capture morphological information and handle out-of-vocabulary words efficiently.\nThat is why it can generate embeddings for out-of-vocabulary words by summing the embeddings of their constituent character n-grams. This makes FastText more robust to rare and unseen words compared to traditional word embedding models."
  },
  {
    "objectID": "materials/18_embeddings/1_class_notes.html#footnotes",
    "href": "materials/18_embeddings/1_class_notes.html#footnotes",
    "title": "Word embeddings",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInterestingly, the assumption of independence was once a big jump forward for computational linguistics, and allows for the implementation of models such as Naive Bayes. Lifting this assumption with word embeddings is what more recently allowed for another big step in the field.\n↩︎\nThese types of analogy tasks are often used to evaluate the performance of word embeddings. There is a series of relatively common questions used to calculate the accuracy of each embedding model. Thus, for example, questions like “What is to Greece what Paris is to France?” are used to compare the embedding’s “response” to the real response. (Mikolov, Chen, et al. 2013) used about 8,869 semantic questions and about 10,675 syntactic questions.↩︎\nThis means that each word is represented as a vector where all elements are zero except for one element which represents the index of that word in the vocabulary↩︎\nFor example, in (Pennington, Socher, and Manning 2014), the original paper of GloVe, one of the most commonly used embedding models, different trial models were trained on the following corpora: 2010 Wikipedia dump (1 billion tokens), 2014 Wikipedia dump (1.6 billion tokens), Gigaword 5 (4.3 billion tokens). Then, they combine Gigaword 5 and Wikipedia 2014. This often entails relatively long training times.↩︎\nThey use the Google NGram dataset, containing around a million books, and Google News, which contains a large number of news articles↩︎"
  },
  {
    "objectID": "materials/5_linear_regression/data/co2_econ/metadata.html",
    "href": "materials/5_linear_regression/data/co2_econ/metadata.html",
    "title": "",
    "section": "",
    "text": "sources: https://cdiac.ess-dive.lbl.gov/ https://www.gapminder.org/data/documentation/gd001/ https://data.worldbank.org/indicator/SI.POV.UMIC.GP https://data.worldbank.org/indicator/NV.AGR.TOTL.ZS https://docs.google.com/spreadsheets/d/1OYxxRgJ0N_3TxhdvhNdrnPI2I9CUPOngZpGGKlFhL6A/edit#gid=501532268 https://docs.google.com/spreadsheets/d/1d0noZrwAWxNBTDSfDgG06_aLGWUz4R6fgDhRaUZbDzE/edit#gid=501532268 https://docs.google.com/spreadsheets/d/1MCXMf9TekU6T1Q31_laZrfBmIUI2CVYRXB9IBXrc7Tc/edit#gid=501532268 https://data.worldbank.org/indicator/SE.ENR.PRSC.FM.ZS https://data.worldbank.org/indicator/NV.IND.TOTL.ZS https://www.gapminder.org/data/documentation/gd004/"
  },
  {
    "objectID": "materials/13_supervised_learning/1_class_notes.html",
    "href": "materials/13_supervised_learning/1_class_notes.html",
    "title": "Supervised Learning I",
    "section": "",
    "text": "Machine learning is a subfield of artificial intelligence (AI) that involves the development of algorithms and models that enable computer systems to automatically learn from data without being explicitly programmed. The goal of machine learning is to develop systems that can identify patterns, make predictions or decisions, and improve their performance over time through experience. In other words, it’s a way of making a computer “learn” a set of rules and patterns from data. There are different kinds of problems we can approach and resolve with machine learning:\n\nPredict whether a patient, hospitalized due to a heart attack, could have a second heart attack. The prediction can be based on demographic, diet and clinical measurements for that patient.\nPredict the price of a stock in 6 months from now, on the basis of company performance measures and economic data.\nIdentify handwritten numbers and words in a picture.\nPredict salary based on demographic and labor variables.\nClassify texts according to the sentiments they express (positive or negative).\n\nThe challenges we mentioned so far belong to the subgroup of supervised learning machine learning tools. We call it supervised because it is characterized by training algorithms on labeled datasets. In a labeled dataset, each input data point is paired with its corresponding correct output. Throughout the training process, the algorithm acquires an understanding of the relationship between the input data and the desired output. The objective of supervised learning is to assimilate a mapping from input data to output labels. This enables the algorithm, when faced with novel, unseen data, to predict the accurate output by leveraging the learned patterns derived from the training dataset.\nVariables in supervised learning problems can be classified as input features (also called X, predictors, features) which will be used to predict an output variable (also called Y, response or dependent variable).\nWhile predicting specific outcomes is a powerful result that can come from machine learning methods, it is not the sole focus. We may also aim to understand the relationship between features and the response variable. This involves identifying crucial predictors among a vast array of variables, discerning the relationship between the response variable and each predictor, and elucidating the form that relationship takes.\nDepending on the type of variable Y, supervised learning problems can be divided into two subgroups:\n\nClassification problems: This occurs when Y is categorical (for example, when predicting the employment status of a person). The response provides the probability that the observation belongs to a particular class.\nRegression problems: This arises when Y is numerical (for example, when predicting the salary of a person). The response predicts a quantity.\n\nWe assume there is some relationship between our predictors and the response variable, which can be written in the general form:\n\\[\nY = f(X)+ ϵ\n\\]\nWhere \\(f\\) is a function that represents the systematic information that \\(X\\) provides about \\(Y\\), and \\(ϵ\\) is a random error term. This function is generally unknown, and one must estimate it based on the observed points. Machine learning can be thought as a set of approaches to estimate \\(f\\). In a way, we can think of these methods as a sophistication of linear and logistic regressions, since we can model complex, nonlinear relationships between the predictors and the outcome.\nFor this class, we will go over two types of supervised learning models: Naive Bayes and K-Nearest Neighbors. These can be classified as two different kinds of algorithms: parametric and non-parametric.\nParametric methods make assumptions about the functional form of \\(f\\), and then proceed to fit the data to this function in order to estimate its parameters. Such an approach simplifies the task of estimating \\(f\\), as it’s generally more manageable to estimate a set of parameters than it is to fit an entirely arbitrary function. However, as previously mentioned, the true form of \\(f\\) is typically unknown. Consequently, the primary challenge lies in the potential mismatch between the assumed shape of \\(f\\) and the actual distribution of the data. This issue can be mitigated by opting for more flexible models capable of accommodating various potential functional forms for \\(f\\). Nonetheless, a trade-off exists, as excessively flexible models may lead to overfitting.\nOn the other hand, non-parametric methods don’t make assumptions regarding the real shape of \\(f\\). Instead, they seeks an estimate of \\(f\\) that gets as close to the data points as possible. This resolves the problems related to assuming a particular form of \\(f\\) that might not fit our data. However, since they do not reduce the problem of estimating \\(f\\) to a small number of parameters, they require a large number of observations to obtain an accurate estimate."
  },
  {
    "objectID": "materials/13_supervised_learning/1_class_notes.html#what-is-machine-learning",
    "href": "materials/13_supervised_learning/1_class_notes.html#what-is-machine-learning",
    "title": "Supervised Learning I",
    "section": "",
    "text": "Machine learning is a subfield of artificial intelligence (AI) that involves the development of algorithms and models that enable computer systems to automatically learn from data without being explicitly programmed. The goal of machine learning is to develop systems that can identify patterns, make predictions or decisions, and improve their performance over time through experience. In other words, it’s a way of making a computer “learn” a set of rules and patterns from data. There are different kinds of problems we can approach and resolve with machine learning:\n\nPredict whether a patient, hospitalized due to a heart attack, could have a second heart attack. The prediction can be based on demographic, diet and clinical measurements for that patient.\nPredict the price of a stock in 6 months from now, on the basis of company performance measures and economic data.\nIdentify handwritten numbers and words in a picture.\nPredict salary based on demographic and labor variables.\nClassify texts according to the sentiments they express (positive or negative).\n\nThe challenges we mentioned so far belong to the subgroup of supervised learning machine learning tools. We call it supervised because it is characterized by training algorithms on labeled datasets. In a labeled dataset, each input data point is paired with its corresponding correct output. Throughout the training process, the algorithm acquires an understanding of the relationship between the input data and the desired output. The objective of supervised learning is to assimilate a mapping from input data to output labels. This enables the algorithm, when faced with novel, unseen data, to predict the accurate output by leveraging the learned patterns derived from the training dataset.\nVariables in supervised learning problems can be classified as input features (also called X, predictors, features) which will be used to predict an output variable (also called Y, response or dependent variable).\nWhile predicting specific outcomes is a powerful result that can come from machine learning methods, it is not the sole focus. We may also aim to understand the relationship between features and the response variable. This involves identifying crucial predictors among a vast array of variables, discerning the relationship between the response variable and each predictor, and elucidating the form that relationship takes.\nDepending on the type of variable Y, supervised learning problems can be divided into two subgroups:\n\nClassification problems: This occurs when Y is categorical (for example, when predicting the employment status of a person). The response provides the probability that the observation belongs to a particular class.\nRegression problems: This arises when Y is numerical (for example, when predicting the salary of a person). The response predicts a quantity.\n\nWe assume there is some relationship between our predictors and the response variable, which can be written in the general form:\n\\[\nY = f(X)+ ϵ\n\\]\nWhere \\(f\\) is a function that represents the systematic information that \\(X\\) provides about \\(Y\\), and \\(ϵ\\) is a random error term. This function is generally unknown, and one must estimate it based on the observed points. Machine learning can be thought as a set of approaches to estimate \\(f\\). In a way, we can think of these methods as a sophistication of linear and logistic regressions, since we can model complex, nonlinear relationships between the predictors and the outcome.\nFor this class, we will go over two types of supervised learning models: Naive Bayes and K-Nearest Neighbors. These can be classified as two different kinds of algorithms: parametric and non-parametric.\nParametric methods make assumptions about the functional form of \\(f\\), and then proceed to fit the data to this function in order to estimate its parameters. Such an approach simplifies the task of estimating \\(f\\), as it’s generally more manageable to estimate a set of parameters than it is to fit an entirely arbitrary function. However, as previously mentioned, the true form of \\(f\\) is typically unknown. Consequently, the primary challenge lies in the potential mismatch between the assumed shape of \\(f\\) and the actual distribution of the data. This issue can be mitigated by opting for more flexible models capable of accommodating various potential functional forms for \\(f\\). Nonetheless, a trade-off exists, as excessively flexible models may lead to overfitting.\nOn the other hand, non-parametric methods don’t make assumptions regarding the real shape of \\(f\\). Instead, they seeks an estimate of \\(f\\) that gets as close to the data points as possible. This resolves the problems related to assuming a particular form of \\(f\\) that might not fit our data. However, since they do not reduce the problem of estimating \\(f\\) to a small number of parameters, they require a large number of observations to obtain an accurate estimate."
  },
  {
    "objectID": "materials/13_supervised_learning/1_class_notes.html#footnotes",
    "href": "materials/13_supervised_learning/1_class_notes.html#footnotes",
    "title": "Supervised Learning I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBayes’ Theorem states that the conditional probability of an event, based on the occurrence of another event, is equal to the likelihood of the second event given the first event multiplied by the probability of the first event. It is a mathematical formula for determining the conditional probability of an event.↩︎\nThere are many other ways to measure distances between observations in different dimensions: Manhattan, Minkowski, Chebychev distance, Cosine Similarity or Hamming are a few of them.↩︎"
  },
  {
    "objectID": "materials/1_intro_I/1_class_notes.html",
    "href": "materials/1_intro_I/1_class_notes.html",
    "title": "Introduction I",
    "section": "",
    "text": "Quantitative analysis can be a very powerful tool to understand systemic phenomena. By translating similar experiences shared by large groups of people into data, we can identify repeating patterns. This allows us to contextualize our personal experiences and address issues from a systemic perspective1, leading to more effective solutions and positive change for ourselves and the communities we are a part of.\nHowever, data are not neutral or objective. They are the products of unequal social relations, and understanding this context is essential for conducting accurate, ethical analysis.\nIt is important to critically analyze data in order to avoid replicating the biases that may be inherent in the data. Standard practices in data science may serve to reinforce these existing inequalities (D’ignazio and Klein 2023).\n\n\nThe demographics of data science, as well as related occupations such as software engineering and artificial intelligence research, are not representative of the population as a whole. The vast majority of individuals working in these fields are elite white men.\nIt is important to keep in mind that science is shaped by the unique perspectives of individual scientists, and therefore researchers’ subjectivities and biases can influence the way data sets are built and later analyzed. Therefore, it is necessary to be transparent about the context in which research is conducted as well as potential sources of bias, thus acknowledging the partiality of our perspectives.\nThe following example illustrates the biases that may result from the work of non-diverse groups. A Ghanaian-American graduate student at MIT, Joy Buolamwini, experienced a problem when working on a class project using facial-analysis software, as the software could not detect her face but had no problem detecting the faces of her lighter-skinned collaborators. She discovered that the system’s facial recognition features worked perfectly when she put on a white mask. Later, she found the data set on which many of facial-recognition algorithms are tested contains 78 percent male faces and 84 percent white faces (D’ignazio and Klein 2023).\n\n\n\nJoy Buolamwini’ experiment\n\n\n\n\n\nData that could be used to address critical social issues may be missing2, but powerful institutions may also create databases and data systems through excessive surveillance of marginalized groups.\nHaving more data on a subject may not always have a positive impact. The paradox of exposure illustrates the complexity of this matter. This term describes a paradoxical situation in which individuals who have the most to gain from being counted or classified are also the most vulnerable to the potential dangers posed by that very act of counting or classifying. For example: undocumented immigrants are less likely to complete the census questionnaire due to fear of deportation, which leads to undercounting in the census. This results in less political representation and federal funding being allocated to these groups, and therefore less voting power and fewer resources for them (D’ignazio and Klein 2023).\nFurthermore, the effectiveness of data analysis in addressing societal issues is heavily influenced by the quality of the input data. In this regard, it is fundamental to analyze which variables are selected, as well as their categories. A clear example of this challenge is racial classification: while it can help understanding and fighting racial stratification, it can also be used to preserve it. The interpretation and impact of these types of variables on data analysis are dependent on various factors, such as the role of critical or intersectional perspectives within research teams, the causal theories guiding the empirical analysis or the contextualized or decontextualized nature of the interpretation of results. Therefore, racial data can play a crucial role in addressing racial inequality, but this does not mean every statistic should be presented racially. In fact, some racial statistics have contributed to racial conflict by implying that race determines behavior (Zuberi 2001)."
  },
  {
    "objectID": "materials/1_intro_I/1_class_notes.html#biases-and-objectivity",
    "href": "materials/1_intro_I/1_class_notes.html#biases-and-objectivity",
    "title": "Introduction I",
    "section": "",
    "text": "The demographics of data science, as well as related occupations such as software engineering and artificial intelligence research, are not representative of the population as a whole. The vast majority of individuals working in these fields are elite white men.\nIt is important to keep in mind that science is shaped by the unique perspectives of individual scientists, and therefore researchers’ subjectivities and biases can influence the way data sets are built and later analyzed. Therefore, it is necessary to be transparent about the context in which research is conducted as well as potential sources of bias, thus acknowledging the partiality of our perspectives.\nThe following example illustrates the biases that may result from the work of non-diverse groups. A Ghanaian-American graduate student at MIT, Joy Buolamwini, experienced a problem when working on a class project using facial-analysis software, as the software could not detect her face but had no problem detecting the faces of her lighter-skinned collaborators. She discovered that the system’s facial recognition features worked perfectly when she put on a white mask. Later, she found the data set on which many of facial-recognition algorithms are tested contains 78 percent male faces and 84 percent white faces (D’ignazio and Klein 2023).\n\n\n\nJoy Buolamwini’ experiment"
  },
  {
    "objectID": "materials/1_intro_I/1_class_notes.html#what-gets-counted-and-what-doesnt",
    "href": "materials/1_intro_I/1_class_notes.html#what-gets-counted-and-what-doesnt",
    "title": "Introduction I",
    "section": "",
    "text": "Data that could be used to address critical social issues may be missing2, but powerful institutions may also create databases and data systems through excessive surveillance of marginalized groups.\nHaving more data on a subject may not always have a positive impact. The paradox of exposure illustrates the complexity of this matter. This term describes a paradoxical situation in which individuals who have the most to gain from being counted or classified are also the most vulnerable to the potential dangers posed by that very act of counting or classifying. For example: undocumented immigrants are less likely to complete the census questionnaire due to fear of deportation, which leads to undercounting in the census. This results in less political representation and federal funding being allocated to these groups, and therefore less voting power and fewer resources for them (D’ignazio and Klein 2023).\nFurthermore, the effectiveness of data analysis in addressing societal issues is heavily influenced by the quality of the input data. In this regard, it is fundamental to analyze which variables are selected, as well as their categories. A clear example of this challenge is racial classification: while it can help understanding and fighting racial stratification, it can also be used to preserve it. The interpretation and impact of these types of variables on data analysis are dependent on various factors, such as the role of critical or intersectional perspectives within research teams, the causal theories guiding the empirical analysis or the contextualized or decontextualized nature of the interpretation of results. Therefore, racial data can play a crucial role in addressing racial inequality, but this does not mean every statistic should be presented racially. In fact, some racial statistics have contributed to racial conflict by implying that race determines behavior (Zuberi 2001)."
  },
  {
    "objectID": "materials/1_intro_I/1_class_notes.html#why-r",
    "href": "materials/1_intro_I/1_class_notes.html#why-r",
    "title": "Introduction I",
    "section": "Why R?",
    "text": "Why R?\n\nR is an open source software, which means that anyone can use, modify, and distribute it without cost. Additionally, R can easily be integrated with other programming languages (such as Python).\nR has a wide variety of functions that make it easy to import, manipulate, and analyze data. It is particularly useful for statistical analysis, data visualization, and machine learning.\nIt also has a large and active community of developers who contribute to the language and create useful packages (and who can help and support you when you encounter problems or have questions about the language!)."
  },
  {
    "objectID": "materials/1_intro_I/1_class_notes.html#the-basics",
    "href": "materials/1_intro_I/1_class_notes.html#the-basics",
    "title": "Introduction I",
    "section": "The basics",
    "text": "The basics\nWe will be using RStudio, an integrated development environment (IDE) designed to work with R (or Python).\nThe RStudio layout5:\n\nWhen working with R, the code is typically written in the source pane (1), although it is also possible to write code directly into the console (2). However, it is important to note that any code written in the console will not be saved.\nTo execute a line (or lines) of code, simply select the code chunk and press Ctrl + Enter (Windows) or Command + Return (Mac). It is also possible to run all the code on the file using the Source command (top right).\nThe results of the executed code are displayed in the console (2).\nThe environment pane (3) displays currently imported and created R objects.\nThe output pane (4) displays the generated plots, file tabs, packages, and help information. The basic command to find out what a function does or simply ask for help is help(“function name”) or ?(“function name”).\nTo modify the appearance of the execution environment, such as changing the colors or switching to “dark mode”, use Tools &gt; Global Options &gt; Appearance6.\nTo create a new script to work on, you can click on File &gt; New File &gt; R Script, or use the keyboard shortcut Ctrl + Shift + N (Windows) or Shift + Command + N (Mac)7.\nKeep in mind that objects created in R are not automatically saved on your computer. Specific functions must be used depending on the object type and the desired format for exporting it (e.g.: .xlsx tables, .png plots, etc.).\nFor more information on RStudio’s panes, visit this site\nRStudio also has a feature called projects, which is a way of compartmentalizing your R code and keeping all the files associated with a project together — input data, R scripts, analytical results, figures. Each project is linked to a working directory, which is the folder where R searches when loading and saving files89. Keep in mind that file paths will be expressed in relation to the project’s location on the computer, but it’s also possible to specify absolute paths (from the C: or D: drive of our computer).\nYou can create an RStudio project using File &gt; New Project:\n\nIn a brand new directory\nIn an existing directory where you already have R code and data\nBy cloning a version control (Git or Subversion) repository\n\nThis action will create a project file (with an .Rproj extension), which you can later open in order to open your project."
  },
  {
    "objectID": "materials/1_intro_I/1_class_notes.html#structuring-a-project",
    "href": "materials/1_intro_I/1_class_notes.html#structuring-a-project",
    "title": "Introduction I",
    "section": "Structuring a project",
    "text": "Structuring a project\nA good practice to structure a project directory is keeping your input, code and outputs in separate folders. Folder and file names should always be descriptive and easy to remember. For example:\nmy_project/\n    data/\n    code/\n    results/\n    report/\nThis folder arrangement will simplify the file import and export process."
  },
  {
    "objectID": "materials/1_intro_I/1_class_notes.html#final-general-setup-advice",
    "href": "materials/1_intro_I/1_class_notes.html#final-general-setup-advice",
    "title": "Introduction I",
    "section": "Final general setup advice",
    "text": "Final general setup advice\nBy default, R will save your environment to a .RData file after you quit a session and reload it when you start a new session. This is not recommended because it might delay start-up, or even generate an endless crash cycle if a previous session crashed. It might also make your work harder to reproduce. To change this setting, got to: Tools &gt; Global Options &gt; General, and untick “Restore .RData into workspace at startup” and select the option “Never” for “Save workspace to .RData on exit:”.\nR will also save a log of all commands entered into the console during an R session in a .Rhistory file. To change this setting untick the “Always save history (even when not saving .RData)” option in the same menu."
  },
  {
    "objectID": "materials/1_intro_I/1_class_notes.html#file-types-.r-and-.rmd",
    "href": "materials/1_intro_I/1_class_notes.html#file-types-.r-and-.rmd",
    "title": "Introduction I",
    "section": "File types: .R and .Rmd",
    "text": "File types: .R and .Rmd\n\n.R files contain plain code that can be executed by R (including commands you create objects, transform them, create visualizations, save objects, etc.).\n\n\nWhen should we use .R files?\n.R files are useful to carry out the first steps of our data-processing workflow: importing and exploring the data base, and starting to designing the elements we will use to communicate our results (tables, graphs, etc.).\n\n\n.Rmd files integrate the R and Markdown languages10. Markdown is a markup language for creating formatted text using a plain-text editor with simple, unobtrusive syntax.11\n\n\nWhen should we use .Rmd files?\n.Rmd files are designed to contain your code and also the narrative surrounding the data. Therefore, they facilitate teamwork and are also useful to communicate our results.\n\n\n\n\nArtwork by @allison_horst\n\n\n\nWhy use RMarkdown?(Alzahawi 2021)\n\nAvoid the common errors (and time consumed) that result from analyzing data using a different software from the one used to communicate your results (no more copy/pasting and/or reformatting all results after every modification!). All results and citations in your reports will be automatically updated after any modification.\nImprove your team-work and code reproducibility by using a report structure specially designed for you to clearly explain your methodology and result interpretation as you navigate your way through the data analysis process.\nSome arguments against RMarkdown:\n\nThe learning curve can be steep.\nBarriers to collaborating with others: all your team must be able to use R/RMarkdown (and additional tools such as GitHub)."
  },
  {
    "objectID": "materials/1_intro_I/1_class_notes.html#packages",
    "href": "materials/1_intro_I/1_class_notes.html#packages",
    "title": "Introduction I",
    "section": "Packages",
    "text": "Packages\nIn R, the fundamental unit of shareable code is the package. A package bundles together code, data, documentation, and tests, and is easy to share with others. As of March 2023, there were over 19,000 packages available on the Comprehensive R Archive Network, or CRAN, the public clearing house for R packages (Wickham 2023).\n\nWhy are packages useful?\nSomeone has probably already solved the problem you’re working on, and you can benefit from their work by downloading their package12."
  },
  {
    "objectID": "materials/1_intro_I/1_class_notes.html#r-syntax",
    "href": "materials/1_intro_I/1_class_notes.html#r-syntax",
    "title": "Introduction I",
    "section": "R syntax",
    "text": "R syntax\nBefore we start coding, some general rules about the R language13:\n\nR is case-sensitive: variable1 and VARIABLE1 will be interpreted as two different objects.\nWhitespaces and new lines: R will ignore whitespaces and new lines inside an expression, using them to delimit expressions only in case of ambiguity. If an expression can terminate at the end of the line the parser will assume it does so. Neutral new lines will be useful to make our code easier to understand.\nComments in our code should be preceded by #. Otherwise, every line we write will be evaluated as a line of code. Comments will be useful to explain our code to future users."
  },
  {
    "objectID": "materials/1_intro_I/1_class_notes.html#operators",
    "href": "materials/1_intro_I/1_class_notes.html#operators",
    "title": "Introduction I",
    "section": "Operators",
    "text": "Operators\nAssignment\nAssignment operators are used to define objects, that is, to assign them a value.\n-&gt;: Right assignment\n&lt;-: Left assignment\n=: Left assignment\nFor left (right) assignment operators, the name of the object should be placed to the left (right) of the operator, while the definition of the object should be placed to the right (left). For example:\n\nvalue_1 &lt;- 1\n\nWhen an object is defined in R, it is saved in the program’s environment and can be accessed and used later in the code. When running a line with the name of an object, its content is displayed in the console.\n\nvalue_1\n\n 1\n\n\nIt is important to keep in mind that if an object is defined with the same name as a previously existing object, the latter replaces the former.\n\nvalue_1 &lt;- 1\nvalue_1 &lt;- 2\n\nvalue_1\n\n 2\n\n\nRelational\nRelational operators are used to describe relationships between objects, which are expressed as true (TRUE) or false (FALSE)14.\n&gt;: Greater than\n&gt;=: Greater than or equal to\n&lt;: Less than\n&lt;=: Less than or equal to\n==: Equal to\n!=: Not-equal to\nFor example:\n\n4==5\n\n FALSE\n\n\n\n4==4\n\n TRUE\n\n\n\n4&gt;=5\n\n FALSE\n\n\nLogical\nLogical operators are used to combine logical expressions.\n!: Not\n&: And\n|: Or\nConsidering the examples mentioned above, these are some of the expressions we can create combining them:\n\n4==5 & 4==4\n\n FALSE\n\n\n\n4==5 | 4==4\n\n TRUE\n\n\n\n4==5 | !(4==4)\n\n FALSE\n\n\nArithmetic\nSome of R’s basic arithmetic operators:\n+: Plus\n-: Minus\n*: Multiplication\n/: Division\n^: Exponentiation\nExample of usage:\n\n(4+4)*8\n\n 64\n\n\nCombinations!\nOperators can be combined to create increasingly complex expressions:\n\n(2*8 == 4^2 | 3*3 &lt;= 2^3) & 3 != 2\n\n TRUE"
  },
  {
    "objectID": "materials/1_intro_I/1_class_notes.html#r-objects",
    "href": "materials/1_intro_I/1_class_notes.html#r-objects",
    "title": "Introduction I",
    "section": "R objects",
    "text": "R objects\nWhile R objects might appear somewhat abstract at this point, we will delve into specific examples during the guided practice to provide a clearer understanding.\nValues\nValues in R are the smallest objects, which will be used as building blocks for everything else. The main value types are:\n\n“numeric” for any numerical value (such as 3.5, -.4).\n“character” for text values, denoted by using quotes around a value (for example: “x” or ‘x’). They can include any printable character (for example: “a”, “Hi!” or “#2.5%jk%?”).\n“logical” for TRUE and FALSE (the Boolean data type).\n“integer” for integer numbers (the qualifier L at the end of a number indicates to R that it’s an integer, for example: 4L).\n“complex” to represent complex numbers with real and imaginary parts (such as 2i or 3+8i).\n\nVectors\nA vector is a set of values of the same class. There may be numeric, character, etc. vectors. To create a vector, we use the c() command (which stands for ‘combine’).\nA special case is the factor type vectors. They are generally used for ordinal data. That is, for a qualitative variable for which we need to establish a certain order in its possible values.\nData Frames\nA data frame is a two-dimensional data structure or table, where each column represents a variable, and each row represents an observation. Data frames can contain data of different classes.\nIt can be considered as a set of equally sized vectors, where each vector (column) should contain data of the same type. However, the classes of vectors that make up the table can be different. Therefore, each observation (row) can be composed of data that can be of different types.\nData frames are a fundamental object in the R programming language, as they are the most commonly used structure for data analysis. Data frames are also typically used for loading external data into the R environment and for exporting the results of our work.\nLists\nA list is a collection of objects of any type, including other lists, data frames, vectors, or individual values. While a vector contains values of a single type, a data frame contains vectors of different types. Similarly, a list can contain data frames, but can also contain other lists, vectors, or individual values, all at the same time. Lists are a flexible and powerful data structure in R that can be used to organize complex and diverse data."
  },
  {
    "objectID": "materials/1_intro_I/1_class_notes.html#footnotes",
    "href": "materials/1_intro_I/1_class_notes.html#footnotes",
    "title": "Introduction I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIs data enough? In 2015, communications researcher Candice Lanius wrote a widely shared blog post, Fact Check: Your Demand for Statistical Proof is Racist in which she summarizes the ample research on how those in positions of power accept anecdotal evidence from those like themselves, but demand endless statistics from minoritized groups. In those cases, she argues, more data will never be enough.↩︎\n“The Library of Missing Datasets is a physical repository of those things that have been excluded in a society where so much is collected. The word missing is used to imply both a lack and an ought: something does not exist, but it should.”↩︎\nYou can find some solutions to frequent problems here.↩︎\nHere you can find some suggestions on how to write more readable code.↩︎\nIt is possible to change the default location of the panes.↩︎\nMore info on customizing RStudio.↩︎\nFor more keyboard shortcuts got to: Help &gt; Keyboard Shortcuts Help. Note that Mac and Windows users have different shortcuts.↩︎\nWhat happens when a project is opened?↩︎\nWhen we position ourselves between the quotation marks that define the location for searching or saving a file, pressing Tab in R will reveal the documents or folders residing within the folder we are actively working in. It’s always a good practice to search for files or folders in this manner instead of manually typing their names in order to prevent errors.↩︎\nThese class notes were written using an .Rmd file!↩︎\nResources on how to use RMarkdown.↩︎\nHow can I find the package I’m looking for? https://cran.r-project.org/web/views/↩︎\nFor a more exhaustive description of the R language, visit this site.↩︎\nRemember R is case-sensitive!↩︎"
  },
  {
    "objectID": "materials/12_preprocessing/1_class_notes.html",
    "href": "materials/12_preprocessing/1_class_notes.html",
    "title": "Preprocessing",
    "section": "",
    "text": "Behind the hype of data modeling and machine learning there is a specialized knowledge of the topics that are modeled and a process of construction of the databases. That is, data does not just ‘appear’. Most of the times, it is built by a group of researchers or public government professionals who carefully build instruments to measure different indicators relevant for complex issues. But of course, this process can present errors: wrongly coded information, missing values, inconsistencies, and so on. Therefore, the preprocessing of a dataset is a highly important step in the whole workflow and it can’t be thought of as something independent. During this class, we will go over some of the steps that one should take into account when preprocessing data for a modeling workflow.\n\n\nThe first step in data processing implies doing a basic exploration. This implies looking over the documentation of the dataset provided (if it wasn’t built by ourselves), importing the dataset into the statistical program we will use and summarizing the main characteristics. Our goal in this step is to gain a deep understanding of the dataset: including its structure, content and context. A key element to take into account is the types of variables and how we expect them to perform. Recall that we have three types of variables:\n\nCategorical: Character values, with no specific order. For example, a variable naming countries can have the categories “United States”, “Spain”, “Germany”, “Argentina”… and so on. There is no implicit or explicit order for these categories.\nContinuous: Numerical values, which can be integers or present decimals. For example, variables indicating income, age, or exam scores.\nOrdinal: Character values that have an order or ranking associated. For example, the educational level can be categorized as “High School”, “Bachelor’s Degree”, “Master’s Degree”, and “PhD”, with an implicit order from lower to higher education.\n\nIt is important to check the type of variables in our dataset to ensure that they are consistent with the element they are supposed to measure. For example, if we find that our ‘Age’ column is a text variable, we quickly realize something is wrong. Additionally, it is advisable to look at the distribution of the variables.\n\nImagine we have an example data frame with the exam’s performance of 10 students in a school.\n\n\n\n   Student_ID      Exam_Score            Gender      Age           \n Min.   : 1.00   Min.   : 78.00   Female    :3   Length:10         \n 1st Qu.: 3.25   1st Qu.: 83.75   FEMALE    :1   Class :character  \n Median : 5.50   Median : 89.00   Male      :4   Mode  :character  \n Mean   : 5.50   Mean   :139.12   Non-Binary:1                     \n 3rd Qu.: 7.75   3rd Qu.: 92.75   Unknown   :1                     \n Max.   :10.00   Max.   :505.00                                    \n                 NA's   :2                                         \n      GPA       \n Min.   :2.900  \n 1st Qu.:3.525  \n Median :3.750  \n Mean   :3.810  \n 3rd Qu.:4.125  \n Max.   :5.000  \n                \n\n\nWe can distinguish some inconsistencies, missing and odd values. The maximum GPA value reaches 5.00 (it should be up to 4.00), and a student had an exam score of 505 (probably responsible for the average to rise to 139.12) . We can also see there are two NA’s (missing values) in the exam scores. The variable ‘Gender’ presents a value labeled as ‘Unknown’ and a female student wrongly coded in capital letters.Additionally, we could argue that female and ‘male’ would not be the corresponding values for a variable referring to gender, since the labels ‘woman’ and ‘man’ would be more appropriate. Lastly, the column ‘Age’ appears as a character feature, which is strange for a numerical value.\nThis can also be explored through plots. We could use different distribution graphs for the numerical values. The box plot tends to be one of the best options, since it easily displays minimal/maximal values and outliers.\n\nFor example, look at the variable Exam_Score:\n\n\nggplot(sample_data, aes(y = Exam_Score))+\n  geom_boxplot()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nNote how the value 505 escapes the distribution.\n\nFor categorical values, a simple count bar for categorical values can be informative:\n\n\nggplot(sample_data, aes(x = Age))+ \n  geom_bar()\n\n\n\n\n\n\n\n\nThese graphs, while simple, provide us a lot of information about the dataset we are working with and help us detect potential issues. In the following sections, we will address different ways to clean and transform our data for a better modeling. While we will provide some tools on how to proceed during this stage, it is important to note that there is no unique feature engineering for all data workflows. Each data set has different variables and addresses different issues, so the transformations we make will be based on the domain knowledge we have on it.\nHowever, some methods also have a set of standard preprocessing steps recommended for modeling: Encoding as dummy variables for logistic and linear regressions, imputation of missing values for Naive Bayes, and so on1."
  },
  {
    "objectID": "materials/12_preprocessing/1_class_notes.html#exploring-the-dataset",
    "href": "materials/12_preprocessing/1_class_notes.html#exploring-the-dataset",
    "title": "Preprocessing",
    "section": "",
    "text": "The first step in data processing implies doing a basic exploration. This implies looking over the documentation of the dataset provided (if it wasn’t built by ourselves), importing the dataset into the statistical program we will use and summarizing the main characteristics. Our goal in this step is to gain a deep understanding of the dataset: including its structure, content and context. A key element to take into account is the types of variables and how we expect them to perform. Recall that we have three types of variables:\n\nCategorical: Character values, with no specific order. For example, a variable naming countries can have the categories “United States”, “Spain”, “Germany”, “Argentina”… and so on. There is no implicit or explicit order for these categories.\nContinuous: Numerical values, which can be integers or present decimals. For example, variables indicating income, age, or exam scores.\nOrdinal: Character values that have an order or ranking associated. For example, the educational level can be categorized as “High School”, “Bachelor’s Degree”, “Master’s Degree”, and “PhD”, with an implicit order from lower to higher education.\n\nIt is important to check the type of variables in our dataset to ensure that they are consistent with the element they are supposed to measure. For example, if we find that our ‘Age’ column is a text variable, we quickly realize something is wrong. Additionally, it is advisable to look at the distribution of the variables.\n\nImagine we have an example data frame with the exam’s performance of 10 students in a school.\n\n\n\n   Student_ID      Exam_Score            Gender      Age           \n Min.   : 1.00   Min.   : 78.00   Female    :3   Length:10         \n 1st Qu.: 3.25   1st Qu.: 83.75   FEMALE    :1   Class :character  \n Median : 5.50   Median : 89.00   Male      :4   Mode  :character  \n Mean   : 5.50   Mean   :139.12   Non-Binary:1                     \n 3rd Qu.: 7.75   3rd Qu.: 92.75   Unknown   :1                     \n Max.   :10.00   Max.   :505.00                                    \n                 NA's   :2                                         \n      GPA       \n Min.   :2.900  \n 1st Qu.:3.525  \n Median :3.750  \n Mean   :3.810  \n 3rd Qu.:4.125  \n Max.   :5.000  \n                \n\n\nWe can distinguish some inconsistencies, missing and odd values. The maximum GPA value reaches 5.00 (it should be up to 4.00), and a student had an exam score of 505 (probably responsible for the average to rise to 139.12) . We can also see there are two NA’s (missing values) in the exam scores. The variable ‘Gender’ presents a value labeled as ‘Unknown’ and a female student wrongly coded in capital letters.Additionally, we could argue that female and ‘male’ would not be the corresponding values for a variable referring to gender, since the labels ‘woman’ and ‘man’ would be more appropriate. Lastly, the column ‘Age’ appears as a character feature, which is strange for a numerical value.\nThis can also be explored through plots. We could use different distribution graphs for the numerical values. The box plot tends to be one of the best options, since it easily displays minimal/maximal values and outliers.\n\nFor example, look at the variable Exam_Score:\n\n\nggplot(sample_data, aes(y = Exam_Score))+\n  geom_boxplot()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nNote how the value 505 escapes the distribution.\n\nFor categorical values, a simple count bar for categorical values can be informative:\n\n\nggplot(sample_data, aes(x = Age))+ \n  geom_bar()\n\n\n\n\n\n\n\n\nThese graphs, while simple, provide us a lot of information about the dataset we are working with and help us detect potential issues. In the following sections, we will address different ways to clean and transform our data for a better modeling. While we will provide some tools on how to proceed during this stage, it is important to note that there is no unique feature engineering for all data workflows. Each data set has different variables and addresses different issues, so the transformations we make will be based on the domain knowledge we have on it.\nHowever, some methods also have a set of standard preprocessing steps recommended for modeling: Encoding as dummy variables for logistic and linear regressions, imputation of missing values for Naive Bayes, and so on1."
  },
  {
    "objectID": "materials/12_preprocessing/1_class_notes.html#cleaning-the-data",
    "href": "materials/12_preprocessing/1_class_notes.html#cleaning-the-data",
    "title": "Preprocessing",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nAs previously highlighted, the quality of data stands as a key factor influencing the efficacy of a model. This concern is particularly pertinent within the realm of contemporary, burgeoning databases that draw from vast and diverse big data sources. Data originating from online platforms is especially susceptible to errors, outliers, and inconsistencies. The rationale behind this susceptibility lies in the absence of a specific and rigorous methodology governing the instruments of measurement and the sampling process. In such less-controlled settings, data quality tends to be more vulnerable to deterioration.\nWe will go over some methods we can use to address these errors and improve the quality of data.\n\nOutliers\nOutliers are data points that significantly differ from the majority of data in a dataset. These data points are often far removed from the central tendency of the data and can be either unusually high values (positive outliers) or unusually low values (negative outliers). Outliers are also sometimes referred to as anomalies or extreme values. The problem with values of this kind is that they do not represent the ‘normality’ of our observations. Therefore, if we input the outliers into our model they might add random variation or unwanted fluctuations in data that can obscure the underlying patterns, trends, or relationships of interest. This is called adding noise to the model information.\nTo clean this noise produced by the presence of outliers, we can adopt two strategies:\n\nDeleting the observations. One strategy to address outliers is to delete the extreme values. However, this approach involves a trade-off, as it leads to a reduction in the sample size. This trade-off becomes more pronounced when working with datasets containing a relatively low number of observations. Removing additional observations may not be advisable, particularly if the outlier is present in only a few features. For instance, in our sample dataset, we identified two outliers: one in the GPA column (5) and another in the exam scores (505). Despite these outliers, the remaining features for each observation exhibited normal behavior, providing valuable information for potential model development. Given the limited size of our dataset (only 10 observations), a cautious approach is required. Removing a significant portion, such as 20%, solely due to the presence of outliers may not be a prudent decision. An alternative strategy is to selectively remove the observation when analyzing the variable with an outlier while retaining it for the analysis of other variables. This way, we balance the need to address outliers without compromising the overall dataset integrity.\nImputation. Another approach to handling these values is to impute or estimate them. Performing imputation, we are able to keep the variables with outliers. However, this might introduce some bias and reduce variability of the data, so it’s important to assess the impact of imputation on the analysis. There are multiple ways to perform imputation. As scientists, the choice for an imputation method should be based on the nature of the data and the type of outlier.\n\nMedian imputation: Replace outliers with the median of the observed data for the respective variable. In our dataset, it would imply replacing the outlier GPA for 3.75 and the outlier exam score for 89.\nMode imputation: Replace extreme values with the mode (most frequent category) of the observed data. In our case, we can’t do it since we do not have outliers with categorical data.\nK-nearest neighbors (K-NN) imputation: Replace outliers with values from the K-nearest data points based on similarity in other features.\nMultiple Imputation: Generate multiple imputed datasets, each with different imputed values, to account for uncertainty and variability in imputations.\nTime-series imputation: For time-series data, outliers can be imputed using interpolation or forecasting methods.\n\n\n\n\nMissing values\nMissing data is a prevalent occurrence when dealing with real-world datasets. Such instances may arise due to insufficient information, such as when a user opts not to respond to a survey question, or due to measurement failures. Regardless of the cause, the presence of missing values in the initial predictors poses a challenge for various predictive models. Consequently, to leverage predictors or employ feature engineering techniques, it becomes imperative to first address the issue of missing data.\nBesides the absence of measurements within the predictors, there might also be missing measurements in the response variable. Many modeling techniques cannot effectively use samples with missing target variables in the training data.\nAs with outliers, addressing the issue of missing values involves both imputation and data removal. The same imputation methods applied to outliers can be used in this context. In data removal, not only can we eliminate observations, but we can also exclude features containing a significant number of missing values. However, caution is necessary. It is important not only to be mindful of removing excessive observations and variables but also to consider that missing data may often be information itself. For instance, if a variable related to sensitive topics like domestic abuse has numerous missing answers, it may indicate under-reporting of the issue. Similarly, in a time-series survey, persistent missing values in certain rows may suggest a group that is less likely to participate in surveys. In such scenarios, indiscriminate data removal or imputation can introduce bias, skewing our information towards an underrepresentation of these cases.\n\nImputation with external sources\nUntil now, we have discussed various methods to replace missing data in our dataset using the available information. However, there has been recent research exploring the use of statistical modeling to incorporate information from different sources to complement our missing values. Administrative data is particularly useful for replacing values, such as gender and race (Kreuter 2013). However, it is crucial to be cautious of potential biases that may arise from the use of secondary data frames."
  },
  {
    "objectID": "materials/12_preprocessing/1_class_notes.html#transformations-of-the-data",
    "href": "materials/12_preprocessing/1_class_notes.html#transformations-of-the-data",
    "title": "Preprocessing",
    "section": "Transformations of the data",
    "text": "Transformations of the data\nAnother part of feature engineering is the transformation of existing features, which implies modifying the representation of the variables.\n\nMulticollinearity\nWe have previously mentioned that highly correlated variables might impact the effectiveness of our models.\nThis is because these variables add redundant information, not new layers of information relevant to the model. This can make it challenging for our model to differentiate the individual effects of these variables, leading to unstable and unreliable estimates of their coefficients.\nAnother issue that might arise with highly correlated variables is overfitting. This can occur because a model might capture the noise or minor variations in the data rather than the true underlying patterns.\nIn general, multicollinearity tends to reduce our model’s robustness. Models with highly correlated variables may become sensitive to small changes in the data. If the data distribution shifts or if there is noise or variability in the dataset, our model’s performance can deteriorate.\nThere are several ways to address highly correlated variables.\n\nFeature selection. The first option is to choose a subset of variables that are relevant and less correlated with each other. To do so, we should possess domain knowledge of the issue and have performed exploratory analysis on the dataset to detect which variables are the most relevant.\nFeature Engineering. Create new features that capture the essential information from correlated variables while reducing multicollinearity. This can be achieved by creating new variables that summarize interactions between two or more variables.\nPrincipal Components Analysis. PCA allows us to “summarize” multiple correlated variables into a smaller set of dimensions. The fundamental concept behind PCA is to find a linear combination of the original features that captures the maximum amount of variance within the data. Essentially, it identifies the most influential directions within the dataset. When performing PCA, we generate as many Principal Components as we specify, each of which is orthogonal to the others. This orthogonality ensures that each feature is uncorrelated with the others. Consequently, PCA not only reduces the dimensionality of the data but also uncovers the underlying structure and relationships among variables, making it a valuable tool for simplifying complex datasets. 3\n\n\n\nRecoding variables\nWhen transforming the data, it is crucial to recode our variables into an appropriate format for the model, especially in the case of date and character variables.\n\nWhen dealing with dates, it is essential to ensure they are treated accordingly. Dates are often initially read as character variables, leading to the loss of the time series component. Alternatively, converting them into numeric values is a viable option, with the model interpreting them as the number of days after a reference date. However, a more meaningful approach is to transform them into derived variables that hold greater potential importance for the model, such as the day of the week, month, year, etc.\nIn many models, inputting categorical or character values directly is not permitted. To retain these variables as information for the model, we employ dummy variables. Also referred to as indicator or binary variables, they quantitatively represent categorical data in a binary format. Each value from the categorical variable is encoded into a new variable with a binary response (0 or 1). For a categorical variable with ‘n’ categories, ‘n-1’ dummy variables are typically created. If an observation belongs to that category, the dummy variable takes on a value of 1; otherwise, it remains 0. This representation allows us to incorporate categorical information into statistical models.\nOften, there is also an interest in regrouping categorical variables to simplify the information for the model. This process involves reorganizing or combining variables to create new groups or categories. Regrouping serves to reduce the number of categories, simplifying the data and mitigating the impact of potential outliers.\n\n\n\nNormalizing variables\nLastly, the normalization of variables is a crucial step in many machine learning methods. This term pertains to the scaling of variables, transforming them into a new set with the same order of magnitude. This step is significant because certain models are sensitive to the magnitudes of the features. For instance, a model might incorrectly perceive the variable “income” as more relevant than “age” solely based on their scales, with the former being in the thousands, potentially biasing the model. To mitigate this bias, we normalize variables, ensuring they share the same order of magnitude. Scaling transformations play a pivotal role in achieving this normalization, and various methods1 will be explored in the guided practice.\nAnother vital aspect of variable normalization is centering the variables. Centering involves shifting the values of a variable so that its mean (average) becomes zero. This is achieved by subtracting the mean of the variable from each data point. Centering is crucial for certain models, such as PCA, which assume that variables are centered around zero. Additionally, it aids in reducing colinearity. By centering variables, we enhance the ability to distinguish the effects of variables in regression models. Finally, centering variables is often employed in interaction terms within regression models to ensure that the interpretation of interaction effects remains meaningful and is not influenced by the choice of reference points."
  },
  {
    "objectID": "materials/12_preprocessing/1_class_notes.html#footnotes",
    "href": "materials/12_preprocessing/1_class_notes.html#footnotes",
    "title": "Preprocessing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis table contains a exhaustive specification of preprocessing steps for each tidyverse (Silge and Julia, n.d.) models.↩︎\nWe will address ways to measure how effective a model is in the evaluation class.↩︎\nFor more information on PCA, you can read the chapter 6.3.1. of Introduction to Statistical Learning by James, Witten, Hastie and Tibshiriani.↩︎"
  },
  {
    "objectID": "materials/11_web_scraping/1_class_notes.html",
    "href": "materials/11_web_scraping/1_class_notes.html",
    "title": "Web Scraping",
    "section": "",
    "text": "The internet is a vast repository of information, making it invaluable for various fields of study. To harness this wealth of data, web scraping tools become essential. Web scraping involves automating the process of extracting data from websites. The fundamental logic of web scraping includes:\n\nEntering a Page: Navigating to a specific webpage.\nSearching in HTML Code: Locating desired information within the HTML code.\nTabular Transformation: Converting the extracted data into a tabular format, typically rows and columns.\n\n\n\nThe content and structure of a web page is written in HTML code. HTML code follows a tree-like structure, with parent nodes that open child ones and are composed by different objects. These different objects are called tags. Tags are essentially special keywords or code enclosed in angle brackets (&lt; &gt;) that indicate how the content on a web page should be displayed or formatted.\n\n\n\nSource: Wikipedia\n\n\nHTML tags come in pairs: an opening tag and a closing tag. The opening tag tells the browser when to start applying a specific formatting or structure, and the closing tag indicates when to stop applying that formatting or structure.\nHere’s the basic structure of an HTML tag:\n\n&lt;tagname&gt;Content goes here&lt;/tagname&gt;\n\nNote that the HTML tag has three elements. First, an opening tag, which is the first part of the tag and specifies the beginning of an element. It consists of the tag name enclosed in angle brackets. Afterwards, it contains the content, which is the the information or text that you want to include within the element. This content is displayed or affected by the tag. Finally, the closing tag is the final part of the tag and indicates the end of the element. It also consists of the tag name enclosed in angle brackets but with a forward slash (/) before the tag name.\nSome common HTML tags include:\n\n&lt;html&gt;: The root element that contains the entire HTML document.\n&lt;head&gt;: Contains metadata about the document, such as the title and links to external resources.\n&lt;title&gt;: Sets the title of the web page, which appears in the browser’s title bar or tab.\n&lt;body&gt;: Contains the main content of the web page that is visible to users.\n&lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt;, etc.: Define headings of different levels, with &lt;h1&gt; being the highest level.\n&lt;p&gt;: Defines a paragraph of text.\n&lt;a&gt;: Creates hyperlinks to other web pages or resources.\n&lt;img&gt;: Embeds images in the web page.\n&lt;ul&gt;: Creates an unordered (bulleted) list.\n&lt;ol&gt;: Creates an ordered (numbered) list.\n&lt;li&gt;: Defines list items within &lt;ul&gt; or &lt;ol&gt; elements.\n&lt;div&gt;: A generic container for grouping and styling elements.\n\nAside from the tags, the elements of an HTML structure contain attributes. HTML attributes are special characteristics or properties that can be added to provide additional information about those elements. Attributes are used to modify or enhance the behavior and appearance of HTML elements. They are always specified within the opening tag of an HTML element and are defined as name-value pairs, separated by an equal sign (=).\nHere’s the basic syntax of an HTML attribute:\n\n&lt;tagname attribute=\"value\"&gt;Content goes here&lt;/tagname&gt;\n\nIt is important to note that the attributes have a name and a value. The value is typically enclosed in double or single quotes, though in some cases, it can be unquoted.\nSome attributes can be used on almost any HTML element and are known as global attributes, while others are specific to certain elements. For instance, the id attribute can be used on most HTML elements, but the src attribute is specific to &lt;img&gt; elements.\nSome of the most commonly used HTML attributes are:\n\nid: Specifies a unique identifier for an element.\nclass: Assigns one or more class names to an element.\nstyle: Allows inline CSS styling to be applied directly to an element.\nsrc: Specifies the source URL for elements like &lt;img&gt;, &lt;script&gt;, and &lt;iframe&gt;.\nhref: Defines the URL of a linked resource for elements like &lt;a&gt;, &lt;link&gt;, and &lt;area&gt;.\ntitle: Specifies additional information about an element, often displayed as a tooltip when the mouse hovers over it.\nname: Used with form elements to identify and access their data when submitted.\nvalue: Sets the initial value of form elements like &lt;input&gt; and &lt;textarea&gt;.\n\nIt is important to note that the names of the attributes will be useful in selecting the elements of the webpage we want to scrape."
  },
  {
    "objectID": "materials/11_web_scraping/1_class_notes.html#html-structure",
    "href": "materials/11_web_scraping/1_class_notes.html#html-structure",
    "title": "Web Scraping",
    "section": "",
    "text": "The content and structure of a web page is written in HTML code. HTML code follows a tree-like structure, with parent nodes that open child ones and are composed by different objects. These different objects are called tags. Tags are essentially special keywords or code enclosed in angle brackets (&lt; &gt;) that indicate how the content on a web page should be displayed or formatted.\n\n\n\nSource: Wikipedia\n\n\nHTML tags come in pairs: an opening tag and a closing tag. The opening tag tells the browser when to start applying a specific formatting or structure, and the closing tag indicates when to stop applying that formatting or structure.\nHere’s the basic structure of an HTML tag:\n\n&lt;tagname&gt;Content goes here&lt;/tagname&gt;\n\nNote that the HTML tag has three elements. First, an opening tag, which is the first part of the tag and specifies the beginning of an element. It consists of the tag name enclosed in angle brackets. Afterwards, it contains the content, which is the the information or text that you want to include within the element. This content is displayed or affected by the tag. Finally, the closing tag is the final part of the tag and indicates the end of the element. It also consists of the tag name enclosed in angle brackets but with a forward slash (/) before the tag name.\nSome common HTML tags include:\n\n&lt;html&gt;: The root element that contains the entire HTML document.\n&lt;head&gt;: Contains metadata about the document, such as the title and links to external resources.\n&lt;title&gt;: Sets the title of the web page, which appears in the browser’s title bar or tab.\n&lt;body&gt;: Contains the main content of the web page that is visible to users.\n&lt;h1&gt;, &lt;h2&gt;, &lt;h3&gt;, etc.: Define headings of different levels, with &lt;h1&gt; being the highest level.\n&lt;p&gt;: Defines a paragraph of text.\n&lt;a&gt;: Creates hyperlinks to other web pages or resources.\n&lt;img&gt;: Embeds images in the web page.\n&lt;ul&gt;: Creates an unordered (bulleted) list.\n&lt;ol&gt;: Creates an ordered (numbered) list.\n&lt;li&gt;: Defines list items within &lt;ul&gt; or &lt;ol&gt; elements.\n&lt;div&gt;: A generic container for grouping and styling elements.\n\nAside from the tags, the elements of an HTML structure contain attributes. HTML attributes are special characteristics or properties that can be added to provide additional information about those elements. Attributes are used to modify or enhance the behavior and appearance of HTML elements. They are always specified within the opening tag of an HTML element and are defined as name-value pairs, separated by an equal sign (=).\nHere’s the basic syntax of an HTML attribute:\n\n&lt;tagname attribute=\"value\"&gt;Content goes here&lt;/tagname&gt;\n\nIt is important to note that the attributes have a name and a value. The value is typically enclosed in double or single quotes, though in some cases, it can be unquoted.\nSome attributes can be used on almost any HTML element and are known as global attributes, while others are specific to certain elements. For instance, the id attribute can be used on most HTML elements, but the src attribute is specific to &lt;img&gt; elements.\nSome of the most commonly used HTML attributes are:\n\nid: Specifies a unique identifier for an element.\nclass: Assigns one or more class names to an element.\nstyle: Allows inline CSS styling to be applied directly to an element.\nsrc: Specifies the source URL for elements like &lt;img&gt;, &lt;script&gt;, and &lt;iframe&gt;.\nhref: Defines the URL of a linked resource for elements like &lt;a&gt;, &lt;link&gt;, and &lt;area&gt;.\ntitle: Specifies additional information about an element, often displayed as a tooltip when the mouse hovers over it.\nname: Used with form elements to identify and access their data when submitted.\nvalue: Sets the initial value of form elements like &lt;input&gt; and &lt;textarea&gt;.\n\nIt is important to note that the names of the attributes will be useful in selecting the elements of the webpage we want to scrape."
  },
  {
    "objectID": "materials/11_web_scraping/1_class_notes.html#xpath-selectors",
    "href": "materials/11_web_scraping/1_class_notes.html#xpath-selectors",
    "title": "Web Scraping",
    "section": "XPath selectors",
    "text": "XPath selectors\nXPath selectors are a set of expressions and patterns used to navigate and select elements or data within an HTML document. XPath selectors allow you to target specific elements or data within the document’s tree-like structure (“XPath Syntax,” n.d.). Some of the most commonly used XPath selectors are:\n\ntagname: Selects all nodes with the tag “tagname”.\n/: Selects from the root node.\n//: Selects nodes in the document from the current node that match the selection no matter where they are.\n.: Selects the current node.\n..: Selects the parent of the current node.\n@attribute: Selects attributes named “attribute”.\n\nThese selectors can be combined to make complex element searches:\n\nelement1/element2: Selects element2 that is a child of element1.\n//element: Selects all elements with the specified name anywhere in the document.\nelement[@attribute=\"value\"]: Selects elements with a specific attribute and attribute value.\nelement[@attribute]: Selects elements with a specific attribute (regardless of the value).\n\nThese selectors can be built manually, however nowadays there are many tools and plugins one can use to click on different elements of a website and get the XPath. A good tool is the plugin xPath Finder, or the Selenium IDE for Google Chrome. However, there are many other options for other browsers or to install in the computer. You can also continue to explore XPath Selectors in this website."
  },
  {
    "objectID": "materials/11_web_scraping/1_class_notes.html#css-selectors",
    "href": "materials/11_web_scraping/1_class_notes.html#css-selectors",
    "title": "Web Scraping",
    "section": "CSS Selectors",
    "text": "CSS Selectors\nThese are selectors that are based on CSS (Cascading Style Sheets) styling. CSS styling is a technology used in web development to control the presentation and visual design of HTML elements on a web page.\nCSS uses selectors to target HTML elements and properties to define how those elements should be styled. Selectors can target specific elements or groups of elements based on their tags, classes, IDs, or other attributes. Properties specify aspects of an element’s appearance, such as color, size, spacing, and more. However, for our case, we will use CSS selectors as patterns to locate HTML elements and extracting the content of these elements (“CSS Selectors Reference,” n.d.).\nSome of the most common CSS selectors are:\n\n.class1: Selects all elements with class=“class1”.\n.class1.class2: Selects all elements with both class1 and class2 set within its class attribute.\n#firstname: Selects the element with id=“firstname”.\n*: Selects all elements.\nelement1: Selects all  elements.\nelement1.class1: Selects all  elements with class=“class1”.\n\nAs in the case of XPath selectors, CSS selectors can be built manually or automatically with a tool. The rvest package for web scraping has the plugin selectorGadget to build CSS selectors."
  },
  {
    "objectID": "materials/11_web_scraping/1_class_notes.html#static-websites",
    "href": "materials/11_web_scraping/1_class_notes.html#static-websites",
    "title": "Web Scraping",
    "section": "Static websites",
    "text": "Static websites\nStatic websites, often referred to as static web pages, are digital platforms composed of unchanging web pages. The content on a static website remains constant for all visitors and is impervious to alterations driven by user interactions or dynamic data sourced from a database. A good example of static websites are Wikipedia pages.\nThe content on static websites remains invariable until a developer intervenes to manually update it. These websites are particularly conducive to web scraping, given their lack of dynamically generated elements in response to user interactions. Extracting information from them primarily entails parsing the HTML structure, employing selectors to pinpoint the desired elements, and subsequently scraping the content.\nInspecting the HTML structure of a web page can be initiated through a simple process of right-clicking and selecting “Inspect” within the web browser, allowing deeper exploration."
  },
  {
    "objectID": "materials/11_web_scraping/1_class_notes.html#dynamic-websites",
    "href": "materials/11_web_scraping/1_class_notes.html#dynamic-websites",
    "title": "Web Scraping",
    "section": "Dynamic websites",
    "text": "Dynamic websites\nDynamic websites are websites that generate web pages on-the-fly, customizing content and appearance for each user or interaction. Unlike static websites, where the content is fixed and unchanging, dynamic websites use server-side scripting, databases, and other technologies to deliver content that can vary based on user input, user profiles, real-time data, or other factors. Dynamic websites are highly interactive and provide a personalized and data-driven experience for users. A good example of this kind of website is the MoMA page of artists in collection, where you have to click on the button ‘Show more results’ several times to get the entire list of artists. It is impossible to scrape the entire list of artists parsing the HTML structure as it appears, because it is updated every time the button ‘Show more results’ is clicked.\nTo scrape this kind of websites, we must use automated web browsers to make a script that interacts with the browser in the way a human would do and scrape the data. The most commonly used is the Selenium WebDriver, which can be integrated via Python. Some of the characteristics of automated web browsers are:\n\nProgrammatic Interaction: These browsers can perform actions such as loading web pages, clicking on links, filling out forms, and submitting data, just like a human user would. They do so through code or scripts.\nData Extraction: They can scrape text, images, links, and other information from web pages.\nHeadless Mode: Many modern web browsers, such as Google Chrome and Mozilla Firefox, offer a headless mode that allows them to run without a visible GUI. This makes them suitable for automation tasks.\nScripting Languages: Automated web browsers are often controlled using scripting languages, with the help of libraries or frameworks specifically designed for web automation.\nScheduled Tasks: They can be used to schedule repetitive tasks, such as checking for updates on websites, monitoring stock prices, or automating social media posts."
  },
  {
    "objectID": "materials/11_web_scraping/1_class_notes.html#api",
    "href": "materials/11_web_scraping/1_class_notes.html#api",
    "title": "Web Scraping",
    "section": "API",
    "text": "API\nAPI stands for Application Programming Interface. It is a server that provides access to the data of an application or service. It is a set of rules, protocols, and tools that allows different software applications to communicate with each other. APIs define the methods and data formats that applications can use to request and exchange information, making it possible for various systems to work together and share data and functionality.\nAPIs depend on the owners of the applications, and they can offer access to them for free, charge for them partially or totally or not offer them at all. In most services, one will have to create a developer account in order to access the API. After gaining access, the user has to make a set of HTTPS requests to the API endpoints specified in the documentation. The output will be an API response, usually in JSON format. This output will have to be parsed in order to extract the needed data, since the result will be a set of nested data structures which may contain the required information in different levels.\n\n\n\nAPI example"
  },
  {
    "objectID": "materials/11_web_scraping/1_class_notes.html#legal-considerations",
    "href": "materials/11_web_scraping/1_class_notes.html#legal-considerations",
    "title": "Web Scraping",
    "section": "Legal Considerations",
    "text": "Legal Considerations\nThe legality of web data scraping is a complex topic, which depends a lot on where you live. However, by general principle, “if the data is public, non-personal, and factual, you’re likely to be ok” (Wickham, Çetinkaya-Rundel, and Grolemund 2023). These aspects are the most important to take into account, because they are directly attached to a site’s terms and conditions, personal identifiable information and copyright. The importance of these points is the following:\n\nTerms and conditions: In here, companies establish the regulations governing the usage of their websites. While scrutinizing these pages, you may notice explicit prohibitions against web scraping.\nPersonally identifiable information: Data elements like names, email addresses, phone numbers, and dates of birth are profoundly sensitive and should not be collected lightly. Even if this information is publicly accessible on a profile, it is crucial to handle such data with the utmost care and ensure that personal information remains anonymized and unidentifiable.\nCopyright: When collecting data, one should also exercise caution with regard to copyrighted materials, including images, books, and other content that belongs to an author. Respecting copyright laws is imperative to avoid legal complications."
  },
  {
    "objectID": "materials/11_web_scraping/1_class_notes.html#scraped-data-as-open-data",
    "href": "materials/11_web_scraping/1_class_notes.html#scraped-data-as-open-data",
    "title": "Web Scraping",
    "section": "Scraped Data as Open Data",
    "text": "Scraped Data as Open Data\nWhile we should take into account the previous considerations in terms of the ethical aspects of scraping web data, it is important to note that web data is often a source of accessing data that wouldn’t be available in any other way. For example, in the field of study of public opinion, scraping online news’ portals is a way of getting information about media coverage of certain topics straight from the source. This can also be done manually, doing a qualitative collection, but scraping data allows us to systematically extract more data, which makes us gain more coverage.\nIn this sense, scraping can be thought as a way of democratizing access to data. Of course, it still maintains certain level of elitism, because one needs to know how to code in order to scrape. However, it is a good tool in order to conquer back the data collected by monopolies of companies. A good example of data that used to be democratized but turned back to elitist and private is Twitter data (“Twitter Is Closing Free Access to Its API Starting Feb. 9” 2023). When Elon Musk acquired Twitter, one of the first policies he adopted was to make the Twitter API a paid service. Twitter data, which used to be a free, open source of data to fuel innovative research topics, became an elitist tool and database which now can only be accessed by a few."
  },
  {
    "objectID": "materials/6_logistic_regression/1_class_notes.html",
    "href": "materials/6_logistic_regression/1_class_notes.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "The linear regression model discussed in previous classes assumes that the dependent variable Y is quantitative. The logistic regression model will help us in those situations where Y is qualitative and our aim is to compute the probability of each observation belonging to each possible category of Y. The process of predicting qualitative responses is known as classification (since it involves assigning the observation to a category or class) (James et al. 2013).\n\n\nDrawing on the synthesis proposed by Molnar (2022), the main reasons why linear regression is not suitable in this context are:\n\nA linear model does not output probabilities, and thus, interpreting its results in such a manner is incorrect.\nA linear model also extrapolates and gives you values below zero and above one1.\nSince the predicted outcome is not a probability, but a linear interpolation between points, there is no meaningful threshold at which you can distinguish one class from the other.\n\nConsider, for example, a problem where \\(Y\\) presents two distinct categories (represented here with values 0 and 1):\n\n\n\n\n\n\n\n\n\nEach observation will belong to one of these categories, and therefore present values of either \\(Y = 1\\) or \\(Y = 0\\).\nSuppose we tried to model the relationship between \\(x\\) and \\(Y\\) using a linear regression model. The following visualization provides a depiction of the results we would obtain through linear regression:\n\n\n\n\n\n\n\n\n\nThe problems mentioned above are clearly illustrated: how could we interpret predicted values below zero? How could we define a threshold to classify our predictions as belonging to each category?\n\n\n\nInstead of the linear equation we used when building a linear regression model, in this case we will make use of the logistic function, which constrains our output (the probability of \\(x\\) belonging to one of \\(Y\\)’s categories) between 0 and 1:\n\\[\nP(x)= \\frac{e^{\\beta_0 + \\sum\\limits_{j=1}^p \\beta_j X}}{1+e^{\\beta_0 + \\sum\\limits_{j=1}^p \\beta_j X}}\n\\]\nA logistic curve2 typically exhibits the following shape:\n\n\n\n\n\n\n\n\n\nBy employing this method, we effectively address the problem of obtaining probabilities outside the range of 0 to 1.\nFollowing a series of transformations, we can arrive at the following expression:\n\\[\n\\frac{P(x)}{1-P(x)}= e^{\\beta_0 + \\sum\\limits_{j=1}^p \\beta_j X}\n\\]\nThe quantity \\(\\frac{P(x)}{1-P(x)}\\) is called the odds (probability of event divided by probability of no event3), and can take on any value between \\(0\\) and \\(\\infty\\). Values of the odds close to \\(0\\) and \\(\\infty\\) indicate very low and very high probabilities of \\(x\\), respectively4.\nFinally, by taking the logarithm of both sides of our last expression, we arrive at:\n\\[\n\\log {\\frac{P(x)}{1-P(x)}}= \\beta_0 + \\sum\\limits_{j=1}^p \\beta_j X\n\\]\nThe left-hand side is the logarithm of the odds and is referred to as the log-odds or logit5.\nRecall from previous classes that in a linear regression model, \\(\\beta_{j}\\) indicates the average change in \\(Y\\) associated with a one-unit increase in \\(x_{j}\\). On the other hand, in a logistic regression model, increasing \\(x_{j}\\) by one unit changes the log-odds by \\(\\beta_{j}\\), as shown in the last formula presented (James et al. 2013).\nIn order to properly interpret the model’s coefficients, you should keep in mind that the relationship between \\(P(x)\\) and \\(x\\) in is not a straight line6: the amount that \\(P(x)\\) changes due to a one-unit change in \\(x\\) depends on the value of \\(x\\). However, a basic interpretation rule to remember is that, regardless of the value of \\(x\\) (James et al. 2013):\n\nif \\(\\beta_{j}\\) is positive then increasing \\(x_{j}\\) will be associated with increasing \\(P(x)\\)\nif \\(\\beta_{j}\\) is negative then increasing \\(x_{j}\\) will be associated with decreasing \\(P(x)\\)\n\nA graphical representation of the logistic curves we would obtain in simple logistic regressions with different values of \\(\\beta\\):\n\n\n\n\n\n\n\n\n\nOn the other hand, a change in a feature \\(x_{j}\\) by one unit (\\(\\Delta x_{j} = 1\\)) in a logistic regression results in a change in the log-odds of the event by an amount equal to the coefficient \\(\\beta_{j}\\) associated with that feature.\n\n\n\nOne can also use categorical predictors in the logistic regression model. However, in order to incorporate qualitative variables into regression analysis, we must first transform them into numerical values by creating dummy variables.\nLet’s start with the simplest case: a binary qualitative variable that can only take on two categories (e.g., a sex variable indicating whether a person is male or female7). In this case, we will assign a value of 1 to indicate the presence of the attribute represented by one of the variable’s possible values (for example: to be male), and 0 to represent the absence of the attribute (for example: not to be male). The group identified with a 0 is referred to as the base or reference category (in this case, females).\nFor a qualitative variable with \\(n\\) categories, we will need to create \\(n−1\\) dummy variables that take on values of 0 or 1 (the base category will be the same for all dummy variables).\nIn terms of coefficient interpretation, if the coefficient \\(\\beta_{j}\\) associated with the dummy variable is positive, the group identified by the dummy variable has a greater possibility of being classified as \\(Y=1\\) (that is to say, a bigger \\(P(x)\\)) than the reference category.\n\n\n\nHow can we test if the relationship between \\(x\\) and \\(Y\\) is statistically significant or not? We can formulate a hypothesis test with the following hypotheses (James et al. 2013):\n\\(H_{0}\\): “There is no relationship between \\(x\\) and \\(Y\\)”\n\\(H_{A}\\): “There is a relationship between \\(x\\) and \\(Y\\)”\nWe will consider the relationship between \\(x\\) and \\(Y\\) to be statistically significant when the p-value of the test is close to 0.\n\n\n\nThe logistic regression model involves a series of underlying assumptions regarding the variables involved. It is therefore fundamental to assess the following main assumptions before drawing conclusions from a regression analysis (Harrison and Pius 2020):\n\nBinary dependent variable (the number of outcomes or categories is two).\nIndependence of observations.\nLinearity of continuous independent variables and the log odds outcome.\nNo multicollinearity (that is to say, explanatory variables should not be highly correlated with each other)."
  },
  {
    "objectID": "materials/6_logistic_regression/1_class_notes.html#why-cant-we-use-a-linear-regression-in-a-classification-problem",
    "href": "materials/6_logistic_regression/1_class_notes.html#why-cant-we-use-a-linear-regression-in-a-classification-problem",
    "title": "Logistic Regression",
    "section": "",
    "text": "Drawing on the synthesis proposed by Molnar (2022), the main reasons why linear regression is not suitable in this context are:\n\nA linear model does not output probabilities, and thus, interpreting its results in such a manner is incorrect.\nA linear model also extrapolates and gives you values below zero and above one1.\nSince the predicted outcome is not a probability, but a linear interpolation between points, there is no meaningful threshold at which you can distinguish one class from the other.\n\nConsider, for example, a problem where \\(Y\\) presents two distinct categories (represented here with values 0 and 1):\n\n\n\n\n\n\n\n\n\nEach observation will belong to one of these categories, and therefore present values of either \\(Y = 1\\) or \\(Y = 0\\).\nSuppose we tried to model the relationship between \\(x\\) and \\(Y\\) using a linear regression model. The following visualization provides a depiction of the results we would obtain through linear regression:\n\n\n\n\n\n\n\n\n\nThe problems mentioned above are clearly illustrated: how could we interpret predicted values below zero? How could we define a threshold to classify our predictions as belonging to each category?"
  },
  {
    "objectID": "materials/6_logistic_regression/1_class_notes.html#formula.-coefficient-interpretation",
    "href": "materials/6_logistic_regression/1_class_notes.html#formula.-coefficient-interpretation",
    "title": "Logistic Regression",
    "section": "",
    "text": "Instead of the linear equation we used when building a linear regression model, in this case we will make use of the logistic function, which constrains our output (the probability of \\(x\\) belonging to one of \\(Y\\)’s categories) between 0 and 1:\n\\[\nP(x)= \\frac{e^{\\beta_0 + \\sum\\limits_{j=1}^p \\beta_j X}}{1+e^{\\beta_0 + \\sum\\limits_{j=1}^p \\beta_j X}}\n\\]\nA logistic curve2 typically exhibits the following shape:\n\n\n\n\n\n\n\n\n\nBy employing this method, we effectively address the problem of obtaining probabilities outside the range of 0 to 1.\nFollowing a series of transformations, we can arrive at the following expression:\n\\[\n\\frac{P(x)}{1-P(x)}= e^{\\beta_0 + \\sum\\limits_{j=1}^p \\beta_j X}\n\\]\nThe quantity \\(\\frac{P(x)}{1-P(x)}\\) is called the odds (probability of event divided by probability of no event3), and can take on any value between \\(0\\) and \\(\\infty\\). Values of the odds close to \\(0\\) and \\(\\infty\\) indicate very low and very high probabilities of \\(x\\), respectively4.\nFinally, by taking the logarithm of both sides of our last expression, we arrive at:\n\\[\n\\log {\\frac{P(x)}{1-P(x)}}= \\beta_0 + \\sum\\limits_{j=1}^p \\beta_j X\n\\]\nThe left-hand side is the logarithm of the odds and is referred to as the log-odds or logit5.\nRecall from previous classes that in a linear regression model, \\(\\beta_{j}\\) indicates the average change in \\(Y\\) associated with a one-unit increase in \\(x_{j}\\). On the other hand, in a logistic regression model, increasing \\(x_{j}\\) by one unit changes the log-odds by \\(\\beta_{j}\\), as shown in the last formula presented (James et al. 2013).\nIn order to properly interpret the model’s coefficients, you should keep in mind that the relationship between \\(P(x)\\) and \\(x\\) in is not a straight line6: the amount that \\(P(x)\\) changes due to a one-unit change in \\(x\\) depends on the value of \\(x\\). However, a basic interpretation rule to remember is that, regardless of the value of \\(x\\) (James et al. 2013):\n\nif \\(\\beta_{j}\\) is positive then increasing \\(x_{j}\\) will be associated with increasing \\(P(x)\\)\nif \\(\\beta_{j}\\) is negative then increasing \\(x_{j}\\) will be associated with decreasing \\(P(x)\\)\n\nA graphical representation of the logistic curves we would obtain in simple logistic regressions with different values of \\(\\beta\\):\n\n\n\n\n\n\n\n\n\nOn the other hand, a change in a feature \\(x_{j}\\) by one unit (\\(\\Delta x_{j} = 1\\)) in a logistic regression results in a change in the log-odds of the event by an amount equal to the coefficient \\(\\beta_{j}\\) associated with that feature."
  },
  {
    "objectID": "materials/6_logistic_regression/1_class_notes.html#categorical-predictors",
    "href": "materials/6_logistic_regression/1_class_notes.html#categorical-predictors",
    "title": "Logistic Regression",
    "section": "",
    "text": "One can also use categorical predictors in the logistic regression model. However, in order to incorporate qualitative variables into regression analysis, we must first transform them into numerical values by creating dummy variables.\nLet’s start with the simplest case: a binary qualitative variable that can only take on two categories (e.g., a sex variable indicating whether a person is male or female7). In this case, we will assign a value of 1 to indicate the presence of the attribute represented by one of the variable’s possible values (for example: to be male), and 0 to represent the absence of the attribute (for example: not to be male). The group identified with a 0 is referred to as the base or reference category (in this case, females).\nFor a qualitative variable with \\(n\\) categories, we will need to create \\(n−1\\) dummy variables that take on values of 0 or 1 (the base category will be the same for all dummy variables).\nIn terms of coefficient interpretation, if the coefficient \\(\\beta_{j}\\) associated with the dummy variable is positive, the group identified by the dummy variable has a greater possibility of being classified as \\(Y=1\\) (that is to say, a bigger \\(P(x)\\)) than the reference category."
  },
  {
    "objectID": "materials/6_logistic_regression/1_class_notes.html#statistical-significance",
    "href": "materials/6_logistic_regression/1_class_notes.html#statistical-significance",
    "title": "Logistic Regression",
    "section": "",
    "text": "How can we test if the relationship between \\(x\\) and \\(Y\\) is statistically significant or not? We can formulate a hypothesis test with the following hypotheses (James et al. 2013):\n\\(H_{0}\\): “There is no relationship between \\(x\\) and \\(Y\\)”\n\\(H_{A}\\): “There is a relationship between \\(x\\) and \\(Y\\)”\nWe will consider the relationship between \\(x\\) and \\(Y\\) to be statistically significant when the p-value of the test is close to 0."
  },
  {
    "objectID": "materials/6_logistic_regression/1_class_notes.html#model-assumptions",
    "href": "materials/6_logistic_regression/1_class_notes.html#model-assumptions",
    "title": "Logistic Regression",
    "section": "",
    "text": "The logistic regression model involves a series of underlying assumptions regarding the variables involved. It is therefore fundamental to assess the following main assumptions before drawing conclusions from a regression analysis (Harrison and Pius 2020):\n\nBinary dependent variable (the number of outcomes or categories is two).\nIndependence of observations.\nLinearity of continuous independent variables and the log odds outcome.\nNo multicollinearity (that is to say, explanatory variables should not be highly correlated with each other)."
  },
  {
    "objectID": "materials/6_logistic_regression/1_class_notes.html#confusion-matrix",
    "href": "materials/6_logistic_regression/1_class_notes.html#confusion-matrix",
    "title": "Logistic Regression",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nA confusion matrix is a fundamental tool used to assess the model’s performance in classification tasks. It provides a clear and concise representation of the model’s ability to make correct and incorrect predictions. The confusion matrix is a 2x2 matrix that categorizes the model’s predictions into four categories: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n\n\n\n\n\n\n\n\n\n\n\n\n**Predicted value**\n\n\n\n**Actual value**\n1\n0\n\n\n\n\n1\nTrue Positive\nFalse Negative\n\n\n0\nFalse Positive\nTrue Negative\n\n\n\nSource:  @james2013\n\n\n\n\n\n\n\n\n\n\nFor a dependent variable with two categories (1 and 0):\n\nTrue Positives are those observations where the model correctly predicts positive outcomes: \\(Y_{Act}=1\\) and \\(Y_{Pred}=1\\)\nTrue Negatives are those where it correctly predicts negative outcomes: \\(Y_{Act}=0\\) and \\(Y_{Pred}=0\\)\nFalse Positives are instances where it incorrectly predicts positive outcomes: \\(Y_{Act}=0\\) and \\(Y_{Pred}=1\\)\nFalse Negatives are instances where it incorrectly predicts negative outcomes: \\(Y_{Act}=1\\) and \\(Y_{Pred}=0\\)\n\nBy examining the values within the confusion matrix, we can gain insights into the model’s strengths and weaknesses, and therefore improve fundamental parameters such as the decision threshold.\nSome performance metrics such as the following can guide our analysis:\nSensitivity \\(= \\frac{TP}{P} = \\frac{TP}{TP + FN}\\)\nSpecificity \\(= \\frac{TN}{N} = \\frac{TN}{TN + FP}\\)\nFalse positive rate \\(= \\frac{FP}{N} = \\frac{FP}{TN + FP}\\)\nAccuracy \\(= \\frac{TN + TP}{N+P} = \\frac{TN + TP}{TN + FP + TN + FP}\\)\nPrecision \\(= \\frac{TP}{TP+FP}\\)\nNegative predictive value \\(= \\frac{TN}{TN+FN}\\)\nEach of these metrics will help us quantify the performance of our model by highlighting different strengths and weaknesses. For example, does the model excel at identifying true positives? (analyze sensitivity!) At the expense of an elevated rate of false positives? (analyze false positive rate!)"
  },
  {
    "objectID": "materials/6_logistic_regression/1_class_notes.html#other-representations",
    "href": "materials/6_logistic_regression/1_class_notes.html#other-representations",
    "title": "Logistic Regression",
    "section": "Other representations",
    "text": "Other representations\n\nROC curves\nThe ROC curve graphically illustrates two types of errors for all potential decision thresholds, depicting the trade-off that emerges when trying to correctly identify true positive cases while also avoiding incorrect predictions of the positive class when the actual class is negative. Sensitivity is depicted on the y-axis, and the false positive rate is represented on the x-axis (James et al. 2013).\n\n\n\n\n\n\n\n\n\nAn ideal ROC curve should closely follow the top-left corner of the visualization, signifying high sensitivity and a low false positive rate. The overall performance of a classifier is given by the area under the ROC curve (AUC). ROC curves will prove to be specially useful when comparing different classifiers, since they consider all possible thresholds (James et al. 2013).\n\n\nHosmer-Lemeshow plot\nThe Hosmer-Lemeshow plot is a graphical tool used to assess the goodness of fit of a logistic regression model. The basic principle behind this plot is that, within a group of observations, we can expect a good-fitting model to produce predicted probabilities that are similar to the observed proportion of \\(Y=1\\) values within that group (Nahhas, n.d.).\nThis assumption is tested by splitting the data into 10 groups based on their predicted probabilities and then, within groups, comparing the observed and expected proportions. The first group will contain the 10% of observations with the lowest predicted probabilities, the second the 10% with the next lowest, and so on. If the model fits well, the observed proportions of \\(Y=1\\) in these groups should be similar to their within-group average predicted probabilities (Nahhas, n.d.)."
  },
  {
    "objectID": "materials/6_logistic_regression/1_class_notes.html#footnotes",
    "href": "materials/6_logistic_regression/1_class_notes.html#footnotes",
    "title": "Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRemember that probabilities can never fall outside the range of 0 to 1!↩︎\nTo be more precise: a sigmoid curve.↩︎\nFor example, odds of 4 would mean it is 4 times more likely for an event to happen than for it not to happen.↩︎\nFor example: For \\(P(x) = 0.2\\): \\(\\frac{P(x)}{1-P(x)} = \\frac{0.2}{0.8} = 0.25\\) For \\(P(x) = 0.8\\): \\(\\frac{P(x)}{1-P(x)} = \\frac{0.8}{0.2} = 4\\) For \\(P(x) = 0.98\\): \\(\\frac{P(x)}{1-P(x)} = \\frac{0.98}{0.02} = 49\\)↩︎\nThis expression highlights the fact that the logistic regression model has a logit that is linear in \\(x\\) (James et al. 2013).↩︎\nSee the first equation of this section, where we first presented the relationship between \\(P(x)\\) and \\(x\\).↩︎\nThis would be a dataset that omits or invisibilizes, for example, intersex people.↩︎\nA function used to convert a vector of \\(K\\) real numbers into a probability distribution of \\(K\\) possible outcomes.↩︎\nFundamentally, this will depend on the type of error we want to avoid. Consider, for example, a model that predicts whether a tumor is benign or malignant. Misclassifying a malignant tumor as benign is much more detrimental to the patient’s health than the reverse scenario. Therefore, in this case, instead of classifying tumors with a probability of 0.5 or higher of being malignant as malignant, we might classify those with a probability greater than 0.1 of being malignant as malignant.↩︎"
  },
  {
    "objectID": "materials/20_SNA/1_class_notes.html",
    "href": "materials/20_SNA/1_class_notes.html",
    "title": "Social network analysis",
    "section": "",
    "text": "Networks are complex structures used to represent relations. In math they are studied as graphs, but they can be used in multiple ways. As a way to represent knowledge, to define a model, as database systems, or to represent information. Many machine learning models are built as graphs: Neural networks, decision trees, and even causal models. In this course, we will focus on networks for social science. Here, we will use graphs to represent data about social relations between entities (people, institutions, countries, etc.)."
  },
  {
    "objectID": "materials/20_SNA/1_class_notes.html#weighted-or-unweighted-networks",
    "href": "materials/20_SNA/1_class_notes.html#weighted-or-unweighted-networks",
    "title": "Social network analysis",
    "section": "Weighted or unweighted networks",
    "text": "Weighted or unweighted networks\nThe relation between two nodes can either be dichotomous or quantitative. For example, a link can represent co-authorship – given two authors, they either co-authored a paper or not– and for it we might use an unweighted network. But a link can also represent the number of papers two co-authors wrote together, and in this case we might use a weighted network. A weighted network can be represented as an unweighted one for those relations above a given threshold."
  },
  {
    "objectID": "materials/20_SNA/1_class_notes.html#directed-or-undirected-networks",
    "href": "materials/20_SNA/1_class_notes.html#directed-or-undirected-networks",
    "title": "Social network analysis",
    "section": "Directed or undirected networks",
    "text": "Directed or undirected networks\nConnections can have a direction5. For example, in Twitter, B can follow A, who follows C, that also follows A back. While in Facebook, the connections are defined as friendship, where there is no direction."
  },
  {
    "objectID": "materials/20_SNA/1_class_notes.html#bipartite-networks",
    "href": "materials/20_SNA/1_class_notes.html#bipartite-networks",
    "title": "Social network analysis",
    "section": "Bipartite networks",
    "text": "Bipartite networks\nNetworks can encode complex relations between different types of entities. For example, if we want to represent people that belongs to different institutions, we can have nodes that represent people {\\(A_1,A_2,...,A_5\\)}, and other type of nodes that represent institutions {\\(I_1,I_2,...,I_4\\)}, and here the links represent the relation of belonging.\n\nThis type of networks can also be simplified using projection, where we keep only one type of node. In the example above, we can relate institutions to each other if they have people in common, and link people if they belong to the same institution:"
  },
  {
    "objectID": "materials/20_SNA/1_class_notes.html#dynamic-networks",
    "href": "materials/20_SNA/1_class_notes.html#dynamic-networks",
    "title": "Social network analysis",
    "section": "Dynamic networks",
    "text": "Dynamic networks\nIf we want to use network analysis to study a social process, a static representation can fall short to understand how the social interactions evolve over time. For this, we can also think networks as a dynamic process, where new nodes enter, old nodes disappear, and links change over time."
  },
  {
    "objectID": "materials/20_SNA/1_class_notes.html#node-level-metrics",
    "href": "materials/20_SNA/1_class_notes.html#node-level-metrics",
    "title": "Social network analysis",
    "section": "Node level metrics",
    "text": "Node level metrics\nA key concept in network analysis is centrality. It refers to the importance of a node within the network. Generally speaking, if a node is well connected, it can exercise more influence over the network.\n\nLet’s imagine our network is composed of a group of 10 people. Links represent friendship and trust between them. If we want to know how fake news are spread across the network, centrality measures can help us to understand what would happen if any of the nodes start a rumor8.\n\n\nWho do you think that can be more effective spreading the misinformation? Node 4, 7 or 2?\n\nThere are many ways to define the influence of a node, and which one is the best depends on the research question.\n\nDegree centrality counts the number of connections of a node. It is one of the most common metrics.\n\n\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 10 × 2\n    node degree\n   &lt;int&gt;  &lt;dbl&gt;\n 1     1      3\n 2     2      1\n 3     3      2\n 4     4      4\n 5     5      4\n 6     6      3\n 7     7      4\n 8     8      2\n 9     9      3\n10    10      2\n\n\nWe can see that by degree centrality nodes 4, 5, and 7 are considered equally important.\n\nCloseness centrality is the average closeness9 between the node and all the other nodes in the graph. The intuition is that the closer it is to all other nodes, the more central a node is.\n\n\n\n\n\n\n\n\n\n\n\\[\ncloseness(i) = \\frac{N-1}{\\sum_{j\\neq i}d_{i,j}}\n\\]\n\n\n# A tibble: 10 × 2\n    node closeness\n   &lt;int&gt;     &lt;dbl&gt;\n 1     5     0.067\n 2     7     0.067\n 3     4     0.059\n 4     6     0.059\n 5     1     0.056\n 6     9     0.05 \n 7    10     0.048\n 8     8     0.045\n 9     3     0.042\n10     2     0.034\n\n\nWhen we consider closeness, node 4 is not as important as in degree centrality. As it is not connected to node 7, its distance to nodes 10 and 2 is rather long.\n\nBetweenness centrality measures a specific type of influence over the network. It counts how many times a node is in the shortest path of other nodes. The influence here implies that, in order to get from one point of the network to another, you must go through this specific node. A central node in this metric behaves as a bridge between others. It can be formally defined as:\n\n\\[\nbetweenness(i)= \\sum_{j,k\\neq i}\\frac{b_{jik}}{b_{jk}}\n\\]Where \\(b_{jk}\\) are all the shortest paths between nodes \\(j\\) and \\(k\\), and \\(b_{jik}\\) are all of those shortest paths that go through \\(i\\).\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 10 × 2\n    node betweenness\n   &lt;int&gt;       &lt;dbl&gt;\n 1     7       16.7 \n 2     5       10.2 \n 3     4        8.83\n 4    10        8   \n 5     6        7   \n 6     1        1.83\n 7     9        1.5 \n 8     2        0   \n 9     3        0   \n10     8        0   \n\n\nFor betweenness, the node 7 is the most important by far, as it is the only bridge to nodes 10 and 2, and one of the two connections between nodes {5, 1, 8} and nodes {6, 9, 4, 3}.\n\nEigen centrality is based on the idea that not only does it matter how many connections a node has, but also the centrality of the nodes with which the node is connected. A node can be connected to many others, but if those nodes are only connected to the first one, the influence of the node in the network will be limited.\n\nThis is a recursive problem, as the centrality can be defined as\n\\[\nx_i = \\kappa^{-1} \\sum_{\\text{nodes j connected to i}}x_j\n\\]\nThe centrality of the node \\(x_i\\) is some proportion of the centrality of its neighbors (\\(\\kappa\\) being a proportionality constant).\nWe can use the adjacency matrix \\(A\\) to redefine this problem, given that if the nodes are connected, \\(A_{ij}\\) is 1, and 0 otherwise.\n\\[\nx_i = \\kappa^{-1} \\sum_{j}^nA_{ij}x_j\n\\] It is called eigen centrality, because the solution to this equation is the eigen vector of the adjacency matrix10.\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 10 × 2\n    node eigen_centrality\n   &lt;int&gt;            &lt;dbl&gt;\n 1     5             1   \n 2     4             0.96\n 3     7             0.91\n 4     6             0.81\n 5     1             0.77\n 6     9             0.72\n 7     8             0.56\n 8     3             0.52\n 9    10             0.32\n10     2             0.1 \n\n\nWhen we consider eigen centrality, node 5 is more important than node 7. This happens because node 5 is connected to both nodes 4 and 7, which are the two following nodes in importance, while nodes 4 and 7 are not connected between them."
  },
  {
    "objectID": "materials/20_SNA/1_class_notes.html#network-level-metrics",
    "href": "materials/20_SNA/1_class_notes.html#network-level-metrics",
    "title": "Social network analysis",
    "section": "Network level metrics",
    "text": "Network level metrics\n\nSize: refers to the number of nodes or links in the network.\nThe diameter is the distance between the two most distant pair of nodes11 in the network.\nDensity/sparsity: the density of the network is the ratio between the number of existing edges and all the possible edges that a network with the same number of nodes can have.\nThe average degree is simply the average degree centrality of all nodes.\nClustering coefficient: it shows how much the nodes are locally cohesive. Each node has a neighborhood that consists of all the nodes that are connected to it. The clustering coefficient shows how much the nodes in the same neighborhood are also connected between themselves. It is computed for each node as the ratio between the existing connections among its neighbors, and the total possible connections. After this, we can average the clustering coefficient of all nodes to get the value for the network level.\nAssortativity measures how much nodes with a high degree tend to be connected between each other. It can be operationalized as the degree-correlation among neighbors. Do central nodes tend to connect more to other central nodes? or they tend to relate with less central nodes?"
  },
  {
    "objectID": "materials/20_SNA/1_class_notes.html#random-networks",
    "href": "materials/20_SNA/1_class_notes.html#random-networks",
    "title": "Social network analysis",
    "section": "Random networks",
    "text": "Random networks\nNetworks are complex structures. There are no simple tests like in descriptive statistics that we can use to check if a network follows a property, but random networks can be used to compare some properties. Random networks are generative models, where we can specify some parameters like the number of nodes, edges, or the probability of a link between nodes, and a network is built following a specific algorithm."
  },
  {
    "objectID": "materials/20_SNA/1_class_notes.html#preferential-attachment-model",
    "href": "materials/20_SNA/1_class_notes.html#preferential-attachment-model",
    "title": "Social network analysis",
    "section": "Preferential attachment model",
    "text": "Preferential attachment model\nThese models can also help us infer which are the evolutive properties of a network. For example, the preferential attachment model iteratively adds new nodes to the network. This new nodes have a probability to connect with old nodes that is given by the degree of the old nodes. This iterative process recreates the rich gets richer effect.\nOne type of preferential attachment model is the Barabási-Albert (BA) model. The algorithm of the BA model works as follows:\n\nThe network begins with an initial connected network with \\(n\\) nodes.\nNew nodes are added to the network, one at a time. Each new node will connect with the previous nodes in the network with the probability:\n\n\\[\np_{i}\\sim k_i^\\alpha\n\\]Where \\(k_i\\) is the degree of the node \\(i\\) and \\(\\alpha\\) is a parameter that defines the power of the preferential attachment.\nLet’s see how this process looks like:\n\n\nWhat effects does a cumulative process have?\n\nLet’s see the degree distribution of such type of networks:\n\n\n\n\n\n\n\n\n\nThe centrality follows a power-law distribution, where a few nodes have a really high centrality, while the great majority has a low centrality."
  },
  {
    "objectID": "materials/20_SNA/1_class_notes.html#small-world",
    "href": "materials/20_SNA/1_class_notes.html#small-world",
    "title": "Social network analysis",
    "section": "Small world",
    "text": "Small world\nAnother property that is commonly found in real-world networks is the small world phenomena: as networks get bigger in number of nodes, the average distance of the network only increases mildly. Huge networks with millions of nodes have very short distances between any pair of nodes, even if it is sparsely connected12.\nThis happens because of a combination of clustering and power-law degree distributions: there are a few highly connected nodes that guarantee the more distant connections, and then the densely connected neighborhoods easily connect the node with that highly connected node in its neighborhoods."
  },
  {
    "objectID": "materials/20_SNA/1_class_notes.html#communities",
    "href": "materials/20_SNA/1_class_notes.html#communities",
    "title": "Social network analysis",
    "section": "Communities",
    "text": "Communities\nDetecting communities within a network is a common task in SNA.\nFinding Twitter communities, for example, can be the key for understanding the patterns of political polarization (Conover et al. 2011). A community in a network is a sub-graph in which the nodes are densely connected within them, and sparsely connected to nodes outside their community.\nIt is the same concept as clustering, but the techniques used to detect them are different, given the particular structure of network’s data.\nModularity (\\(Q\\)) (Newman 2006) is an important metric for community detection. It shows the extent to which the number of links between a group of nodes is greater (\\(Q&gt;0\\)) or smaller (\\(Q&lt;0\\)) than expected at random in that graph.\n\nOne of the many community detection algorithms is the Louvain algorithm (Blondel et al. 2008), that maximizes modularity to find the most distinctive communities. Given that this results in very small communities, the algorithm iteratively merges communities by considering its nodes as a unit.\n\nLet’s see how our networks looks like if we build communities using Louvain algorithm:\n\n\n\n\n\n\n\n\n\n\nOne of the most important problems of community detection in real-world networks is how to do it efficiently. Although the intuition of what we want as communities can be simple, the way to find them in a fast-enough way is complex. The walktrap algorithm (Pons and Latapy 2005) is a creative solution for this: By using random walks, it computes the distance between nodes, and uses those distances to build a hierarchical clustering.\n\n\nPlaceholder (you should not see this)"
  },
  {
    "objectID": "materials/20_SNA/1_class_notes.html#homophily",
    "href": "materials/20_SNA/1_class_notes.html#homophily",
    "title": "Social network analysis",
    "section": "Homophily",
    "text": "Homophily\nOne of the main findings across several studies is the concept of homophily: the idea that people tend to relate more to others that they perceive similar to them in some way.\nA great example is the political homophily: people tend to relate more with people that share their political beliefs. For example, the figure below shows the Twitter network of Argentina.\n\n\n\nTwitter Argentina (González 2016)\n\n\nTwitter US also shows a similar split between democrats and republicans.\n\n\n\nTwitter US (Ribeiro et al., n.d.)\n\n\nTo measure homophily, we can also use the metric of modularity, but instead of measuring the groups built as a community detection algorithm, we use it with taking the nodes grouped by an attribute attached to each one of them (e.g. democrat or republican)."
  },
  {
    "objectID": "materials/20_SNA/1_class_notes.html#why-does-the-rich-get-richer",
    "href": "materials/20_SNA/1_class_notes.html#why-does-the-rich-get-richer",
    "title": "Social network analysis",
    "section": "Why does the rich get richer?",
    "text": "Why does the rich get richer?\nThe preferential attachments mechanism for building networks is an interesting model to rethink inequalities. The time component allows to think how if a small difference happens repeatedly over time, and has a cumulative nature, it can create an outcome with extreme inequalities. This model also shows that those who enter first to the system of accumulation already have a unbeatable advantage with respect to those that enter afterwards.\n\nIncome has such cumulative mechanism. Money makes money. If a group of the population was systematically deprived for generations, even if there seems to be equal opportunities at some point in time, the cumulative nature of income will continue to affect that group. This is why colorblind policies are not enough, and without restorative actions reaching equality is impossible."
  },
  {
    "objectID": "materials/20_SNA/1_class_notes.html#re-discussing-homophily",
    "href": "materials/20_SNA/1_class_notes.html#re-discussing-homophily",
    "title": "Social network analysis",
    "section": "Re-discussing homophily",
    "text": "Re-discussing homophily\nHomophily is one of the biggest sociological ideas that came up from network studies. A classical example has always been the friendship network in a US high school, which shows how black and white kids tend to play with other kids that share their identity (Bearman, Moody, and Stovel 2002). Nevertheless, this concept is also problematic, and without enough care it can foster misleading conclusions.\n\nWhen we talk about identities, there are privileged and excluded populations. The concept of homophily equas all groups to a single dynamic, when in reality there are different phenomena happening for different groups.\nThe empirical observation of a high modularity needs to be properly contextualized. An extreme example would be the residential segregation in US. In the 30’s, the Federal Housing Administration made public housing projects exclusively for white people. If we would measure the racial segregation by neighborhoods at that time, we could think that both black and white people showed homophily behaviors. But black people did not had a choice, as they were excluded from white neighborhoods. The reasons behind the behavior of each group are strikingly different, and modularity alone cannot capture that complexity."
  },
  {
    "objectID": "materials/20_SNA/1_class_notes.html#footnotes",
    "href": "materials/20_SNA/1_class_notes.html#footnotes",
    "title": "Social network analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe differences in the vocabulary are due to the usages of the different scientific communities that use networks. Graph theory is a sub-field of math, while networks are used in other disciplines like computer science or sociology.↩︎\nThere can be more than one shortest path between a pair of nodes.↩︎\nIf there is no path between the two nodes, we can say that the distance is \\(\\infty\\).↩︎\nWe can also build the ego network that contains all the nodes at a distance of 2, 3, etc.↩︎\nThis changes a little the definition of walks and paths, as they now need to follow the direction of links. Also the distance between two nodes can be asymmetrical.↩︎\nHow nodes and links are arranged globally.↩︎\nIn weighted networks, 0 and 1 can be replaced with any continuous scale.↩︎\nThis same kind of analysis can be made for the spread of diseases, political beliefs, among many different research questions.↩︎\nCloseness is the opposite of the length of the shortest path (1/distance)↩︎\nIn algebra this is defined as \\(Ax=\\lambda x\\), where \\(\\lambda\\) is the eigen value and \\(x\\) the eigen vector that solve this equation. For the scope of this class, we can just think that this gives us the solution of our recursive problem.↩︎\nThe distance between two pair of nodes is their shortest paths, so the diameter of the network is the largest of all the shortest paths.↩︎\n“Six degrees of separation”: is the idea that all people are six or fewer social connections away from each other. Although this number is somehow forced, in Facebook the average distance between any pair of users is 5.73, and in twitter 4.67.↩︎"
  },
  {
    "objectID": "materials/10_tidy_dataviz/1_class_notes.html",
    "href": "materials/10_tidy_dataviz/1_class_notes.html",
    "title": "Tidyverse and dataviz",
    "section": "",
    "text": "One of the main advantages that R has as a software for statistic analysis is its incremental syntax. This means that the things you can do in R are constantly updated and expanded through the creation of new packages, developed by researchers, users or private companies.\n\n\n\nSource: Gergely Daróczi\n\n\nThese packages contain code, data, and documentation in a standardized collection that can be installed by users of R. Most of the time, we will install them in order to use functions that will do certain tasks that help us work with our data. So far, we were using functions contained in R base: such as mean(), median(), quantile(), etc. But as we dive deep into the data science life cycle, we might address certain challenges that require more complex, or specific, functions. We will need to import the data, tidy the data into a format that is easy to work with, explore the data, generate visualizations, carry out the analysis and communicate the insights. The tidyverse ecosystem provides a powerful tool for streamlining the workflow in a coherent manner that can be easily connected with other data science tools."
  },
  {
    "objectID": "materials/10_tidy_dataviz/1_class_notes.html#packages",
    "href": "materials/10_tidy_dataviz/1_class_notes.html#packages",
    "title": "Tidyverse and dataviz",
    "section": "",
    "text": "One of the main advantages that R has as a software for statistic analysis is its incremental syntax. This means that the things you can do in R are constantly updated and expanded through the creation of new packages, developed by researchers, users or private companies.\n\n\n\nSource: Gergely Daróczi\n\n\nThese packages contain code, data, and documentation in a standardized collection that can be installed by users of R. Most of the time, we will install them in order to use functions that will do certain tasks that help us work with our data. So far, we were using functions contained in R base: such as mean(), median(), quantile(), etc. But as we dive deep into the data science life cycle, we might address certain challenges that require more complex, or specific, functions. We will need to import the data, tidy the data into a format that is easy to work with, explore the data, generate visualizations, carry out the analysis and communicate the insights. The tidyverse ecosystem provides a powerful tool for streamlining the workflow in a coherent manner that can be easily connected with other data science tools."
  },
  {
    "objectID": "materials/10_tidy_dataviz/1_class_notes.html#tidy-data",
    "href": "materials/10_tidy_dataviz/1_class_notes.html#tidy-data",
    "title": "Tidyverse and dataviz",
    "section": "Tidy data",
    "text": "Tidy data\nAll packages share an underlying design philosophy, grammar, and data structures. These structures refer to the format of tidy datasets. But, what is tidy data? It is a way to describe data that’s organized with a particular structure: a rectangular structure, where each variable has its own column and each observation has its own row (Wickham 2014a; Peng n.d.).\n\n\n\nJulie Lowndes and Allison Horst\n\n\nAccording to (Wickham 2014b),\n\n“Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.”\n\nWorking with tidy data means that we work with information that has a consistent data structure. The main benefit behind this is that we will have to spend less time thinking how to process and clean data, because we can use existing tools instead of starting from scratch each time we work with a new dataset. In other words, we only require a small set of tools to be learned, since we can reuse them from one project to the other.\n\n\n\nJulie Lowndes and Allison Horst"
  },
  {
    "objectID": "materials/10_tidy_dataviz/1_class_notes.html#tidyverse-ecosystem",
    "href": "materials/10_tidy_dataviz/1_class_notes.html#tidyverse-ecosystem",
    "title": "Tidyverse and dataviz",
    "section": "tidyverse ecosystem",
    "text": "tidyverse ecosystem\nWe can think of the tidyverse ecosystem (Grolemund n.d.) as the set of tools we can reuse in our tidy data. It is an ecosystem because consists in a set of various packages that can be installed in one line of code (install.packages(tidyverse)), and each package fits into a part of the data science life cycle. This is the main reason why we prefer to use tidyverse instead of R base. It provides us with a consistent set of tools we can use for many different datasets, it is designed to keep our data tidy, and it contains all the specific tools we might need in our data science workflow.\n\n\n\nTidyverse\n\n\nThere is a set of core tidyverse packages that are installed with the main ecosystem, which are ones you are likely to use in everyday data analyses.\n\ntibble: it is a package that re-imagines the familiar R data.frame. It is a way to store information in columns and rows, but does so in a way that addresses problems earlier in the pipeline. That is to say, it stores it in the tidy data format. The official documentation calls tibbles ‘lazy and slurly’, since they do less (they don’t change variable names or types, and don’t do partial matching) and complain more (e.g. when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code.\nreadr: this is a package we will use every time we start a new project. It helps read rectangular data into R, and it includes data in .csv and .tsv format. It is designed to flexibly parse many types of data found.\n\nto read data in .xlsx or .xlx format, you should install the tidyverse-adjacent package readxl.\n\ndplyr: designed for data wrangling. It is built around five primary verbs (mutate, select, filter, summarize, and arrange) that help make the data wrangling process simpler.\ntidyr: it is quite similar to dplyr, but its main goal is to provide a set of functions to help us convert dataframes into tidy data.\npurr: enhances R’s functional programming toolkit by providing a complete and consistent set of tools for working with functions and vectors. It makes easier for us to work with loops inside a dataframe.\nstringr: it is designed to help us work with data in string format.\nforcats: it provides functions to help us work with data in the factor format.\nggplot: the main package for data visualization in the R community. It is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.\n\nWhile this is the main structure of the tidyverse ecosystem, there are multiple adjacent packages that we can install which fit into the tidy syntax and work well with the tidy data format. Some of them are readxl to read data in .xlsx or .xl format, janitor for cleaning data, patchwork to paste ggplot graphs together, tidymodels for machine learning… and many more."
  },
  {
    "objectID": "materials/10_tidy_dataviz/1_class_notes.html#tidy-data-functions",
    "href": "materials/10_tidy_dataviz/1_class_notes.html#tidy-data-functions",
    "title": "Tidyverse and dataviz",
    "section": "Tidy data functions",
    "text": "Tidy data functions\nthere are some basic functions that we use to tidy data:\n\nmutate: to transform columns,\nselect: to select certain columns,\nfilter: to select certain rows (we can think of filter as the row-wise version of select, and select as the column-wise version of filter),\narrange: to reorder values of rows.\n\nNow, we will go over the basics of more complex data transformation functions to reshape the data: joins, pivots, summarizes and date processing.\nFor example if we have a survey on households\n\nhousehold\n\n# A tibble: 7 × 3\n  household_ID neighborhood_income n_people_household\n         &lt;int&gt; &lt;chr&gt;                            &lt;dbl&gt;\n1            1 low income                           2\n2            2 low income                           1\n3            3 high income                          1\n4            4 high income                          1\n5            5 low income                           5\n6            6 high income                          4\n7            7 high income                          3\n\nhousehold |&gt; \n  filter(n_people_household&gt;2)\n\n# A tibble: 3 × 3\n  household_ID neighborhood_income n_people_household\n         &lt;int&gt; &lt;chr&gt;                            &lt;dbl&gt;\n1            5 low income                           5\n2            6 high income                          4\n3            7 high income                          3\n\nhousehold |&gt; \n  filter(n_people_household&gt;2) |&gt; \n  mutate(neighborhood_income = toupper(neighborhood_income))\n\n# A tibble: 3 × 3\n  household_ID neighborhood_income n_people_household\n         &lt;int&gt; &lt;chr&gt;                            &lt;dbl&gt;\n1            5 LOW INCOME                           5\n2            6 HIGH INCOME                          4\n3            7 HIGH INCOME                          3\n\nhousehold |&gt; \n  filter(n_people_household&gt;2) |&gt; \n  mutate(neighborhood_income = toupper(neighborhood_income)) |&gt; \n  select(neighborhood_income,n_people_household)\n\n# A tibble: 3 × 2\n  neighborhood_income n_people_household\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 LOW INCOME                           5\n2 HIGH INCOME                          4\n3 HIGH INCOME                          3\n\n\n\nMerges of dataframes\nIf you ever worked with SQL or multiple databases in the past, you might already be familiar with the concept of joining data. A tidy dataset should contain one type of observational unit per table. For example, if we have done a survey regarding labor conditions to individuals, but we also have sociodemographical information regarding their household, we should have each information in a different dataset. However, we will probably be interested in crossing the information regarding the individuals and their household conditions. So we need to have key IDs to be able to combine these two datasets: for example, in this case, the ID for the household.\n\nNote that the household ID is not the same as the person ID. Each person has a unique identifier, but so does each household. The information is consistent: when we have two individuals who share a household ID, we see in the household information that there are two people living there.\nNow, how do dataframe joins work?\n\n\n\nNiloy Biswas\n\n\n\nInner join: this creates a new dataset that only contains the information of rows where the IDs match. For example, in our case, the data of the household 5 wouldn’t appear since it doesn’t join any individual data.\n\n\ninner_join &lt;- individual %&gt;% inner_join(household)\ninner_join\n\n# A tibble: 7 × 7\n  person_ID household_ID gender   age income neighborhood_income\n      &lt;int&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;              \n1         1            1 f         33   4222 low income         \n2         2            1 f         47   3378 low income         \n3         3            2 m         36   3299 low income         \n4         4            3 m         34   4169 high income        \n5         5            4 m         30   4664 high income        \n6         6            6 f         38   1762 high income        \n7         7            7 f         32   3316 high income        \n# ℹ 1 more variable: n_people_household &lt;dbl&gt;\n\n\n\nLeft join: it keeps all the rows of the data on the “left” and adds the columns that match the dataframe on the right. The decision of which dataframe goes where (left or right) is arbitrary and up to us, but we must keep in mind that it will be our main dataframe in the join. In our example, if we chose the individual dataset as the left, we would have the same table as the result of the inner join. But if we chose the household dataset, we would have a dataset with empty values for the ID’s that don’t match.\n\n\nleft_join_house &lt;- household %&gt;% left_join(individual)\nleft_join_house\n\n# A tibble: 8 × 7\n  household_ID neighborhood_income n_people_household person_ID gender   age\n         &lt;dbl&gt; &lt;chr&gt;                            &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n1            1 low income                           2         1 f         33\n2            1 low income                           2         2 f         47\n3            2 low income                           1         3 m         36\n4            3 high income                          1         4 m         34\n5            4 high income                          1         5 m         30\n6            5 low income                           5        NA &lt;NA&gt;      NA\n7            6 high income                          4         6 f         38\n8            7 high income                          3         7 f         32\n# ℹ 1 more variable: income &lt;dbl&gt;\n\n\n\nFull join: it keeps all the columns and all the rows in both dataframes.\n\n\nout_join &lt;- household %&gt;% full_join(individual)\nout_join\n\n# A tibble: 8 × 7\n  household_ID neighborhood_income n_people_household person_ID gender   age\n         &lt;dbl&gt; &lt;chr&gt;                            &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n1            1 low income                           2         1 f         33\n2            1 low income                           2         2 f         47\n3            2 low income                           1         3 m         36\n4            3 high income                          1         4 m         34\n5            4 high income                          1         5 m         30\n6            5 low income                           5        NA &lt;NA&gt;      NA\n7            6 high income                          4         6 f         38\n8            7 high income                          3         7 f         32\n# ℹ 1 more variable: income &lt;dbl&gt;\n\n\n\n\nSummarizing information\nMany times, we will want to summarize the information in our dataset. We can catch a glimpse of the summarized dataset with functions such as summary() or str(). However, more often, we will want to see specific information regarding groups in our dataset. For example, how many households we have in the different types of neighborhoods. To do this, it is necessary to group our data. We will not look into the total number of households, but we will group the data by the neighborhood of the households. Then, we can count each group.\n\nhousehold %&gt;% group_by(neighborhood_income) %&gt;%\n  summarise(n=n())\n\n# A tibble: 2 × 2\n  neighborhood_income     n\n  &lt;chr&gt;               &lt;int&gt;\n1 high income             4\n2 low income              3\n\n\nOther times, we might be interested in getting descriptive statistics per group. For example, the median age for men and women in the individuals dataset.\n\nindividual %&gt;% group_by(gender) %&gt;%\n  summarise(median_age = median(age))\n\n# A tibble: 2 × 2\n  gender median_age\n  &lt;chr&gt;       &lt;dbl&gt;\n1 f            35.5\n2 m            34  \n\n\nOr even combine groups and subgroups. For example, the median income for women and men who live in low income or high income neighborhoods\n\nindividual %&gt;% \n  left_join(household) %&gt;%\n  group_by(neighborhood_income, gender) %&gt;%\n  summarise(income = median(income))\n\n# A tibble: 4 × 3\n# Groups:   neighborhood_income [2]\n  neighborhood_income gender income\n  &lt;chr&gt;               &lt;chr&gt;   &lt;dbl&gt;\n1 high income         f       2539 \n2 high income         m       4416.\n3 low income          f       3800 \n4 low income          m       3299 \n\n\n\n\nReshaping data\nConverting your data from wide-to-long or from long-to-wide data formats is referred to as reshaping your data.\nFor example, take this subset of columns from our individuals dataset.\n\nindividual %&gt;% select(person_ID, gender, age, income)\n\n# A tibble: 7 × 4\n  person_ID gender   age income\n      &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1         1 f         33   4222\n2         2 f         47   3378\n3         3 m         36   3299\n4         4 m         34   4169\n5         5 m         30   4664\n6         6 f         38   1762\n7         7 f         32   3316\n\n\nThis is what we call a wide format: each variable has its own column, and each row represents a single observation. Long data, on the other hand, refers to a dataset where each variable is contained in its own column. This format is often used when working with large datasets or when performing statistical analyses that require data to be presented in a more condensed format.\nThis is the same dataset we had previously but reshaped as long data.\n\nindividual %&gt;% \n  select(person_ID, gender, age, income) %&gt;% \n  pivot_longer(cols = -c(person_ID, gender), names_to = \"variable\", values_to = \"value\" )\n\n# A tibble: 14 × 4\n   person_ID gender variable value\n       &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n 1         1 f      age         33\n 2         1 f      income    4222\n 3         2 f      age         47\n 4         2 f      income    3378\n 5         3 m      age         36\n 6         3 m      income    3299\n 7         4 m      age         34\n 8         4 m      income    4169\n 9         5 m      age         30\n10         5 m      income    4664\n11         6 f      age         38\n12         6 f      income    1762\n13         7 f      age         32\n14         7 f      income    3316\n\n\nAs you can see, the wide data format has a separate column for each variable, whereas the long data format has a single “variable” column that indicates whether the value refers to age or income."
  },
  {
    "objectID": "materials/10_tidy_dataviz/1_class_notes.html#data-visualization",
    "href": "materials/10_tidy_dataviz/1_class_notes.html#data-visualization",
    "title": "Tidyverse and dataviz",
    "section": "Data visualization",
    "text": "Data visualization\nVisualization is a key part of the data analysis workflow. Not only does it help us explore our data and analysis results, but it also serves as the primary tool for sharing our findings with others. Whether we’re presenting our work in an academic paper, journalistic article, or a presentation to colleagues, graphs are essential to attract their attention and making our work interesting. This is why it is important to learn how to make graphs that display information in an effective but also aesthetically pleasing way.\n\nGrammar of graphics\nDoing a good graphic involves data and creativity. However, there are certain common elements behind the creation of a graph that we can sum up as the grammar of graphics Wickham (2010). Graphics are constructed by combining independent components, such as data, aesthetics, geometries, and scales.\nThe package we will be using to make plots in R is called ggplot2, and its syntax is based on this grammar of graphics. In ggplot2, a plot is constructed by starting with a layer of data, and then adding additional layers to specify the aesthetics (e.g., color, shape, size) of the plot, the type of geometry (e.g., point, line, bar) to use to display the data, and the scale (e.g., linear, log, discrete) to use for each aesthetic. Each of these components can be modified and customized in a variety of ways to create complex and informative visualizations.\n\n\n\nJoe Roe\n\n\nThis process of assigning data variables to the visual properties of a plot is called mapping of aesthetic attributes. In other words, it is the way to show in a visually perceptible way the difference between values. Aesthetics refer to visual properties such as color, shape, size, transparency, and position that can be used to visually represent data.\n\n\n\nCommonly used aesthetics in data visualization: position, shape, size, color, line width, line type. Some of these aesthetics can represent both continuous and discrete data (position, size, line width, color) while others can usually only represent discrete data (shape, line type) (Wilke n.d.a)\n\n\nThe grammar of graphics approach allows users to create a wide range of plots, from basic scatter plots and histograms to complex multi-layered visualizations, using a consistent and intuitive syntax. It is a powerful tool for data exploration and communication and has become a popular approach for data visualization in the R community.\n\n\nTypes of graphs\nWe can group plots into various clusters according to the kind of data we want to show. We will often be interested in showing the magnitude of numbers: the total number of people living in different countries, the mean salary for different groups of populations. These cases have a set of categorical values (such as countries, or demographic groups) and a quantitative value for each category. This is the same as saying we are going to show amounts (Wilke (n.d.b)). The most common graph to showcase amounts are bar plots. For example, we can display the countries with lowest life expectancy in the year 2007.\n\n\n\n\n\n\n\n\n\nEach bar represents a categorical variable, and the length represents the quantitative value. Bars can be arranged either vertically or horizontally. By convention, when there are not many categories, vertical bars are the best option. But when one is working with many categories, the names on the horizontal axis might overlap, so displaying the values in the y axis is a better idea. Take a look at how the life expectancy ages in 2007 for the Americas would look like in a vertical axis…\n\n\n\n\n\n\n\n\n\nand in a horizontal one.\n\n\n\n\n\n\n\n\n\nBar plots tend to be the most common kind of visualization due to their effectiveness. Pretty much anyone can interpret them, and simple, easy-to-understand graphs are a key of data visualization. However, reusing them over and over in the same document might be repetitive and lose a reader’s attention, so it is also good to keep in mind other alternatives. For example, dot plots provide a cleaner graph by only showing a point where the bar would end, removing the “insides” of the bars. This minimalist approach is often considered synonymous with a good plot in the data visualization community. Expanding our knowledge of different types of graphs and their uses can keep things interesting and enhance the clarity of our data representation.\n\n\n\n\n\n\n\n\n\nSo far, we have seen examples showing values for individual groups. However, in other cases we might be interested in showing how some group, entity, or amount breaks down into individual pieces that each represent a proportion of the whole. Common examples include the percentage of races in a group of people (45% of the population in this neighborhood is black, 40% is white, 10% is latinx and 5% is asian) or the percentages of people voting for different political parties in an election (60% of the population voted for X candidate, while 40% voted for candidate Y).\nThe archetypal visualization for this kind of information is the pie chart, where each group is displayed as colored areas inside of a circle. However, over time, this graph has gained quite a negative reputation. While it is very popular, it has been shown that the human eye understands proportions more easily when they are displayed vertically. Take a look at the proportion of people in each continent represented within the world’s total population in 2007, in a pie chart…\n\n\n\n\n\n\n\n\n\nor in a stacked bar chart.\n\n\n\n\n\n\n\n\n\nThe stacked bar chart shows more clearly the weight each continent has on the total population of the world, and allows us to see a small line representing Oceania, which wasn’t visible in the bar chart.\nVisualizing proportions can be challenging, specially when the whole is broken into many different pieces. That is to say, when we want to see the values for sub-groups inside of our groups. And this is specially useful when we want to control how a variable changes by different groups. For example, we could be interested in the amount of countries that have a high or a low life expectancy across continents. This could be done in a stacked bar chart:\n\n\n\n\n\n\n\n\n\nStacked bar charts are useful for showing absolute values within the larger groups. It is clear from this graph that Africa has the most countries, but also that the majority of them have a low life expectancy. Another option to show this information would be a dodged bar chart, where the subgroup bars are positioned next to each other rather that on top.\n\n\n\n\n\n\n\n\n\nIn this case, there is no clear answer to which kind of plot is better! Data visualization is a field where scientists should explore and try out different options to decide which one suits the case better.\nSo far, we have worked with the simplest kind of visualizations: counts or proportions. However, as we dive into the analysis, we might be interested in checking out how a particular variable is distributed in a dataset. That is to say, how the variable is spread: which values are more common and less common, which are extreme values, and so on.\nThe most common visualization to show distributions is the histogram, which in practice is a variant of… the bar plot!\n\n\n\n\n\n\n\n\n\nThe histogram shows the range of values (from the minimum to the maximum that they reach), and how often they are observed in each range.\nOne thing to note about histograms is that their appearance (and therefore the message they convey) can change depending on the number of intervals used. The previous plot had divided the range of values into 30 ‘bins’ or equal intervals (e.g. ‘0-10’, ‘10-20’, etc.) and counts how many observations fall into each one. We can increase the level of detail in the histogram by increasing the number of intervals, at the cost of losing generalization. Conversely, if we reduce the number of intervals, we show a more summarized distribution of the data, at the cost of losing detail.\n\n\n\n\n\n\n\n\n\nAnother option to plot distributions are density graphs. Density plots are direct descendants of histograms. But instead of counts of observations per range of values, they show the probability distribution of the variable, i.e. how likely it is to find a particular value if we randomly selected one of the observations. Unlike histograms, which have been in use for a couple of centuries because they are relatively easy to create by hand, the (previously) laborious density plots have become popular with the advent of software and computers capable of creating them instantly.\n\n\n\n\n\n\n\n\n\nThe results of density plots are interpreted similarly to those of a histogram: we notice the range of the data and how common they are in one range compared to another.\nFinally, we can plot distributions as boxplots (also called box and whisker plot). It displays a summary of the minimum, first quartile, median, third quartile, and maximum values of the data. First, let’s take a look at the following boxplot that displays the GDP per capita across continents in 2007.\n\n\n\n\n\n\n\n\n\nThis type of graphic contains a lot of information.\n\nThe box in the middle represents the middle 50% of the data, with the lower edge representing the first quartile (25th percentile) and the upper edge representing the third quartile (75th percentile).\nThe line inside the box represents the median.\nThe whiskers extending from the box show the range of the data, typically defined as 1.5 times the interquartile range (IQR), which is the distance between the first and third quartiles.\nOutliers, which are data points that fall outside the whiskers, are shown as individual points.\n\nFinally, we could be interested in showing the relationship between variables (x-y). The most common way to show this is through scatterplots.\n\n\n\n\n\n\n\n\n\n\n\nMapping more aesthetic attributes\nSo far, we mostly saw how data can be mapped into an x and y axis. However, we mentioned we can also map data as shapes, sizes and colors. We briefly saw how color can be introduced to show categorical variables when we are seeing proportions of the data. However, it can also be used as a continuous variable.\n\n\n\n\n\n\n\n\n\nThis graph shows that there is a relationship between longevity and GDP: countries with higher life expectancy are found at the top of the graph. As we continue to explore this relationship in the following plots, note that we also introduced a transformation on the X axis. We can see that the values in the scale go from 1,000,0000 to 10,000,000, and then to 100,000,000 until 1,000,000,000. This is called a logaritmic scale: the values are no longer evenly spaced, they increase exponentially. This is specially useful when we are working with data with a wide range of values, such as populations or income. For example, let’s introduce the variable continent into our ‘Wealth and health’ graph with a logarithmic scale in the GDP variable.\n\n\n\n\n\n\n\n\n\nThis returns us a more clear and compact graph, where we can better see the variability in the lower values for both variables, and relate them mostly to Africa.\nWe could also show the continents as shapes:\n\n\n\n\n\n\n\n\n\nHowever, the color tends to be a better option to plot categorical data. We could also introduce more data into the plot through the size of the dots or shapes."
  },
  {
    "objectID": "materials/14_unsupervised_learning/1_class_notes.html",
    "href": "materials/14_unsupervised_learning/1_class_notes.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "During the last lecture we addressed supervised classification models in machine learning. The main characteristic of these methods is that we have a target variable we want to predict, based on features or predictors. When the target variable is continuous, we talk about regression problems, but when it’s a categorical variable, we talk about classification problems.\nIn this opportunity, we will discover a different type of machine learning methods: unsupervised learning. Unlike the supervised case, now we will not have a Y variable that we want to predict based on X features. We will only have a set of X features that are represented in a data space, from which we can extract relevant information. That is, we are not interested in being able to predict values, but rather finding interesting patterns in the data that can be inferred from the features.\nThere are mainly two types of problems that can be approached through unsupervised learning:\n\nDimensionality reduction: This refers to the process of finding combinations of features to replace the original ones, in order to reduce the dimension of the problem. In other words, to “rewrite” the data into fewer variables. This is often useful for the case of highly dimensional dataframes, where several variables are correlated. Dimensionality reduction is useful to compress information in order to reduce computational memory requirements, visualize data into more readable representations, and as input for other machine learning methods (such as classification or clustering).\nClustering: The goal of these type of algorithms is to find ‘similar’ cases. That is, to find groups within the observations that share similar values within the feature space. In this class, we will address different clustering methods."
  },
  {
    "objectID": "materials/14_unsupervised_learning/1_class_notes.html#overview",
    "href": "materials/14_unsupervised_learning/1_class_notes.html#overview",
    "title": "Unsupervised Learning",
    "section": "Overview",
    "text": "Overview\nAs explained before, clustering refers to a set of methods that aim to find subgroups of similar observations within the data. The goal of these methods is to partition the sample in a way that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other (Hastie, Tibshirani, and Friedman 2009). However, what does it mean that observations are similar and different?\n\n\n\nFinding subgroups within the observations.\n\n\nBroadly speaking, subgroups of similar data are those where the intra-cluster distance is minimized, and the inter-cluster distance is maximized. The observations in one group should be as homogeneous as possible, and the observations of different groups should be as heterogeneous as possible between each other. The measures to define similarity and difference are domain-specific considerations that should be taken into account by the knowledge of the data. We will now look into different approaches for clustering."
  },
  {
    "objectID": "materials/14_unsupervised_learning/1_class_notes.html#k-means-clustering",
    "href": "materials/14_unsupervised_learning/1_class_notes.html#k-means-clustering",
    "title": "Unsupervised Learning",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nTake a look at the following figure. It takes information from the Gapminder dataset and plots the relationship between GDP per capita and life expectancy for all the countries in 2007.\n\nBased on this information, how many groups do you think you could make from these observations? And which criteria would you use to group them?\n\n\n\n\n\n\n\n\n\n\nIntuitively, we might initially consider forming two distinct groups: one comprising countries characterized by both high life expectancy and a substantial GDP per capita, and another consisting of low-income nations with correspondingly low life expectancy. However, an alternative approach could involve categorizing our observations into three distinct groups, thereby introducing an additional cluster comprising middle-income countries with intermediate life expectancy. Determining the number of groups can be a subjective decision and can be tailored to the specific characteristics of the dataset; in essence, the possibilities are as diverse as the number of observations.\nThis conceptual framework underlies the fundamental principles of the k-means clustering algorithm. Operating on the basis of a predefined value for K (the number of clusters):\n\nEach observation is assigned to one of the K defined clusters.\nThe clusters are mutually exclusive, ensuring that no observation is part of more than one cluster.\n\nNow, how does this algorithm define which observation will belong into each cluster? How does it define the similarity between the subgroups? The k-means clustering algorithm does so minimizing within-cluster variation. The key metric for measuring this variation is the squared Euclidean distance1. For each cluster k, the within-cluster variation is computed as the sum of squared Euclidean distances between all pairs of observations within that cluster, divided by the total number of observations in that cluster.\nThe algorithm is guided by the goal of optimizing this within-cluster variation problem, seeking to minimize the following expression:\n\\[\n\\min_{C_{1},...,C_{k}} \\left\\{W(C_{k})=\\frac{1}{|C_{k}|} \\sum_{i,i^{'} \\in C_{k}} \\sum_{j=1}^p (x_{ij} - x_{i^{'}j})^2\\right\\}\n\\]\nTherefore, the goal of the algorithm is to find a method to partition the observations into K clusters that optimizes this problem, that is, that returns the smaller value of the within-cluster variation.\nThe algorithm has the following steps:\n\nRandomly assigns to each observation a value from 1 to K, based on the K defined clusters.\nRepeats until convergence:\n\nFor each of the K clusters, computes the centroid. The kth cluster centroid is the p feature average (mean) for the observations in that cluster.\nAssign x to the nearest center (the closest one in terms of Euclidean distance).\nRecalculate the cluster centers as the mean of all data points assigned to each cluster.\n\nThe algorithm converges to K clusters (more iterations do not change the result), each with its own center.\n\nAs the algorithm progresses, the obtained clustering continually improves, ensuring that the within-cluster distance is minimized. This iterative process leads to clusters with observations that are more similar to each other than to those in other clusters. When the result no longer changes, it means that the distance was optimized, a local optimum has been reached.\n\n\n\nAllison Horst\n\n\nThe k-means clustering method has several limitations that should be considered when applying it.\nFirst, a significant challenge lies in the determination of the pre-defined number of clusters, K, which is often unknown. The algorithm’s effectiveness is sensitive to this parameter, and different values of K can lead to varied clustering outcomes. To address this, practitioners commonly iterate over different values of K and assess the clustering results to find the most suitable configuration.\nSecondly, the algorithm is susceptible to converging to a local optimum rather than a global optimum. This sensitivity arises from the dependence on the initial random assignment of clusters in Step 1. To mitigate this, it is advisable to run the algorithm multiple times with different initial configurations, and then select the solution that yields the most favorable clustering results.\nAdditionally, k-means is constrained to continuous variables, as computing centroids relies on the mean of the p features of the data. Therefore, when dealing with categorical or non-continuous data, alternative clustering methods may be more appropriate2.\nMoreover, the algorithm is highly sensitive to outliers, as the centroid is influenced by the mean of the features. Outliers can disproportionately impact the mean, leading to biased cluster centers. Preprocessing steps, such as outlier removal or transformation, may be necessary to enhance the robustness of the algorithm in the presence of outliers 3.\nIn summary, while k-means clustering is a valuable tool for partitioning data into clusters, users should be mindful of its sensitivity to the pre-defined number of clusters, the reliance on random initialization, and its suitability for continuous variables. Careful consideration and experimentation with different configurations are essential to obtain meaningful and reliable clustering results."
  },
  {
    "objectID": "materials/14_unsupervised_learning/1_class_notes.html#hierarchical-clustering",
    "href": "materials/14_unsupervised_learning/1_class_notes.html#hierarchical-clustering",
    "title": "Unsupervised Learning",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nSome of the limitations from the K-Means clustering can be overcome with another method: Hierarchical Clustering. The premise of this method is that we do not know the number of clusters before training the data, and we use a dendogram to get an overview and define the ideal K.\n\nInterpreting the dendogram\nThe dendrogram is a tree-based graphical representation of the results of hierarchical clustering. It visualizes the hierarchy of clusters and the relationships between data points.\n\n\n\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani\n\n\nThe upper figure illustrates a dendrogram. The X-axis represents individual data points in the dataset, while the Y-axis depicts the distances between data points or clusters. At the dendrogram’s bottom, each vertical line represents a single data point, essentially treating each as its own cluster. Moving up on the dendrogram, vertical lines merge into clusters, formed by grouping data points more similar to each other than to the rest of the data. The creation of a new cluster occurs where vertical lines merge into a horizontal line, with the height of the line reflecting the dissimilarity between the merging clusters.\nTo determine the number of clusters for analysis, a horizontal line is drawn across the dendrogram at a chosen height. The number of clusters corresponds to the intersections between this line and the vertical lines. The choice of the cutoff point depends on the desired level of granularity in clustering results. A higher cutoff yields fewer, larger clusters, while a lower cutoff results in more, smaller clusters.\nAfter determining the number of clusters and the cutting point on the dendrogram, the resulting clusters are used in the analysis. Each data point is assigned to one of these clusters. For example, in the second figure, a cutoff at height 9 produces two clusters, depicted in pink and green. In the last image, a cutoff at level 5 leads to a split into three clusters, colored in pink, yellow, and green. This flexibility in choosing the cutoff allows for tailoring the clustering solution to the specific problem and the desired level of detail for the results.\n\n\nAlgorithm steps\nThe most common type of hierarchical clustering is the bottom-up and aglomerative clustering. It refers to the fact that the clusters will be grouped from the individual observations to larger groups. The general idea is that each observation will begin by being considered as an individual cluster. Then, the clusters that are closer to each other will be identified and merged into one larger cluster. This process will be iterated until all the observations are agglomerated into one cluster. Therefore, it adds up the clusters in each iteration.\nHow does the algorithm work?\n\nInitialization: We start by considering that each data point is an individual cluster. In the beginning, we have N clusters, where N is the number of data points.\nSimilarity calculation: We compute the similarity matrix or distance between all pairs of clusters. There are several ways to measure similarity, such as Euclidean distance, Manhattan distance, cosine similarity, etc.\nCluster merging: The two most similar clusters are merged into one. The choice of which clusters to merge is based on the similarity matrix. This is repeated at each step until only one cluster remains.\nHierarchy construction: As the clusters are merged, a tree hierarchy (dendrogram) is constructed showing how the data are grouped at different levels of similarity. In the dendrogram, clusters are merged based on distance on the vertical axis.\nCluster selection: To determine the optimal number of clusters, you can examine the dendrogram and decide at which level of similarity you want to cut the tree. This will determine the number of clusters.\n\n\n\n\nHierarchical Clustering by Allison Horst\n\n\n\n\nDissimilarity measures\nThe algorithm for hierarchical clustering is straightforward, yet the choice of dissimilarity measurement and linkage methods plays a crucial role in shaping the resulting dendrogram and clusters. Unlike K-Means clustering, hierarchical clustering involves assessing dissimilarity when clusters contain multiple observations, achieved through different linkage methods. The selection of the linkage method significantly influences the clustering outcomes. Here are four common types of linkage methods:\nThe four most common types of linkage methods include:\n\nSingle Linkage (Minimum Linkage): Measures the distance between the closest pair of data points in two different clusters. It tends to produce elongated clusters and is sensitive to outliers.\n\n\n\nDhivya\n\n\nComplete Linkage (Maximum Linkage): Measures the distance between the farthest pair of data points in two different clusters. It tends to produce compact, spherical clusters and is less sensitive to outliers.\n\n\n\nDhivya\n\n\nAverage Linkage (UPGMA - Unweighted Pair Group Method with Arithmetic Mean): Computes the average distance between all pairs of data points in two different clusters. It provides a balance between single and complete linkage, yielding relatively balanced clusters.\n\n\n\nDhivya\n\n\nCentroid Linkage: Calculates the distance between the centroids (mean points) of two clusters. It can be computationally efficient but may not work well with clusters of irregular shapes.\n\n\n\nDhivya\n\n\n\nThe choice of linkage method depends on the characteristics of the data and the desired structure of the clusters. Each method has its advantages and limitations, making it essential to consider the nature of the dataset and the goals of the analysis when selecting an appropriate linkage method for hierarchical clustering. Experimentation with different methods is often necessary to determine the most suitable approach for a particular dataset and clustering objective."
  },
  {
    "objectID": "materials/14_unsupervised_learning/1_class_notes.html#practical-issues-in-clustering",
    "href": "materials/14_unsupervised_learning/1_class_notes.html#practical-issues-in-clustering",
    "title": "Unsupervised Learning",
    "section": "Practical issues in clustering",
    "text": "Practical issues in clustering\nBoth K-Means and Hierarchical Clustering assign each observation to a cluster of similar data points. However, as researchers, it’s crucial to consider the following points when employing such methods:\n\nCorrelated Variables: Variables with high correlations may lead to the formation of clusters based on these correlations rather than capturing meaningful patterns in the data. Employing dimensionality reduction techniques, such as Principal Component Analysis (PCA), can help address this issue by transforming the data into uncorrelated variables.\nHigh-Dimensional Datasets: High-dimensional datasets, characterized by a large number of variables, can pose challenges for clustering methods. The increased complexity may result in suboptimal clustering outcomes. It is advisable to explore dimensionality reduction techniques or feature selection to enhance the performance of clustering algorithms.\nOutliers: Outliers have the potential to distort clustering results. Traditional clustering methods like K-Means and Hierarchical Clustering lack built-in mechanisms to robustly handle extreme values. Researchers may consider employing alternative clustering methods, such as Hierarchical DBSCAN4, which is more robust to outliers and density variations in data.\nVariable Standardization: Clustering algorithms are sensitive to the scale of variables, and when variables have varying units of measurement, standardization becomes crucial. Failing to standardize may result in certain variables dominating the clustering process due to their larger units. Ensuring that all variables are on a similar scale through standardization helps to avoid this issue.\n\nThese considerations highlight the importance of preprocessing and understanding the characteristics of the data before applying clustering methods. By addressing issues related to correlated variables, high dimensionality, outliers, and variable scale, researchers can enhance the robustness and reliability of clustering results for more meaningful insights."
  },
  {
    "objectID": "materials/14_unsupervised_learning/1_class_notes.html#footnotes",
    "href": "materials/14_unsupervised_learning/1_class_notes.html#footnotes",
    "title": "Unsupervised Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe euclidean distance measures the distance between two points in a n-dimensional space. It is given by \\(D = √((x2 - x1)^2 + (y2 - y1)^2 + (z2 - z1)^2 + ... + (xn - x1)^2)\\)↩︎\nThe categorical variant for this algorithm is K-Modes clustering.↩︎\nAn alternative method to K-Means clustering that overcomes this limitation is its variant K-Medoids. K-Medoids is similar to K-Means in that it aims to partition a dataset into K clusters, but it represents the cluster centers not by the mean but by the medoid of the cluster. It also outperforms K-Means by taking into account categorical variables. However, it is computationally more expensive than the K-Means algorithm.↩︎\nHierarchical DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a variation of the DBSCAN clustering algorithm that organizes clusters in a hierarchical or tree-like structure rather than identifying clusters as separate, non-overlapping groups.↩︎"
  },
  {
    "objectID": "materials/17_topic_modeling/1_class_notes.html",
    "href": "materials/17_topic_modeling/1_class_notes.html",
    "title": "Topic Modeling",
    "section": "",
    "text": "In text mining, we often have collections of document that can be grouped into different topics:\n\nNews articles are naturally divided into topics such as sports, politics, economics, or showbiz. And each of this could be further divided into more specific groups.\nResearch articles follow a disciplinary structure, and within that they delve into specific research subjects\n\nTopic modeling is is a statistical technique used in Natural Language Processing (NLP) to discover abstract topics within a collection of documents. By doing so, it enables both an automatic grouping of documents, and the revelation of underlying subjects or motifs discussed across the corpus without the need of prior knowledge of the topics. It can be considered as a clustering algorithm, given that it groups documents. It can be also consider a method of dimensionality reduction, similar to PCA on numeric data, because it reduces the representation of documents from all their word to a classification or distribution among a shorter group topics. You could think of topic modeling as a set of “amplified reading” techniques.\nAs David Blei puts it:\n\ntopic modeling algorithms do not require any prior annotations or labeling of the documents. The topics emerge from the analysis of the original texts. Topic modeling enables us to organize and summarize (…) archives at a scale that would be impossible by human annotation (Blei 2012)"
  },
  {
    "objectID": "materials/17_topic_modeling/1_class_notes.html#which-is-the-logic-behind-the-lda-algorithm",
    "href": "materials/17_topic_modeling/1_class_notes.html#which-is-the-logic-behind-the-lda-algorithm",
    "title": "Topic Modeling",
    "section": "¿Which is the logic behind the LDA algorithm?",
    "text": "¿Which is the logic behind the LDA algorithm?\nLDA assumes that topics are pre-existent: they are defined before any data has been generated. So, the number of topics (generally, named \\(k\\)) is an hyperparameter.\nThe logic is that the data generating process of documents in the corpus goes as follows:\n\nFor every document within the corpus, we initiate the generation of its words through a two-step procedure:\nInitially, we randomly select a distribution over topics for the document.\nThen, for each word present in the document:\n\nWe randomly select a topic from the distribution over topics determined in the first step.\nand then, we randomly select a word from the corresponding distribution over the vocabulary associated with the chosen topic.\n\n\nThis idea behind LDA can be illustrated like:\n\nIt is called Dirichlet because of Dirichlet distributions, which is a type of function in bayesian statistics that generates as an output a multinomial distribution (the type of distribution that we need to represent a distribution over topics for example).\nIf we assume that the data generating process of text in documents is that described above, we can then go to our corpus to see the observed words in documents and infer backwards which are the topics that generated them.\nOf course, no one thinks that the authors of the text actually roll a dice for each word based on a distribution of topics. Nevertheless, the outcomes of this model have proven to be useful tools to build topics in documents."
  },
  {
    "objectID": "materials/17_topic_modeling/1_class_notes.html#automatic-evaluation",
    "href": "materials/17_topic_modeling/1_class_notes.html#automatic-evaluation",
    "title": "Topic Modeling",
    "section": "Automatic evaluation",
    "text": "Automatic evaluation\nSelecting the \\(k\\) value is not trivial, and it can create widely different results. There are various metrics that allow quantifying the fit (i.e., how “good” it is) of the number of topics defined in quantitative terms. Some of those are:\n\nPerplexity: measures a model’s ability to generalize and predict newly-presented documents. It is based on the model’s likelihood; lower values are better.\nCoherence: Is the average pointwise mutual information (PMI) of two words randomly taken from the same document. A coherent topic will display words that tend to occur in the same documents. This means that the most probably words in a topic should have high mutual information. Higher values are better, and it imply a more interpretable topic model.\nDiversity: Is the percentage of unique words in the top 25 words of all topics. A diversity close to 0 indicates redundant topics; diversity close to 1 indicates more varied topics."
  },
  {
    "objectID": "materials/17_topic_modeling/1_class_notes.html#qualitative-evaluation",
    "href": "materials/17_topic_modeling/1_class_notes.html#qualitative-evaluation",
    "title": "Topic Modeling",
    "section": "Qualitative evaluation",
    "text": "Qualitative evaluation\nthe use of these metrics leads to models that achieve good statistical performance but may not necessarily generate interpretable topics.\nA larger number of topics tends to yield better metrics and allows for a high resolution of the latent structure of the corpus. However, it has been observed that increasing the number of topics tends to decrease the quality of the topics in terms of interpretability (Mimno et al. 2011) (Chang et al. 2009). Thus, similar to many other problems, model complexity and interpretability tend to move in opposite directions.\nTherefore, it is often possible to use other approaches to select the number of topics. In general, we could think of two main ways to do this:\n\nwe could define a relatively small number of topics and try to make them all “interpretable”\nwe could estimate a large number of topics and only consider those that are relevant to our analysis questions; this is the method that (Diego Kozlowski and Altszyler 2023) followed in the paper we will discuss in the next section.\n\nWhich approach is the most suitable? This will depend on the research questions and the type of corpus in use. If it is a relatively homogeneous corpus and the interest is to have an initial overview of its content, perhaps the first approach may be more fruitful (“few but good”). If, on the other hand, it is a potentially very diverse corpus and we are looking to detect some specific topics, then the second approach might work better.\nUp to this point, we have thought of LDA (or any topic modeling technique) as the endpoint of the analysis. Once we train a certain \\(k\\) that is satisfactory, we move on to model validation and interpretation. However, we can also think of it as a starting point. For example, we can use topic modeling to have an initial descriptive overview of a given corpus and to “guide” a deep reading of some documents in the corpus. Suppose we have a very large corpus of textual documents. We cannot read the entire corpus, but we would like to have an idea of how to choose which documents to read. We can train an LDA model and use it to guide our in-depth reading of the corpus. This could either be a couple of articles per topic, or as a tool to find those articles that delve into the topic of interest for our study.\nWe can also use it as part of the preprocessing of a corpus and the feature construction process. It is common for a corpus to contain documents or parts of documents in languages other than the majority language of the corpus. If this were to happen, it is highly likely that one or several topics would concentrate terms in that language. Since we can also obtain the topic distribution per document, it would be very easy to clean the texts that do not correspond to the majority language: we simply filter out those documents with a high prevalence in the topic that groups terms in another language. Also, it can be used to build the dictionary of tokens to remove together with the stop words. For example, if some of the documents have latex or html code, they might appear as specific topics.\nWe can also think of the topic-document matrix as features to be used. Instead of having a TDM with all the vocabulary, we can use the document-topic matrix as a dense representation of each document and use it as features for a text classification model."
  },
  {
    "objectID": "materials/17_topic_modeling/1_class_notes.html#some-limitations-on-lda",
    "href": "materials/17_topic_modeling/1_class_notes.html#some-limitations-on-lda",
    "title": "Topic Modeling",
    "section": "Some limitations on LDA",
    "text": "Some limitations on LDA\nWe would like to emphasize some specific assumptions of LDA:\n\nTopics are independent of each other.\nThe distribution of words within a topic (the content of a topic) is constant. Document 1 uses the same words to compose topic 1 as documents 2, 3, etc.\nThe estimation of topics is done exclusively based on the text of each document in the corpus, without incorporating any other type of information.\n\nLet’s consider, for example, a study over a long period of time, like the work by (Blei 2012) that we mentioned above containing scientific articles spanning over 100 years. There is a possibility that the vocabulary about genetics changed between the late 19th century and the present, and that different terms are used.\n\nStructural Topic Modeling (STM)\nSTM (Structural Topic Modeling) is a topic modeling technique that aims to address the limitations mentioned above. The main feature is that it allows the introduction of covariates, meaning it allows topics to change the model based on one or more metadata variables of the text (publication date, author’s gender, nationality, publication type, etc.). Any information about the documents can be used in STM.\nThus, we first need to define two “dimensions”: topic content and topic prevalence. The former refers to the composition of words that create each topic. The latter refers to the composition of topics that constitute each document. STM will allow the introduction of covariates that affect both dimensions."
  },
  {
    "objectID": "materials/17_topic_modeling/1_class_notes.html#limitations-of-bag-of-words",
    "href": "materials/17_topic_modeling/1_class_notes.html#limitations-of-bag-of-words",
    "title": "Topic Modeling",
    "section": "Limitations of Bag of Words",
    "text": "Limitations of Bag of Words\nThese topic modeling techniques are based on the Bag of Words (BoW) model. This way of vectorizing text has some advantages:\n\nit is simple: we just count the frequency of occurrence of each word in the vocabulary in each document\nit is easy to implement\n\nHowever, it also comes with limitations:\n\nit is insensitive to the order of words\nit does not take into account grammatical structure\nit fails to fully capture semantic relationships between words: Each word is conceived independently, so this cannot consider the conceptual similarity of terms. For example, ‘dog’ will be as distant to ‘beagle’ than to ‘cat’ or ‘table’\nit has high dimensionality: it is common for the vocabulary size of an average corpus to be high, above 10,000 words; this means that our TFM will have about 10,000 columns\nit has high sparsity: in turn, since a document only uses a limited subset of terms from the vocabulary, it is expected that the TFM will contain many empty elements (zeros); this poses a problem when calculating similarity or correlation metrics between documents or words."
  },
  {
    "objectID": "materials/9_data_storytelling/1_class_notes.html",
    "href": "materials/9_data_storytelling/1_class_notes.html",
    "title": "Data Storytelling",
    "section": "",
    "text": "Introduction\nVisualization is a key part of the data analysis workflow. Not only does it help us explore our data and analysis results, but it also serves as the primary tool for sharing our findings with others. Whether we’re presenting our work in an academic paper, journalistic article, or a presentation to colleagues, graphs are essential to attract their attention and making our work interesting. This is why it is important to learn how to make graphs that display information in an effective but also aesthetically pleasing way. In this lecture, we will address these topics.\n\n\n\nHadley Wickham and Garrett Grolemund of RStudio's book R for Data Science\n\n\n\n\nSome history\nWhile data visualization has become easier and more accessible over the last years thanks to statistical software, it is not a discipline that was born in the last couple of years. Using graphs to convey relevant information is something that has been done for centuries. One notable example is the map created by John Snow in 1854, which plotted illness-related deaths during London’s cholera pandemic. By using lines to represent each death, he was able to prove that the disease was not spread through the air, but through polluted water from a specific pump on Broad Street. This shows how effective and impactful data visualization can be when used correctly.\n\n\n\nJohn Snow, 1854\n\n\nAnother example are the infographics done by the sociologist W.E.B. Du Bois and his team in the Exposition Universelle, the Paris World Fair of 1900. Here, the authors used a variant of the classic pie chart to showcase the representation of black and white people in different occupations, comparing the state level of Georgia and the total of the country. It is notorious here how in the southern state there was still the heritage of institutionalized racism, where black people were overrepresented in the sector of domestic and personal service.\n\n\n\nW.E.B. Du Bois\n\n\nFinally, we have this polar graph done by the famous Florence Nightingale that dates back to 1858. This plot indicates the annual rate of mortality per 1,000 in each month that occurred from preventable diseases (in blue), those that were the results of wounds (in red), and those due to other causes (in black). It is notorious how many more soldiers died from preventable diseases rather than by being wounded in battle.\n\n\n\nNightingale, Florence. Notes on Matters Affecting the Health, Efficiency, and Hospital Administration of the British Army (1858)\n\n\nThese three cases are good examples of the historical use of data visualization to display quantitative information. The three authors use aesthetical attributes in order to show numerical values, categories, and locations.\nHowever, as social scientists, it’s important to note that they also employed infographics to capture the attention of a wider audience and present evidence on specific topics. Snow’s maps, for instance, demonstrated how cholera could be prevented through better water sanitation, while Dubois and his team illustrated the lingering effects of slavery in Georgia compared to the rest of the United States. Nightingale’s work, on the other hand, revealed how mortality rates could be reduced through improved sanitation. Overall, data visualization serves as a Trojan Horse to highlight crucial, data-based information.\n\n\nGrammar of graphics\nDoing a good graphic involves data and creativity. However, there are certain common elements behind the creation of a graph that we can sum up as the grammar of graphics Wickham (2010). Graphics are constructed by combining independent components, such as data, aesthetics, geometries, and scales.\nThe package we will be using to make plots in R is called ggplot2, and its syntax is based on this grammar of graphics. In ggplot2, a plot is constructed by starting with a layer of data, and then adding additional layers to specify the aesthetics (e.g., color, shape, size) of the plot, the type of geometry (e.g., point, line, bar) to use to display the data, and the scale (e.g., linear, log, discrete) to use for each aesthetic. Each of these components can be modified and customized in a variety of ways to create complex and informative visualizations.\n\n\n\nJoe Roe\n\n\nThis process of assigning data variables to the visual properties of a plot is called mapping of aesthetic attributes. In other words, it is the way to show in a visually perceptible way the difference between values. Aesthetics refer to visual properties such as color, shape, size, transparency, and position that can be used to visually represent data.\n\n\n\nCommonly used aesthetics in data visualization: position, shape, size, color, line width, line type. Some of these aesthetics can represent both continuous and discrete data (position, size, line width, color) while others can usually only represent discrete data (shape, line type) (Wilke n.d.a)\n\n\nThe grammar of graphics approach allows users to create a wide range of plots, from basic scatter plots and histograms to complex multi-layered visualizations, using a consistent and intuitive syntax. It is a powerful tool for data exploration and communication and has become a popular approach for data visualization in the R community.\n\n\nTypes of graphs\nWe can group plots into various clusters according to the kind of data we want to show. We will often be interested in showing the magnitude of numbers: the total number of people living in different countries, the mean salary for different groups of populations. These cases have a set of categorical values (such as countries, or demographic groups) and a quantitative value for each category. This is the same as saying we are going to show amounts (Wilke (n.d.b)). The most common graph to showcase amounts are bar plots. For example, we can display the countries with lowest life expectancy in the year 2007.\n\n\n\n\n\n\n\n\n\nEach bar represents a categorical variable, and the length represents the quantitative value. Bars can be arranged either vertically or horizontally. By convention, when there are not many categories, vertical bars are the best option. But when one is working with many categories, the names on the horizontal axis might overlap, so displaying the values in the y axis is a better idea. Take a look at how the life expectancy ages in 2007 for the Americas would look like in a vertical axis…\n\n\n\n\n\n\n\n\n\nand in a horizontal one.\n\n\n\n\n\n\n\n\n\nBar plots tend to be the most common kind of visualization due to their effectiveness. Pretty much anyone can interpret them, and simple, easy-to-understand graphs are a key of data visualization. However, reusing them over and over in the same document might be repetitive and lose a reader’s attention, so it is also good to keep in mind other alternatives. For example, dot plots provide a cleaner graph by only showing a point where the bar would end, removing the “insides” of the bars. This minimalist approach is often considered synonymous with a good plot in the data visualization community. Expanding our knowledge of different types of graphs and their uses can keep things interesting and enhance the clarity of our data representation.\n\n\n\n\n\n\n\n\n\nSo far, we have seen examples showing values for individual groups. However, in other cases we might be interested in showing how some group, entity, or amount breaks down into individual pieces that each represent a proportion of the whole. Common examples include the percentage of races in a group of people (45% of the population in this neighborhood is black, 40% is white, 10% is latinx and 5% is asian) or the percentages of people voting for different political parties in an election (60% of the population voted for X candidate, while 40% voted for candidate Y).\nThe archetypal visualization for this kind of information is the pie chart, where each group is displayed as colored areas inside of a circle. However, over time, this graph has gained quite a negative reputation. While it is very popular, it has been shown that the human eye understands proportions more easily when they are displayed vertically. Take a look at the proportion of people in each continent represented within the world’s total population in 2007, in a pie chart…\n\n\n\n\n\n\n\n\n\nor in a stacked bar chart.\n\n\n\n\n\n\n\n\n\nThe stacked bar chart shows more clearly the weight each continent has on the total population of the world, and allows us to see a small line representing Oceania, which wasn’t visible in the bar chart.\nVisualizing proportions can be challenging, specially when the whole is broken into many different pieces. That is to say, when we want to see the values for sub-groups inside of our groups. And this is specially useful when we want to control how a variable changes by different groups. For example, we could be interested in the amount of countries that have a high or a low life expectancy across continents. This could be done in a stacked bar chart:\n\n\n\n\n\n\n\n\n\nStacked bar charts are useful for showing absolute values within the larger groups. It is clear from this graph that Africa has the most countries, but also that the majority of them have a low life expectancy. Another option to show this information would be a dodged bar chart, where the subgroup bars are positioned next to each other rather that on top.\n\n\n\n\n\n\n\n\n\nIn this case, there is no clear answer to which kind of plot is better! Data visualization is a field where scientists should explore and try out different options to decide which one suits the case better.\nSo far, we have worked with the simplest kind of visualizations: counts or proportions. However, as we dive into the analysis, we might be interested in checking out how a particular variable is distributed in a dataset. That is to say, how the variable is spread: which values are more common and less common, which are extreme values, and so on.\nThe most common visualization to show distributions is the histogram, which in practice is a variant of… the bar plot!\n\n\n\n\n\n\n\n\n\nThe histogram shows the range of values (from the minimum to the maximum that they reach), and how often they are observed in each range.\nOne thing to note about histograms is that their appearance (and therefore the message they convey) can change depending on the number of intervals used. The previous plot had divided the range of values into 30 ‘bins’ or equal intervals (e.g. ‘0-10’, ‘10-20’, etc.) and counts how many observations fall into each one. We can increase the level of detail in the histogram by increasing the number of intervals, at the cost of losing generalization. Conversely, if we reduce the number of intervals, we show a more summarized distribution of the data, at the cost of losing detail.\n\n\n\n\n\n\n\n\n\nAnother option to plot distributions are density graphs. Density plots are direct descendants of histograms. But instead of counts of observations per range of values, they show the probability distribution of the variable, i.e. how likely it is to find a particular value if we randomly selected one of the observations. Unlike histograms, which have been in use for a couple of centuries because they are relatively easy to create by hand, the (previously) laborious density plots have become popular with the advent of software and computers capable of creating them instantly.\n\n\n\n\n\n\n\n\n\nThe results of density plots are interpreted similarly to those of a histogram: we notice the range of the data and how common they are in one range compared to another.\nFinally, we can plot distributions as boxplots (also called box and whisker plot). It displays a summary of the minimum, first quartile, median, third quartile, and maximum values of the data. First, let’s take a look at the following boxplot that displays the GDP per capita across continents in 2007.\n\n\n\n\n\n\n\n\n\nThis type of graphic contains a lot of information.\n\nThe box in the middle represents the middle 50% of the data, with the lower edge representing the first quartile (25th percentile) and the upper edge representing the third quartile (75th percentile).\nThe line inside the box represents the median.\nThe whiskers extending from the box show the range of the data, typically defined as 1.5 times the interquartile range (IQR), which is the distance between the first and third quartiles.\nOutliers, which are data points that fall outside the whiskers, are shown as individual points.\n\nFinally, we could be interested in showing the relationship between variables (x-y). The most common way to show this is through scatterplots.\n\n\n\n\n\n\n\n\n\n\n\nMapping more aesthetic attributes\nSo far, we mostly saw how data can be mapped into an x and y axis. However, we mentioned we can also map data as shapes, sizes and colors. We briefly saw how color can be introduced to show categorical variables when we are seeing proportions of the data. However, it can also be used as a continuous variable.\n\n\n\n\n\n\n\n\n\nThis graph shows that there is a relationship between longevity and GDP: countries with higher life expectancy are found at the top of the graph. As we continue to explore this relationship in the following plots, note that we also introduced a transformation on the X axis. We can see that the values in the scale go from 1,000,0000 to 10,000,000, and then to 100,000,000 until 1,000,000,000. This is called a logaritmic scale: the values are no longer evenly spaced, they increase exponentially. This is specially useful when we are working with data with a wide range of values, such as populations or income. For example, let’s introduce the variable continent into our ‘Wealth and health’ graph with a logarithmic scale in the GDP variable.\n\n\n\n\n\n\n\n\n\nThis returns us a more clear and compact graph, where we can better see the variability in the lower values for both variables, and relate them mostly to Africa.\nWe could also show the continents as shapes:\n\n\n\n\n\n\n\n\n\nHowever, the color tends to be a better option to plot categorical data. We could also introduce more data into the plot through the size of the dots or shapes.\n\n\n\n\n\n\n\n\n\nWhile we might get excited about the possibilities of creatively mapping data as a visual image, we should remember that just because we can use many resources we shouldn’t use all of them at the same time. A data visualization expert, Tufte (2001), has outlined several principles for creating effective and clear visualizations. Tufte’s principles emphasize the importance of clarity, simplicity, and precision in creating effective and informative graphs. His key principles are:\n\nShow the data: The data should be the primary focus of the graph, and any extraneous elements should be minimized.\nMaximize the data-ink ratio: The amount of ink used to represent the data should be maximized, while any unnecessary ink, such as gridlines, should be minimized.\nUse small multiples: Rather than cramming all the data into one graph, consider breaking it down into smaller graphs with a common scale and format to allow for easier comparison.\nUse clear and detailed labels: Labels should be easy to read and provide sufficient detail to help the viewer understand the data.\nAvoid chartjunk: Avoid adding decorative or unnecessary elements that do not contribute to understanding the data.\nUse a high data resolution: The graph should use a high enough resolution to show the details of the data clearly.\nUse a relevant scale: Choose a scale that is appropriate for the data being presented, and ensure that it is clearly labeled.\n\nWe recommend following these principles to make a clear, effective visualization.\n\n\nDiscussion\nThe design of a graph should always be based on the data and focused on the topic we are researching. However, we should always be careful about not lying with graphs. That is to say, it is important to not toy with our graph to show only information that reinforces our hypothesis or what we want the data to say. Kieran (2018) explains this with a clear example from the New York Times. In November 2016, this newspaper reported some research on peoples’ confidence in the institutions of democracy. The headline in the Times ran “How Stable Are Democracies? ‘Warning Signs Are Flashing Red’”, and the graph accompanying the article certainly seemed to show an alarming decline.\n\n\nThe graph was widely circulated on social media. However, this is an example on how one can lie with data. This plot presumable shows the the percentage of respondents who said “Yes”, presumably in contrast to those who said “No”. However, the survey asked respondants to rate the importance of living in a democracy on a ten point scale. The graph showed the difference across ages of people who had given of “10” (Absolutely important) only, not changes in the average score on the question. As it turns out, while there is some variation by year of birth, most people in these countries tend to rate the importance of living in a democracy very highly, even if they do not all score it as “Absolutely Important”. The political scientist Erik Voeten redrew the figure based using the average response.\n\n\n\nErik Voeten\n\n\nWhile we still see a decline in the average score by age cohort, on the order of between half a point to one and a half points on a ten point scale, it is not such a drastic decline as the one showed originally.\n\n\n\n\n\n\n\n\nReferences\n\nKieran, Healy. 2018. Data Visualization. https://press.princeton.edu/books/hardcover/9780691181615/data-visualization.\n\n\nTufte, Edward R. 2001. The Visual Display of Quantitative Information, 2nd Ed. Cheshire, Conn.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098.\n\n\nWilke, Claus O. n.d.a. Fundamentals of Data Visualization. Accessed May 8, 2023. https://clauswilke.com/dataviz/aesthetic-mapping.html.\n\n\n———. n.d.b. Fundamentals of Data Visualization. https://clauswilke.com/dataviz/aesthetic-mapping.html.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. Statistics and Computing. New York: Springer-Verlag. https://doi.org/10.1007/0-387-28695-0."
  },
  {
    "objectID": "materials/15_evaluation/1_class_notes.html",
    "href": "materials/15_evaluation/1_class_notes.html",
    "title": "Evaluation",
    "section": "",
    "text": "So far, we addressed different statistical and machine learning methods we can use to predict different values in datasets. Why is it necessary to introduce so many approaches, instead of just one? In statistics, there is no ideal universal approach for every dataset and problem. The results will vary depending on the approach, the research questions and the data, and in return it is important to decide for any given set of data which methods produce the best results. The definition of ‘best result’ is also not universal, because it depends on our goal with the predictions. Broadly speaking, are we interested in capturing as many cases as possible? Or are we interested in capturing specific, correct cases?\nIn this class we will address different ways to measure how ‘fit’ our model is given our dataset and project goals."
  },
  {
    "objectID": "materials/15_evaluation/1_class_notes.html#validation-set-approach",
    "href": "materials/15_evaluation/1_class_notes.html#validation-set-approach",
    "title": "Evaluation",
    "section": "Validation set approach",
    "text": "Validation set approach\nThe validation set approach, also known as “hold-out validation” or “simple validation,” is a straightforward technique used to evaluate models. It entails randomly dividing the available observations into two parts: a training set and a validation set. The model is trained on the training set, and then used to predict responses for the observations in the validation set. The resulting validation set error rate serves as an estimate of the test error rate.\n\n\n\nMalmi Amadoru\n\n\nTypically, a random partition allocates a substantial proportion of the dataset, usually around 70% to 80%, to the training set. This segment serves as the foundation for model development, where the model is fitted and trained. The remaining observations form a distinct validation set, enabling us to gauge the model’s performance on previously unseen data. Results from the validation set provide insights into how the model may perform with new, unencountered data, as these observations were excluded from the model’s learning process, allowing us to assess its generalization capabilities.\nIn most cases, we employ random sampling to divide the data into the train/validation set. However, situations may arise where the classes in the data are imbalanced. An imbalanced dataset occurs when one class constitutes a significant portion of the training data (the majority class), while the other class is underrepresented (the minority class). This imbalance can lead to bias in our model, causing it to favor one class over the other. Consequently, the model may exhibit strong performance on the training data but struggle with new data. To address this issue, we can consider three strategies to balance the data.\n\nCapturing more cases: To address imbalanced datasets, researchers have several options. One approach involves capturing more cases to achieve a balanced sample. Alternatively, data augmentation techniques can be employed. Data augmentation involves expanding our sample by applying various transformations to the existing data. For structured data, this may include introducing synthetic noise or generating new samples by perturbing existing data points. Techniques such as adding random values or utilizing statistical methods can be used for this purpose.\nSubsampling the data: It involves reducing the size of the majority class until it occurs with the same frequency as the minority class. This approach typically results in better-calibrated models, where the distributions of class probabilities are more consistent. Another method is oversampling, where the sample of the minority class is increased to match the frequency of the majority class through resampling1 (“Tidymodels - Subsampling for Class Imbalances,” n.d.). However, oversampling may lead the model to memorize training data patterns, especially when the minority class consists of a limited number of unique data points, potentially causing performance issues on the validation set."
  },
  {
    "objectID": "materials/15_evaluation/1_class_notes.html#cross-validation",
    "href": "materials/15_evaluation/1_class_notes.html#cross-validation",
    "title": "Evaluation",
    "section": "Cross-validation",
    "text": "Cross-validation\nCross-validation is a method that entails randomly dividing the observation dataset into k groups or “folds,” each approximately equal in size. During each iteration, one fold is designated as the validation set, while the model is trained on the remaining k-1 folds. This process repeats k times, with each iteration utilizing a different fold as the test set and the remaining folds as the training set.\n\n\n\nMLTut\n\n\nK-fold cross-validation offers several advantages:\n\nRobust Performance Estimate: By testing the model on different subsets of the data, it provides a more robust estimate of its performance. This is essential for evaluating how well the model generalizes to unseen data.\nOverfitting Detection: It helps in detecting overfitting by testing the model on multiple diverse data subsets, unlike the validation set approach, which uses only one subset.\nOptimal Use of Data: It maximizes the utilization of available data for both training and testing purposes, particularly beneficial when dealing with limited data resources."
  },
  {
    "objectID": "materials/15_evaluation/1_class_notes.html#metrics-for-regression",
    "href": "materials/15_evaluation/1_class_notes.html#metrics-for-regression",
    "title": "Evaluation",
    "section": "Metrics for regression",
    "text": "Metrics for regression\nIn supervised learning, when our objective is to forecast a continuous numeric value, we encounter regression problems. Some examples include:\n\nPredicting a persons’ income based on their work experience, age, gender, race, years of education.\nEstimating the potential sales of a product.\nProjecting the graduation rate of students in schools across different neighborhoods.\nPredicting GDP per capita in various countries, considering developmental variables.\n\nIntuitively, one of the simplest methods to evaluate the effectiveness of a model’s predictions is by calculating the difference between the predicted values and the actual values. This difference, often termed the residual, is computed for each observation.\n\n\n\nNewcastle University\n\n\nThe Mean Absolute Error (MAE) calculates the average of these residuals within the model, providing insight into the magnitude of the deviations between predictions and actual outputs. However, MAE does not indicate the direction of the errors, such as whether the model underestimates or overestimates the data. Its formula is:\n\\[\n\\sum_{i=1}^{D}|x_i-y_i|\n\\]\n\n\nWhere \\(|x_i-y_i|\\) represents the absolute difference between the \\(i\\)-th observed value \\(y_i\\) and the \\(i\\)th predicted value (\\(x_i\\)).\nIn machine learning, a common variation of MAE is Mean Squared Error (MSE). MSE computes the average of the squared differences between the actual values and the predicted values. The primary advantage of MSE is its capability to amplify the impact of larger errors by squaring the differences, thereby emphasizing significant deviations more prominently. This property makes MSE particularly effective in highlighting and pinpointing substantial disparities between predictions and actual outcomes. The MSE is given by:\n\\[\nsum_{i=1}^{D}(x_i-y_i)^2\n\\]\nMetrics like these allow us to assess the accuracy of a model. Accuracy, in the context of machine learning, quantifies the proportion of correct predictions made by a model relative to the total number of predictions.\nIn contrast, a metric such as the coefficient of determination (also known as \\(R^2\\)) enables us to measure the correlation between the dependent variable and the independent variables. This value offers insights into the model’s capacity to explain the predictions of the target variables and is calculated using the following formula:\n\\[\n1−∑(yi−yi)^2/∑(yi−y)^2\n\\]\nThis coefficient produces a score between 0 and 1, where a value closer to 1 signifies better performance of the model."
  },
  {
    "objectID": "materials/15_evaluation/1_class_notes.html#metrics-for-classification",
    "href": "materials/15_evaluation/1_class_notes.html#metrics-for-classification",
    "title": "Evaluation",
    "section": "Metrics for classification",
    "text": "Metrics for classification\nClassification problems involve predicting the output of a categorical variable. Examples include:\n\nPredicting employment status of a person.\nPredicting highest level of education achieved.\n\nWhen evaluating a classification model, a common approach is to compare the observed category with the predicted category. These comparisons can be visualized using a confusion matrix:\n\n\nThere are four possible outcomes in the result of a classification:\n\nTrue Positives (TP): Observations belonging to the positive class and correctly classified as such by the model. The model accurately identifies cases that are genuinely positive.\nFalse Negatives (FN) or Type I error: Observations that actually belong to the positive class but are incorrectly classified as negative by the model. In this case, the model fails to identify instances that are genuinely positive.\nTrue Negatives (TN): Observations belonging to the negative class and correctly classified as such by the model. The model correctly identifies cases that are genuinely negative.\nFalse Positives (FP) or Type II error: Observations that actually belong to the negative class but are incorrectly classified as positive by the model. In this scenario, the model mistakenly identifies instances as positive when they are genuinely negative.\n\nThese outcomes provide ways to evaluate the model. One common method is using the Receiver Operating Characteristic (ROC) curve. The ROC curve is a graphical representation of a model’s performance, showing the trade-off between the true positive rate (TPR: the model’s ability to correctly identify positive cases) and the false positive rate (FPR: the model’s tendency to classify negative cases as positive) across different thresholds.\n\n\n\nClasseval\n\n\nThe ROC space is defined by placing the TPR or sensitivity in the y-axis, and the FPR in the x-axis. Given different thresholds for classifying an observation into a positive class, different values for these two rates are achieved. The ROC curve illustrates all these points graphically, typically starting at the bottom-left corner (0,0) and ending at the top-right corner (1,1).\nAn ideal ROC curve would hug the top-left corner of the graph, indicating a model that achieves high true positive rates while keeping false positive rates low across all thresholds. In contrast, a diagonal line from the bottom-left to the top-right corner represents a model with no predictive ability, where the true positive and false positive rates are roughly equal.\nThe area under the ROC curve (AUC-ROC) is often used as a single metric to summarize the model’s overall performance. A higher AUC-ROC value (closer to 1) indicates better discrimination between positive and negative cases by the model.\nWe utilize different metrics to evaluate the performance of our model based on the specific goal of the prediction. If our aim is to identify the maximum number of relevant cases, we prioritize recall as our metric for assessing results. Conversely, if our focus is on pinpointing true positives with the highest accuracy, precision becomes the chosen metric, emphasizing the precision of true positive predictions. The choice of metric depends on the context and the specific objectives of the analysis.\n\nPrecision: It is a measure of the accuracy of positive predictions made by the model. It quantifies the proportion of positively labeled instances predicted by the model that are actually correct. High precision indicates that when the model predicts a positive instance, it is more likely to be correct.\nPrecision is calculated as the ratio of true positives (correctly predicted positive instances) to the sum of true positives and false positives (instances predicted as positive but are actually negative). It is given by the formula:\n\\[\nTP/(TP + FP)\n\\]\nRecall: Also known as Sensitivity or True Positive Rate (TPR), measures how many of the actual positive instances were correctly predicted by the model. High recall indicates that the model effectively captures most of the positive instances.\nRecall is calculated as the ratio of true positives to the sum of true positives and false negatives (instances that are actually positive but were predicted as negative). It is given by the formula:\n\\[\nTP / (TP + FN)\n\\]\nF1 Score: providing a balanced measure between these two metrics. It is particularly useful when you want to consider both precision and recall simultaneously, ensuring a balance between false positives and false negatives. This metric is especially valuable in scenarios with imbalanced datasets, where one class significantly outnumbers the other. The F1 Score is calculated using the formula:\n\\[\n2*(Precision*Recall)/(Precision + Recall)\n\\]"
  },
  {
    "objectID": "materials/15_evaluation/1_class_notes.html#footnotes",
    "href": "materials/15_evaluation/1_class_notes.html#footnotes",
    "title": "Evaluation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nResampling implies repeating cases to get the desired n.↩︎"
  },
  {
    "objectID": "materials/19_deep_learning/1_class_notes.html",
    "href": "materials/19_deep_learning/1_class_notes.html",
    "title": "Deep learning",
    "section": "",
    "text": "Deep learning is a subset of machine learning that is somehow inspired by the structure and function of the human brain. Following the logic of small units interconnected, like neurons in the brain, these models are called deep neural networks because they have many layers of interconnected nodes, or “neurons,” that allow them to learn increasingly complex patterns from the data. Deep learning is used in a variety of applications such as image recognition or natural language processing.\nA good way to understand the novelty of these models is to compare them with the simplest and most widespread model that we know, linear regressions. There are several key differences between neural networks and linear regression. Many of these concepts are specific to deep learning, and we will learn about them in this module:\n\nComplexity: Neural networks are capable of modeling a much wider range of problems than linear regression.\nInputs and outputs: Neural networks can have multiple inputs and multiple outputs, whereas linear regression typically has only one output. This also applies to the dimensions of both input and outputs: while linear models can only have a vector as input, neural networks can receive and produce all sorts of data, like images or text.\nNon-linearity: Neural networks are non-linear models, meaning that the outputs are not directly proportional to the inputs, as it happens with the linear regressions.\nNumber of layers: Neural networks can have multiple layers, each with its own set of weights and biases. Linear regression, on the other hand, has only one layer.\nTraining: Neural networks require a lot of data to be trained, and this is done through an iterative process known as backpropagation. Linear regressions, on the other hand, can be trained with a smaller amount of data and it is typically trained using a closed-form solution.\nFlexibility: Neural networks are more flexible and can be adapted to different tasks, such as image classification, speech recognition, or natural language processing. Linear regression is a simpler model and it is mostly used for predicting numerical values.\nExplainability: Linear regression models are considered to be more explainable than neural networks because the relationships between inputs and outputs are easy to understand. The coefficients of the linear regression model can be used to determine the relative importance of the inputs to the output. On the other hand, the inner workings of a neural network, such as the values of the weights and biases, are difficult to interpret and understand. There are techniques that aim to make neural networks explainable, but they are limited."
  },
  {
    "objectID": "materials/19_deep_learning/1_class_notes.html#the-neuron",
    "href": "materials/19_deep_learning/1_class_notes.html#the-neuron",
    "title": "Deep learning",
    "section": "The neuron",
    "text": "The neuron\nIn the traditional neural networks, the basic unit of computation is really simple. The neuron starts with the weighted sum of all the inputs it receives. These weights are the parameters of the model, which need to be optimized. A bias is added to this sum —also a parameter of the model—, and this sum goes through a non-linear activation function. This means that if the input is sufficiently strong, the neuron will activate, and will create an output.\n\nIn summary, a neuron has:\n\nInputs: Usually, features of a dataset.\nWeights (to be optimized through training): The real values associated with the features, expressing their relative importance.\nA bias (also to be optimized): Allows to shift the activation function towards the positive or negative side (it is the equivalent to the constant in a linear function).\nAn activation function (there are several possible activation functions, and the choosing one is an hyper-parameter of the model): Required to add non-linearity to the model."
  },
  {
    "objectID": "materials/19_deep_learning/1_class_notes.html#the-network",
    "href": "materials/19_deep_learning/1_class_notes.html#the-network",
    "title": "Deep learning",
    "section": "The network",
    "text": "The network\nA fully connected neural network (FCNN) contains:\n\nAn input layer that has the input data of the observation. The size of this layer is determined by the dimensionality of the input.\nAn output layer which encodes the predicted values. The size of this layer is determined by the dimensionality of the output.\nA number of hidden layers. The first hidden layer receives the original inputs, while the following layers receive as inputs the outputs of the previous layers. This composition of layers creates a system with increasing levels of abstraction. The number of nodes in each hidden layer as well as the number of hidden layers is arbitrary (a hyper-parameter)."
  },
  {
    "objectID": "materials/19_deep_learning/1_class_notes.html#activation-functions",
    "href": "materials/19_deep_learning/1_class_notes.html#activation-functions",
    "title": "Deep learning",
    "section": "Activation functions",
    "text": "Activation functions\nThe activation functions are a central element of a network because they introduce the non-linearity into the model. This allows the neural networks to be more expressive, in the sense that they can represent complex relations between the combination of different inputs and the output.\nWhen they are used as part of a large network, each node in a hidden layer receives a linear combination of the non-linear output of the previous hidden layer, and creates a new non-linear signal —using its activation function— to be sent to the next hidden layer. This process allows the network to capture multiple complex relations.\nSome of the most common functions used are the Rectified Linear Unit (ReLU), the Sigmoid, and the Softmax1.\nTheir definitions are actually quite simple, and we can write a one-line function for each of them to see how they look.\n\nrelu &lt;- function(x) ifelse(x &gt;= 0, x, 0)\nsigmoid &lt;- function(x) 1 / (1 + exp(-x))\nsoftmax &lt;- function(x) exp(x) / sum(exp(x))\n\n\n\n\n\n\n\n\n\n\n\n\nWhich activation function to use? There is no single answer, but here are some tips:\nReLU’s are really popular, but they are only meant to be used on hidden layers.\nThe crucial decision is which activation function to use for the final (output) layer, and this depends of the type of problem:\n\nIf your prediction is a binary classification, you can map to the probability space using a Sigmoid.\nIf you have a classification problem with multiple categories, you can use a Softmax, as this will create a probability vector.\nIf the problem is a regression, you can use the Identity (which is actually linear)."
  },
  {
    "objectID": "materials/19_deep_learning/1_class_notes.html#backpropagation",
    "href": "materials/19_deep_learning/1_class_notes.html#backpropagation",
    "title": "Deep learning",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nThe weights of the network are initialized with random values, and are used to make a prediction based on the input data.\nA loss function2 is defined to measure how well the model is performing. This loss function takes the predicted output and the true output, and returns a value that indicates how far is the prediction from the true output (the error).\nThe error is then backpropagated through the network, which means that the error is used to adjust the weights of the network in a way that reduces the error. This is made using an optimization algorithm, such as gradient descent. This process is repeated until the error is minimized to an acceptable level."
  },
  {
    "objectID": "materials/19_deep_learning/1_class_notes.html#gradient-descent",
    "href": "materials/19_deep_learning/1_class_notes.html#gradient-descent",
    "title": "Deep learning",
    "section": "Gradient descent",
    "text": "Gradient descent\nThere are several possible optimizers used in deep learning. One of the first to be used was gradient descent, which takes the following steps:\n\nThe gradients of the loss with respect to the parameters are calculated using the chain rule3. The gradients indicate the direction in which the parameters should be adjusted in order to minimize the loss.\nThe parameters are updated in the direction that minimizes the loss. This is done by taking a small step (called the learning rate) in the opposite direction of the gradients.\n\nIn practice, the gradient descent is very costly and it’s not used. But several different implementations that share it’s spirit, such as Stochastic Gradient Descent or Adam are used.4"
  },
  {
    "objectID": "materials/19_deep_learning/1_class_notes.html#batches",
    "href": "materials/19_deep_learning/1_class_notes.html#batches",
    "title": "Deep learning",
    "section": "Batches",
    "text": "Batches\nWhen training a neural network, the backpropagation is done through batches of data. This means that the computation of the loss, the gradient, and the update of the parameters is done over a subset of the training data. The amount of observations used at each step is known as batch size, and it can be adjusted as an hyper-parameter5.\nOnce the training has done enough batches to go through all the data points, it has completed an epoch. But the training does not stop there. We can train the model by running several epochs. This is yet another hyper-parameter to set6."
  },
  {
    "objectID": "materials/19_deep_learning/1_class_notes.html#dropout",
    "href": "materials/19_deep_learning/1_class_notes.html#dropout",
    "title": "Deep learning",
    "section": "Dropout",
    "text": "Dropout\nNeural networks are very flexible models7 and have a lot of parameters. This makes them prone to overfitting. One of the most important techniques to prevent this is the dropout, which limits the learning capacity that the model has over the training data on every batch, randomly, so it can learn a more general representation of the process.\n\n\n\nDropout (Srivastava et al. 2014)"
  },
  {
    "objectID": "materials/19_deep_learning/1_class_notes.html#images-as-data",
    "href": "materials/19_deep_learning/1_class_notes.html#images-as-data",
    "title": "Deep learning",
    "section": "Images as data",
    "text": "Images as data\nBefore we move into CNNs, we need to understand how images can be represented as data.\nThe MNIST database is a large database of handwritten digits that is commonly used for training various image processing systems. This classic benchmark has 60K images of 28x28 pixels.\n\nEach black & white image can be thought of as a matrix. Each cell of the matrix represents a pixel, and the value is a representation of the grey scale, between 0 (white) and 255 (black).\n\nThis will completely change the logic of machine learning models. Until now, the observations had a vector of features \\({x_1,x_2,x_3...x_n}\\) and a expected output \\({y}\\). Now, an image (matrix) can be an observation’s set of features. Convolutions are operations over matrices, and hence can deal with this new type of data."
  },
  {
    "objectID": "materials/19_deep_learning/1_class_notes.html#what-is-a-convolution",
    "href": "materials/19_deep_learning/1_class_notes.html#what-is-a-convolution",
    "title": "Deep learning",
    "section": "What is a convolution?",
    "text": "What is a convolution?\nA convolution is a mathematical operation between two matrices, where a small matrix, the filter or kernel, is slid over the input data. At each position, the dot product is computed, and its sum is the value used on that position on the resulting matrix.\n\n\n\nConvolution operation (Bluche 2017)\n\n\n\n\n\nMovement (Bansac 2018)\n\n\n\nThe values of the filter are like the weights of the FCNN, they are trained through backpropagation.\nEach layer of the network will have a set of filters.\nThe number of filters per layer, their dimension, the activation function used over the output of the convolution, and the stride8 of the convolution are hyper-parameters to define.\n\nThe filters are combined with activation functions, to work as detectors of an attribute in a region of image. They usually start by recognizing simple features, like lines and diagonals, and build up more complex representations in the final layers, such as faces (on face-recognition problems).\n\n\n\nFilters (Harris 2015)\n\n\nThe image received as input creates a new representation for each filter the layer has. These are aggregated through a sum, and the bias is added before going through the activation function.\n\n\n\nFilter composition(Bai 2019)\n\n\n\n\n\nKernels’ sum (Shafkat 2018)\n\n\n\n\n\nBias (Shafkat 2018)"
  },
  {
    "objectID": "materials/19_deep_learning/1_class_notes.html#max-pooling",
    "href": "materials/19_deep_learning/1_class_notes.html#max-pooling",
    "title": "Deep learning",
    "section": "Max-pooling",
    "text": "Max-pooling\nCNN layers are usually combined with a dimensionality reduction technique called max-pooling. This helps to reduce the size of the input, so the following layers just focus on the perceived attributes. Max-pooling also slides through the matrix and picks the maximum value of each region of the matrix. The size of the window and the stride are hyper-parameters of the network. For example, a 2x2 window would reduce the size of the matrix by half.\n\n\n\nMax-pooling (“Max-Pooling / Pooling - Computer Science Wiki” n.d.)\n\n\n\n\nSummary: In this lecture, we explored the basics of artificial neural networks (ANNs) and their architectures, with a focus on fully connected neural networks (FCNNs) and convolutional neural networks (CNNs). We began by discussing the fundamental building blocks of ANNs, including neurons, weights, biases, and activation functions. We then delved into FCNNs, which are composed of multiple layers of neurons that are fully connected to each other. We discussed how these networks learn the relationships between input and output data using backpropagation and gradient descent algorithms. Next, we explored CNNs, which are specifically designed to process image and video data. We discussed the intuition behind convolutional layers, which use filters to extract features from input data, and pooling layers, which reduce the spatial dimensionality of the data."
  },
  {
    "objectID": "materials/19_deep_learning/1_class_notes.html#footnotes",
    "href": "materials/19_deep_learning/1_class_notes.html#footnotes",
    "title": "Deep learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBut there are many others, you can check them here https://keras.io/api/layers/activations/↩︎\nThe loss function is a function composed by the model’s predicted output and the true output \\(f(\\hat{y}\\ ,y)\\). The model’s output is itself a composite function of the inputs and the model’s parameters \\(f(X,P)\\).↩︎\nThe chain rule states that the derivative of a composite function is equal to the product of the derivative of the outer function with respect to the inner function and the derivative of the inner function with respect to the parameter \\(h(x)=f(g(x)) \\rightarrow h'(x) =f'(g(x))g'(x)\\). This is used to disentangle the composite nature of the nested layers of the model.↩︎\nCheck https://keras.io/api/optimizers/ for the explanation of the different implementations↩︎\nThere is a trade-off. A very small batch-size would allow to use all the information that each observation provides, but it would be computationally expensive, and prone to overfitting.↩︎\nA nice trick is to use early stopping, which automatically stops the training when there are no more significant improvements. Check it here: https://keras.io/api/callbacks/early_stopping/↩︎\nThey are so flexible that they are universal aproximators, which means that no matter which is the real data generating process, there is a neural network that can approximately approach to it, replicating the results.↩︎\nThe number of pixels to move when sliding through the input matrix. For example, if a neural network’s stride is set to 2, the filter will move two pixels at a time.↩︎\nhttp://gendershades.org/↩︎"
  },
  {
    "objectID": "materials/3_version_control/1_class_notes.html",
    "href": "materials/3_version_control/1_class_notes.html",
    "title": "Version control and Documentation",
    "section": "",
    "text": "In this class, we will delve into two essential tools that facilitate code sharing and ensure its reproducibility. In the first place, we will delve into version control and collaboration using a very powerful tool: GitHub. Subsequently, we will shift our attention to code documentation through R Markdown. R Markdown will allow us to blend code, narratives, and visualizations into dynamic, easily comprehensible documents."
  },
  {
    "objectID": "materials/3_version_control/1_class_notes.html#your-user",
    "href": "materials/3_version_control/1_class_notes.html#your-user",
    "title": "Version control and Documentation",
    "section": "Your user",
    "text": "Your user\nIn order to use GitHub, the first step will be to create and set up our account5,6. We therefore need to create an account in GitHub and verify our email7.\nThe next step is to install Git8. We will also need to create a personal access token,9 the last step to authenticate our identity when interacting with GitHub from a device. The first time we try to interact with a remote GitHub repository from our device, GitHub will ask for our user name and our token10. In order to avoid authentication every single time we interact with GitHub, we will globally set up the user name11 and email12 associated with all modifications in our repository13."
  },
  {
    "objectID": "materials/3_version_control/1_class_notes.html#main-commands",
    "href": "materials/3_version_control/1_class_notes.html#main-commands",
    "title": "Version control and Documentation",
    "section": "Main commands",
    "text": "Main commands\nThe following list describes some of the main commands we will use when working with Git14:\n\n\n\nArtwork by @allison_horst\n\n\n\ngit clone: Clone (download) a repository that already exists on GitHub, including all of the files, branches, and commits.\ngit pull: It synchronizes the local development repository with updates from the corresponding remote repository on GitHub. In other words, it fetches and incorporates the modifications made by our team into our local repository.\ngit status: This command displays our current status, including the state of local changes, the branch we are on, and any other relevant information.\ngit add + git commit + git push: These commands will enable us to upload our local changes to the repository.\n\ngit add: This is the initial step where we add new or modified files in the local working directory to the Git staging area. This process prepares the files to be included in the next commit.\ngit commit -m”a descriptive message”: This command records our changes in the version history. Anything that has been staged using git add will be permanently stored in the history. Additionally, when using this command, we can include a descriptive message that explains the nature of the changes we made.\ngit push: Uploads all local commits to the remote repository."
  },
  {
    "objectID": "materials/3_version_control/1_class_notes.html#repositories",
    "href": "materials/3_version_control/1_class_notes.html#repositories",
    "title": "Version control and Documentation",
    "section": "Repositories",
    "text": "Repositories\nYou can easily create a new repository15 on your personal account using the \\(+\\) drop-down menu in the upper-right corner of GitHub, and selecting New repository. You will need to chose the repository’s name, add a brief description and decide on its visibility.\nYour repositories should be organized and structured in a similar manner to an R project (including separate directories for data, scripts, results, etc.). By structuring your repositories in this way, you can enhance the maintainability and reproducibility of your projects.\nHowever, we will also encounter some files that are specific to the functioning of Git.\n\nGitHub lets you add a README file when you create a new repository, which should contain information about your project. README files are written in the plain text Markdown language.16\n\n\nYou can configure Git to ignore files you don’t want to check in to GitHub (such as extremely large data sets). The .gitignore file in your repository will list which files and directories to ignore.\nPublic repositories on GitHub are often used to share open source software. In this case, you’ll need to license it so that others are free to use, change, and distribute the software.\n\nOnce you have created a repository, click the Code tab in the upper-left corner of of the screen, click on the green &lt;&gt; Code button (upper-right corner), choose the Local tab and copy the link under the HTTPS option. We will be using this clone URL to create a local copy of the repository in our device."
  },
  {
    "objectID": "materials/3_version_control/1_class_notes.html#choosing-how-to-interact-with-github",
    "href": "materials/3_version_control/1_class_notes.html#choosing-how-to-interact-with-github",
    "title": "Version control and Documentation",
    "section": "Choosing how to interact with GitHub",
    "text": "Choosing how to interact with GitHub\nThere are several interfaces and methods we can use to interact with GitHub. Feel free to chose what works best for your needs.\nWe will focus on how to integrate RStudio and Git17,18(Bryan 2021). In RStudio, we can start a new Project using: File &gt; New Project &gt; Version Control &gt; Git. In Repository URL, we will paste the clone URL of the GitHub repository we just copied. By default, R will use as project directory name the repository name. Note where the Project will be saved locally, or choose any folder you want using the Browse… button under Create project as subdirectory of. Once you are done, click Create Project.\nYou should find yourself in a new local RStudio Project which contains all files in your repository. In the environment pane you should now see a new tab named Git. After making any changes in your project (for example, creating a script and writing some code), modified or created files will appear listed in the Git tab. In order to upload any of them to your remote repository, you should:\n\nCheck the Staged box for the files you want to upload.\nClick Commit (a button on the upper-left corner of the Git tab featuring a check mark)19.\n\n\nA pop-up window will appear: write a brief description of your changes in the box under Commit message and click the Commit button.\nOnce the changes have been committed, you can upload them to GitHub using the Push button (an upwards green arrow)20.\n\nIf you wanted to bring remote changes of the repository to your local version, you can use the Pull button (an downwards light blue arrow)21."
  },
  {
    "objectID": "materials/3_version_control/1_class_notes.html#advanced-git",
    "href": "materials/3_version_control/1_class_notes.html#advanced-git",
    "title": "Version control and Documentation",
    "section": "Advanced git",
    "text": "Advanced git\n\nBranches and pull requests\nBranching lets you simultaneously have multiple different versions of a repository22,23. This is a helpful tool when you want to experiment and make edits without changing the main source of code. The work done on different branches will not show up on the main branch (or affect it) unless you merge it.\n\n\n\nGitHub Documentation\n\n\nWhen you open a pull request, you’re proposing your changes and requesting that someone review and pull in your contribution and merge them into their branch24. Once your branch is merged, you can safely delete it 25.\n\n\nForks\nA fork is a new repository that shares code and visibility settings with the original repository26, that is to say, a copy of the repository where we can make any modifications we like. However, a fork is not just a copy because after you fork a repository, you can fetch updates from the original repository to keep your fork up to date, and you can also propose the changes from your fork to the main repository using pull requests.\nAnybody can fork a public repository and propose changes which may or may not be accepted by the repository owners. This is a frequent collaboration dynamic in open source projects. Deleting a fork will not delete or affect in any way the original repository.\n\n\nJobs and workflows\nGitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your pipeline. It lets you trigger workflows (a configurable automated process) when certain events happen in your repository.\nA workflow run is made up of one or more jobs (a set of steps), which run in parallel by default. Each job either runs a script that you define, or an action (a custom application for the GitHub Actions platform that performs a complex but frequently repeated task) 27."
  },
  {
    "objectID": "materials/3_version_control/1_class_notes.html#why-r-markdown",
    "href": "materials/3_version_control/1_class_notes.html#why-r-markdown",
    "title": "Version control and Documentation",
    "section": "Why R Markdown?",
    "text": "Why R Markdown?\nSome reasons to use RMarkdown (Alzahawi 2021):\n\nAvoid the common errors (and time consumed) that result from analyzing data using a different software from the one used to communicate your results (no more copy/pasting and/or reformatting all results after every modification!). All results and citations in your reports will be automatically updated after any modification.\nImprove your team-work and code reproducibility by using a report structure specially designed for you to clearly explain your methodology and result interpretation as you navigate your way through the data analysis process.\nR Markdown provides the flexibility of sharing reports in two distinct ways. You can distribute a rendered version of your report with hidden code to those interested in a high-level overview of the procedure and results, while also sharing the .Rmd file itself with individuals who need to verify (or modify) the underlying code.\n\n\n\n\nArtwork by @allison_horst\n\n\nSome arguments against RMarkdown:\n\nThe learning curve can be steep.\nBarriers to collaborating with others: all your team must be able to use R/RMarkdown (and additional tools such as GitHub).\n\nCustomizing the R Markdown format can be both a fun and time-consuming task. However, the good news is that a variety of templates and themes are readily available to simplify this process28."
  },
  {
    "objectID": "materials/3_version_control/1_class_notes.html#creating-an-r-markdown-document",
    "href": "materials/3_version_control/1_class_notes.html#creating-an-r-markdown-document",
    "title": "Version control and Documentation",
    "section": "Creating an R Markdown document",
    "text": "Creating an R Markdown document\nWe can create a new R Markdown file in RStudio using: File &gt; New File &gt; R Markdown….\nWhile editing your R Markdown (.Rmd) file, you have the option to work in either Source mode or Visual mode. The Source format in RStudio displays the raw, unrendered content of an .Rmd file, showing the actual code and Markdown syntax. In contrast, the Visual format provides a preview of the document with the Markdown elements transformed into their formatted representation, allowing you to see how the final document will appear when rendered.\n\nGeneral set up\nYAML metadata is placed at the very beginning of the document and contains basic metadata about the document (title, author, date, etc.) and rendering instructions (mainly the output type and its settings) (Xie, Dervieux, and Riederer 2020). For example:\n---\ntitle: \"My R Markdown Report\"\nauthor: \"My name\"\noutput: \"html_document\"\n---\nIt’s important to highlight that R Markdown supports many output document and presentation formats (such as .doc, .pdf, etc.). Visit this site for the full list.\nAn R Markdown document includes prose (narratives) and code. Your prose will not be evaluated as an R code, and must be formatted according to the Markdown syntax we will discuss later. On the other hand, there are two types of code in an Rmd document: code chunks and inline R code (Xie, Dervieux, and Riederer 2020).\nInline R code can be is embedded in the narratives of the document using the syntax `r your_code`. This feature is useful to refer to your results with no need to manually update your observations every time your results change.\nA code chunk29 starts with ```{r} and ends with ```. You can write any number of lines of code in it. For example:\n::: {.cell}\n\n```{.r .cell-code}\na &lt;- 1 + 1\nb &lt;- 1 + 3\na + b\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6\n```\n\n\n:::\n:::\nCode chunks can be labelled (the chunk label for each chunk is assumed to be unique within the document) and it is also possible to set up their options in order to decide whether the code will be evaluated30, included in our report31, hidden keeping its results32, etc. We can combine different conditions separating them with a comma. For example: {r label = my_label, eval = FALSE, include = TRUE, echo = TRUE}.\nYou may use knitr::opts_chunk$set() to change the default values of chunk options in the whole document (this will usually be our first code chunk).\n\n\nKnitting\nIn order to create our output file (html document, pdf, word, etc.) we will use the Knit command33 on the top of our source pane, which will execute all code embedded within the .Rmd file, and prepare the code output to be displayed within the specified output document type. The whole document will be converted to the output format specified in the output parameter."
  },
  {
    "objectID": "materials/3_version_control/1_class_notes.html#markdown-syntax",
    "href": "materials/3_version_control/1_class_notes.html#markdown-syntax",
    "title": "Version control and Documentation",
    "section": "Markdown syntax",
    "text": "Markdown syntax\n.Rmd files integrate the R and Markdown languages. Markdown is a markup language for creating formatted text using a plain-text editor with simple, unobtrusive syntax34.\nText will be italic if surrounded by underscores or asterisks, e.g., _text_ or *text*. Bold text is produced using a pair of double asterisks (**text**). A pair of tildes (~) turn text to a subscript and a pair of carets (^) produce a superscript.\nSection headers can be written after a number of # signs:\n# First-level header\n\n## Second-level header\n\n### Third-level header\nUnordered list items start with *, -, or +, and you can nest one list within another list by indenting the sub-list:\n- one item\n- one item\n- one item\n    * one more item\n    * one more item\n    * one more item\nOrdered list items start with numbers (you can also nest lists within lists):\n1. the first item\n2. the second item\n3. the third item\nHyperlinks are created using the syntax [text](link), e.g., [RStudio](https://www.rstudio.com). The syntax for images is similar: we just need to add an exclamation mark, e.g., ![alt text or image title](path/to/image.png). Footnotes are put inside the square brackets after a caret ^[] (Xie, Allaire, and Grolemund 2019).\nMath expressions can be written using inline LaTeX equations between a pair of dollar signs using the LaTeX syntax, such as $\\frac{1}{2} + \\frac{1}{2} = 1$, which renders \\(\\frac{1}{2} + \\frac{1}{2} = 1\\)"
  },
  {
    "objectID": "materials/3_version_control/1_class_notes.html#footnotes",
    "href": "materials/3_version_control/1_class_notes.html#footnotes",
    "title": "Version control and Documentation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThroughout this document, we will heavily rely on GitHub’s documentation files (GitHub, n.d.).↩︎\nWe will get back to this subject when discussing branches.↩︎\nOn the other hand, GitHub also functions as a social network, allowing users to showcase their work and contributions. People may explore your GitHub profile to gain insights into your professional portfolio and the projects you’ve been involved in.↩︎\nThere are, however, limits to the size of the files that can be uploaded to GitHub. Nonetheless, it’s a good practice to keep all the files we use in our workflow within the same working repository (including large files). While we may not upload everything to GitHub (as explained further below, this can be set up using the .gitignore file), we can share large files through other platforms, such as Dropbox, or provide clear indications of the source from which we obtained these files (so that anyone collaborating on our code can easily access the same materials).↩︎\nGitHub’s documentation to help you get started.↩︎\nIn this document, we will describe how to authenticate and clone repositories using HTTPS (as recommended by Git), you can find instructions to use SSH instead here.↩︎\nTrouble verifying your email?↩︎\nNeed help?(https://github.com/git-guides/install-git)↩︎\nYou can find a step by step guide to set up your personal access token here.↩︎\nWe should see the following output when trying to use a command:  Username: Your user name  Password: Your personal access token↩︎\nWe will use the command:  git config –global user.name “Your user name”↩︎\nWe will use the command:  git config –global user.email “Your email”↩︎\nIf you are using a Mac, you will also need to store your password.↩︎\nYou can find more commands in the Git cheat sheet or the Visual Git cheat sheet.↩︎\nFor more details, here is a step by step tutorial.↩︎\nA cheat sheet to help you out.↩︎\nMore details on how to integrate RStudio and Git. Something went wrong? (Bryan 2021)↩︎\nOther popular options are: visual studio code and github desktop.↩︎\n↩︎\n↩︎\n↩︎\nYou can find a step by step guide to create a branch here.↩︎\nOnly users with read access to a repository can create a branch, and only users with write access to a repository can push a branch.↩︎\nYou can find a step by step guide to open a pull request here.↩︎\nYou can find a step by step guide to delete a branch here.↩︎\nYou can find a step by step guide to fork a repository here.↩︎\nA workflow example.↩︎\nConsider, for example, those provided in the following sites: https://rmarkdown.rstudio.com/gallery.html https://juba.github.io/rmdformats/ https://www.datadreaming.org/post/r-markdown-theme-gallery/ https://rpubs.com/ranydc/rmarkdown_themes↩︎\nUse RStudio keyboard shortcuts to create chunks.↩︎\nIf we set eval = TRUE, the code will be evaluated. Otherwise, when using eval = FALSE it will not.↩︎\nIf we set include = TRUE, the code will be included. Otherwise, when using include = FALSE it will not. However, the code will still be evaluated.↩︎\nIf we set echo = TRUE, both the code and its output will be shown. Otherwise, when using echo = FALSE only its output will be shown.↩︎\nMore details: what happens when we render?(Xie, Dervieux, and Riederer 2020)↩︎\nThis section relies on this book on how to use RMarkdown syntax and RMarkdown’s cheat sheet.↩︎\nYou can explore communities such as the R Community or R-Ladies.↩︎"
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html",
    "href": "materials/2_intro_II/1_class_notes.html",
    "title": "Introduction II",
    "section": "",
    "text": "Quantitative analysis can be a very powerful tool to understand systemic phenomena. By translating similar experiences shared by large groups of people into data, we can identify repeating patterns. This allows us to contextualize our personal experiences and address issues from a systemic perspective1, leading to more effective solutions and positive change for ourselves and the communities we are a part of.\nHowever, data are not neutral or objective. They are the products of unequal social relations, and understanding this context is essential for conducting accurate, ethical analysis.\nIt is important to critically analyze data in order to avoid replicating the biases that may be inherent in the data. Standard practices in data science may serve to reinforce these existing inequalities (D’ignazio and Klein 2023).\n\n\nThe demographics of data science, as well as related occupations such as software engineering and artificial intelligence research, are not representative of the population as a whole. The vast majority of individuals working in these fields are elite white men.\nIt is important to keep in mind that science is shaped by the unique perspectives of individual scientists, and therefore researchers’ subjectivities and biases can influence the way data sets are built and later analyzed. Therefore, it is necessary to be transparent about the context in which research is conducted as well as potential sources of bias, thus acknowledging the partiality of our perspectives.\nThe following example illustrates the biases that may result from the work of non-diverse groups. A Ghanaian-American graduate student at MIT, Joy Buolamwini, experienced a problem when working on a class project using facial-analysis software, as the software could not detect her face but had no problem detecting the faces of her lighter-skinned collaborators. She discovered that the system’s facial recognition features worked perfectly when she put on a white mask. Later, she found the data set on which many of facial-recognition algorithms are tested contains 78 percent male faces and 84 percent white faces (D’ignazio and Klein 2023).\n\n\n\nJoy Buolamwini’ experiment\n\n\n\n\n\nData that could be used to address critical social issues may be missing2, but powerful institutions may also create databases and data systems through excessive surveillance of marginalized groups.\nHaving more data on a subject may not always have a positive impact. The paradox of exposure illustrates the complexity of this matter. This term describes a paradoxical situation in which individuals who have the most to gain from being counted or classified are also the most vulnerable to the potential dangers posed by that very act of counting or classifying. For example: undocumented immigrants are less likely to complete the census questionnaire due to fear of deportation, which leads to undercounting in the census. This results in less political representation and federal funding being allocated to these groups, and therefore less voting power and fewer resources for them (D’ignazio and Klein 2023).\nFurthermore, the effectiveness of data analysis in addressing societal issues is heavily influenced by the quality of the input data. In this regard, it is fundamental to analyze which variables are selected, as well as their categories. A clear example of this challenge is racial classification: while it can help understanding and fighting racial stratification, it can also be used to preserve it. The interpretation and impact of these types of variables on data analysis are dependent on various factors, such as the role of critical or intersectional perspectives within research teams, the causal theories guiding the empirical analysis or the contextualized or decontextualized nature of the interpretation of results. Therefore, racial data can play a crucial role in addressing racial inequality, but this does not mean every statistic should be presented racially. In fact, some racial statistics have contributed to racial conflict by implying that race determines behavior (Zuberi 2001)."
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#biases-and-objectivity",
    "href": "materials/2_intro_II/1_class_notes.html#biases-and-objectivity",
    "title": "Introduction II",
    "section": "",
    "text": "The demographics of data science, as well as related occupations such as software engineering and artificial intelligence research, are not representative of the population as a whole. The vast majority of individuals working in these fields are elite white men.\nIt is important to keep in mind that science is shaped by the unique perspectives of individual scientists, and therefore researchers’ subjectivities and biases can influence the way data sets are built and later analyzed. Therefore, it is necessary to be transparent about the context in which research is conducted as well as potential sources of bias, thus acknowledging the partiality of our perspectives.\nThe following example illustrates the biases that may result from the work of non-diverse groups. A Ghanaian-American graduate student at MIT, Joy Buolamwini, experienced a problem when working on a class project using facial-analysis software, as the software could not detect her face but had no problem detecting the faces of her lighter-skinned collaborators. She discovered that the system’s facial recognition features worked perfectly when she put on a white mask. Later, she found the data set on which many of facial-recognition algorithms are tested contains 78 percent male faces and 84 percent white faces (D’ignazio and Klein 2023).\n\n\n\nJoy Buolamwini’ experiment"
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#what-gets-counted-and-what-doesnt",
    "href": "materials/2_intro_II/1_class_notes.html#what-gets-counted-and-what-doesnt",
    "title": "Introduction II",
    "section": "",
    "text": "Data that could be used to address critical social issues may be missing2, but powerful institutions may also create databases and data systems through excessive surveillance of marginalized groups.\nHaving more data on a subject may not always have a positive impact. The paradox of exposure illustrates the complexity of this matter. This term describes a paradoxical situation in which individuals who have the most to gain from being counted or classified are also the most vulnerable to the potential dangers posed by that very act of counting or classifying. For example: undocumented immigrants are less likely to complete the census questionnaire due to fear of deportation, which leads to undercounting in the census. This results in less political representation and federal funding being allocated to these groups, and therefore less voting power and fewer resources for them (D’ignazio and Klein 2023).\nFurthermore, the effectiveness of data analysis in addressing societal issues is heavily influenced by the quality of the input data. In this regard, it is fundamental to analyze which variables are selected, as well as their categories. A clear example of this challenge is racial classification: while it can help understanding and fighting racial stratification, it can also be used to preserve it. The interpretation and impact of these types of variables on data analysis are dependent on various factors, such as the role of critical or intersectional perspectives within research teams, the causal theories guiding the empirical analysis or the contextualized or decontextualized nature of the interpretation of results. Therefore, racial data can play a crucial role in addressing racial inequality, but this does not mean every statistic should be presented racially. In fact, some racial statistics have contributed to racial conflict by implying that race determines behavior (Zuberi 2001)."
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#why-r",
    "href": "materials/2_intro_II/1_class_notes.html#why-r",
    "title": "Introduction II",
    "section": "Why R?",
    "text": "Why R?\n\nR is an open source software, which means that anyone can use, modify, and distribute it without cost. Additionally, R can easily be integrated with other programming languages (such as Python).\nR has a wide variety of functions that make it easy to import, manipulate, and analyze data. It is particularly useful for statistical analysis, data visualization, and machine learning.\nIt also has a large and active community of developers who contribute to the language and create useful packages (and who can help and support you when you encounter problems or have questions about the language!)."
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#the-basics",
    "href": "materials/2_intro_II/1_class_notes.html#the-basics",
    "title": "Introduction II",
    "section": "The basics",
    "text": "The basics\nWe will be using RStudio, an integrated development environment (IDE) designed to work with R (or Python).\nThe RStudio layout5:\n\nWhen working with R, the code is typically written in the source pane (1), although it is also possible to write code directly into the console (2). However, it is important to note that any code written in the console will not be saved.\nTo execute a line (or lines) of code, simply select the code chunk and press Ctrl + Enter (Windows) or Command + Return (Mac). It is also possible to run all the code on the file using the Source command (top right).\nThe results of the executed code are displayed in the console (2).\nThe environment pane (3) displays currently imported and created R objects.\nThe output pane (4) displays the generated plots, file tabs, packages, and help information. The basic command to find out what a function does or simply ask for help is help(“function name”) or ?(“function name”).\nTo modify the appearance of the execution environment, such as changing the colors or switching to “dark mode”, use Tools &gt; Global Options &gt; Appearance6.\nTo create a new script to work on, you can click on File &gt; New File &gt; R Script, or use the keyboard shortcut Ctrl + Shift + N (Windows) or Shift + Command + N (Mac)7.\nKeep in mind that objects created in R are not automatically saved on your computer. Specific functions must be used depending on the object type and the desired format for exporting it (e.g.: .xlsx tables, .png plots, etc.).\nFor more information on RStudio’s panes, visit this site\nRStudio also has a feature called projects, which is a way of compartmentalizing your R code and keeping all the files associated with a project together — input data, R scripts, analytical results, figures. Each project is linked to a working directory, which is the folder where R searches when loading and saving files89. Keep in mind that file paths will be expressed in relation to the project’s location on the computer, but it’s also possible to specify absolute paths (from the C: or D: drive of our computer).\nYou can create an RStudio project using File &gt; New Project:\n\nIn a brand new directory\nIn an existing directory where you already have R code and data\nBy cloning a version control (Git or Subversion) repository\n\nThis action will create a project file (with an .Rproj extension), which you can later open in order to open your project."
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#structuring-a-project",
    "href": "materials/2_intro_II/1_class_notes.html#structuring-a-project",
    "title": "Introduction II",
    "section": "Structuring a project",
    "text": "Structuring a project\nA good practice to structure a project directory is keeping your input, code and outputs in separate folders. Folder and file names should always be descriptive and easy to remember. For example:\nmy_project/\n    data/\n    code/\n    results/\n    report/\nThis folder arrangement will simplify the file import and export process."
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#final-general-setup-advice",
    "href": "materials/2_intro_II/1_class_notes.html#final-general-setup-advice",
    "title": "Introduction II",
    "section": "Final general setup advice",
    "text": "Final general setup advice\nBy default, R will save your environment to a .RData file after you quit a session and reload it when you start a new session. This is not recommended because it might delay start-up, or even generate an endless crash cycle if a previous session crashed. It might also make your work harder to reproduce. To change this setting, got to: Tools &gt; Global Options &gt; General, and untick “Restore .RData into workspace at startup” and select the option “Never” for “Save workspace to .RData on exit:”.\nR will also save a log of all commands entered into the console during an R session in a .Rhistory file. To change this setting untick the “Always save history (even when not saving .RData)” option in the same menu."
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#file-types-.r-and-.rmd",
    "href": "materials/2_intro_II/1_class_notes.html#file-types-.r-and-.rmd",
    "title": "Introduction II",
    "section": "File types: .R and .Rmd",
    "text": "File types: .R and .Rmd\n\n.R files contain plain code that can be executed by R (including commands you create objects, transform them, create visualizations, save objects, etc.).\n\n\nWhen should we use .R files?\n.R files are useful to carry out the first steps of our data-processing workflow: importing and exploring the data base, and starting to designing the elements we will use to communicate our results (tables, graphs, etc.).\n\n\n.Rmd files integrate the R and Markdown languages10. Markdown is a markup language for creating formatted text using a plain-text editor with simple, unobtrusive syntax.11\n\n\nWhen should we use .Rmd files?\n.Rmd files are designed to contain your code and also the narrative surrounding the data. Therefore, they facilitate teamwork and are also useful to communicate our results.\n\n\n\n\nArtwork by @allison_horst\n\n\n\nWhy use RMarkdown?(Alzahawi 2021)\n\nAvoid the common errors (and time consumed) that result from analyzing data using a different software from the one used to communicate your results (no more copy/pasting and/or reformatting all results after every modification!). All results and citations in your reports will be automatically updated after any modification.\nImprove your team-work and code reproducibility by using a report structure specially designed for you to clearly explain your methodology and result interpretation as you navigate your way through the data analysis process.\nSome arguments against RMarkdown:\n\nThe learning curve can be steep.\nBarriers to collaborating with others: all your team must be able to use R/RMarkdown (and additional tools such as GitHub)."
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#packages",
    "href": "materials/2_intro_II/1_class_notes.html#packages",
    "title": "Introduction II",
    "section": "Packages",
    "text": "Packages\nIn R, the fundamental unit of shareable code is the package. A package bundles together code, data, documentation, and tests, and is easy to share with others. As of March 2023, there were over 19,000 packages available on the Comprehensive R Archive Network, or CRAN, the public clearing house for R packages (Wickham 2023).\n\nWhy are packages useful?\nSomeone has probably already solved the problem you’re working on, and you can benefit from their work by downloading their package12."
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#r-syntax",
    "href": "materials/2_intro_II/1_class_notes.html#r-syntax",
    "title": "Introduction II",
    "section": "R syntax",
    "text": "R syntax\nBefore we start coding, some general rules about the R language13:\n\nR is case-sensitive: variable1 and VARIABLE1 will be interpreted as two different objects.\nWhitespaces and new lines: R will ignore whitespaces and new lines inside an expression, using them to delimit expressions only in case of ambiguity. If an expression can terminate at the end of the line the parser will assume it does so. Neutral new lines will be useful to make our code easier to understand.\nComments in our code should be preceded by #. Otherwise, every line we write will be evaluated as a line of code. Comments will be useful to explain our code to future users."
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#operators",
    "href": "materials/2_intro_II/1_class_notes.html#operators",
    "title": "Introduction II",
    "section": "Operators",
    "text": "Operators\nAssignment\nAssignment operators are used to define objects, that is, to assign them a value.\n-&gt;: Right assignment\n&lt;-: Left assignment\n=: Left assignment\nFor left (right) assignment operators, the name of the object should be placed to the left (right) of the operator, while the definition of the object should be placed to the right (left). For example:\n\nvalue_1 &lt;- 1\n\nWhen an object is defined in R, it is saved in the program’s environment and can be accessed and used later in the code. When running a line with the name of an object, its content is displayed in the console.\n\nvalue_1\n\n 1\n\n\nIt is important to keep in mind that if an object is defined with the same name as a previously existing object, the latter replaces the former.\n\nvalue_1 &lt;- 1\nvalue_1 &lt;- 2\n\nvalue_1\n\n 2\n\n\nRelational\nRelational operators are used to describe relationships between objects, which are expressed as true (TRUE) or false (FALSE)14.\n&gt;: Greater than\n&gt;=: Greater than or equal to\n&lt;: Less than\n&lt;=: Less than or equal to\n==: Equal to\n!=: Not-equal to\nFor example:\n\n4==5\n\n FALSE\n\n\n\n4==4\n\n TRUE\n\n\n\n4&gt;=5\n\n FALSE\n\n\nLogical\nLogical operators are used to combine logical expressions.\n!: Not\n&: And\n|: Or\nConsidering the examples mentioned above, these are some of the expressions we can create combining them:\n\n4==5 & 4==4\n\n FALSE\n\n\n\n4==5 | 4==4\n\n TRUE\n\n\n\n4==5 | !(4==4)\n\n FALSE\n\n\nArithmetic\nSome of R’s basic arithmetic operators:\n+: Plus\n-: Minus\n*: Multiplication\n/: Division\n^: Exponentiation\nExample of usage:\n\n(4+4)*8\n\n 64\n\n\nCombinations!\nOperators can be combined to create increasingly complex expressions:\n\n(2*8 == 4^2 | 3*3 &lt;= 2^3) & 3 != 2\n\n TRUE"
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#r-objects",
    "href": "materials/2_intro_II/1_class_notes.html#r-objects",
    "title": "Introduction II",
    "section": "R objects",
    "text": "R objects\nWhile R objects might appear somewhat abstract at this point, we will delve into specific examples during the guided practice to provide a clearer understanding.\nValues\nValues in R are the smallest objects, which will be used as building blocks for everything else. The main value types are:\n\n“numeric” for any numerical value (such as 3.5, -.4).\n“character” for text values, denoted by using quotes around a value (for example: “x” or ‘x’). They can include any printable character (for example: “a”, “Hi!” or “#2.5%jk%?”).\n“logical” for TRUE and FALSE (the Boolean data type).\n“integer” for integer numbers (the qualifier L at the end of a number indicates to R that it’s an integer, for example: 4L).\n“complex” to represent complex numbers with real and imaginary parts (such as 2i or 3+8i).\n\nVectors\nA vector is a set of values of the same class. There may be numeric, character, etc. vectors. To create a vector, we use the c() command (which stands for ‘combine’).\nA special case is the factor type vectors. They are generally used for ordinal data. That is, for a qualitative variable for which we need to establish a certain order in its possible values.\nData Frames\nA data frame is a two-dimensional data structure or table, where each column represents a variable, and each row represents an observation. Data frames can contain data of different classes.\nIt can be considered as a set of equally sized vectors, where each vector (column) should contain data of the same type. However, the classes of vectors that make up the table can be different. Therefore, each observation (row) can be composed of data that can be of different types.\nData frames are a fundamental object in the R programming language, as they are the most commonly used structure for data analysis. Data frames are also typically used for loading external data into the R environment and for exporting the results of our work.\nLists\nA list is a collection of objects of any type, including other lists, data frames, vectors, or individual values. While a vector contains values of a single type, a data frame contains vectors of different types. Similarly, a list can contain data frames, but can also contain other lists, vectors, or individual values, all at the same time. Lists are a flexible and powerful data structure in R that can be used to organize complex and diverse data."
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#your-user",
    "href": "materials/2_intro_II/1_class_notes.html#your-user",
    "title": "Introduction II",
    "section": "Your user",
    "text": "Your user\nIn order to use GitHub, the first step will be to create and set up our account19,20. We therefore need to create an account in GitHub and verify our email21.\nThe next step is to install Git22. We will also need to create a personal access token,23 the last step to authenticate our identity when interacting with GitHub from a device. The first time we try to interact with a remote GitHub repository from our device, GitHub will ask for our user name and our token24. In order to avoid authentication every single time we interact with GitHub, we will globally set up the user name25 and email26 associated with all modifications in our repository27."
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#main-commands",
    "href": "materials/2_intro_II/1_class_notes.html#main-commands",
    "title": "Introduction II",
    "section": "Main commands",
    "text": "Main commands\nThe following list describes some of the main commands we will use when working with Git28:\n\n\n\nArtwork by @allison_horst\n\n\n\ngit clone: Clone (download) a repository that already exists on GitHub, including all of the files, branches, and commits.\ngit pull: It synchronizes the local development repository with updates from the corresponding remote repository on GitHub. In other words, it fetches and incorporates the modifications made by our team into our local repository.\ngit status: This command displays our current status, including the state of local changes, the branch we are on, and any other relevant information.\ngit add + git commit + git push: These commands will enable us to upload our local changes to the repository.\n\ngit add: This is the initial step where we add new or modified files in the local working directory to the Git staging area. This process prepares the files to be included in the next commit.\ngit commit -m”a descriptive message”: This command records our changes in the version history. Anything that has been staged using git add will be permanently stored in the history. Additionally, when using this command, we can include a descriptive message that explains the nature of the changes we made.\ngit push: Uploads all local commits to the remote repository."
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#repositories",
    "href": "materials/2_intro_II/1_class_notes.html#repositories",
    "title": "Introduction II",
    "section": "Repositories",
    "text": "Repositories\nYou can easily create a new repository29 on your personal account using the \\(+\\) drop-down menu in the upper-right corner of GitHub, and selecting New repository. You will need to chose the repository’s name, add a brief description and decide on its visibility.\nYour repositories should be organized and structured in a similar manner to an R project (including separate directories for data, scripts, results, etc.). By structuring your repositories in this way, you can enhance the maintainability and reproducibility of your projects.\nHowever, we will also encounter some files that are specific to the functioning of Git.\n\nGitHub lets you add a README file when you create a new repository, which should contain information about your project. README files are written in the plain text Markdown language.30\n\n\n\nYou can configure Git to ignore files you don’t want to check in to GitHub (such as extremely large data sets). The .gitignore file in your repository will list which files and directories to ignore.\nPublic repositories on GitHub are often used to share open source software. In this case, you’ll need to license it so that others are free to use, change, and distribute the software.\n\nOnce you have created a repository, click the Code tab in the upper-left corner of of the screen, click on the green &lt;&gt; Code button (upper-right corner), choose the Local tab and copy the link under the HTTPS option. We will be using this clone URL to create a local copy of the repository in our device."
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#choosing-how-to-interact-with-github",
    "href": "materials/2_intro_II/1_class_notes.html#choosing-how-to-interact-with-github",
    "title": "Introduction II",
    "section": "Choosing how to interact with GitHub",
    "text": "Choosing how to interact with GitHub\nThere are several interfaces and methods we can use to interact with GitHub. Feel free to chose what works best for your needs.\nWe will focus on how to integrate RStudio and Git31,32(Bryan 2021). In RStudio, we can start a new Project using: File &gt; New Project &gt; Version Control &gt; Git. In Repository URL, we will paste the clone URL of the GitHub repository we just copied. By default, R will use as project directory name the repository name. Note where the Project will be saved locally, or choose any folder you want using the Browse… button under Create project as subdirectory of. Once you are done, click Create Project.\nYou should find yourself in a new local RStudio Project which contains all files in your repository. In the environment pane you should now see a new tab named Git. After making any changes in your project (for example, creating a script and writing some code), modified or created files will appear listed in the Git tab. In order to upload any of them to your remote repository, you should:\n\nCheck the Staged box for the files you want to upload.\nClick Commit (a button on the upper-left corner of the Git tab featuring a check mark).\n\n\n\nA pop-up window will appear: write a brief description of your changes in the box under Commit message and click the Commit button.\nOnce the changes have been committed, you can upload them to GitHub using the Push button (an upwards green arrow).\n\nIf you wanted to bring remote changes of the repository to your local version, you can use the Pull button (an downwards light blue arrow)."
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#advanced-git",
    "href": "materials/2_intro_II/1_class_notes.html#advanced-git",
    "title": "Introduction II",
    "section": "Advanced git",
    "text": "Advanced git\n\nBranches and pull requests\nBranching lets you simultaneously have multiple different versions of a repository33,34. This is a helpful tool when you want to experiment and make edits without changing the main source of code. The work done on different branches will not show up on the main branch (or affect it) unless you merge it.\n\n\n\nGitHub Documentation\n\n\nWhen you open a pull request, you’re proposing your changes and requesting that someone review and pull in your contribution and merge them into their branch35. Once your branch is merged, you can safely delete it 36.\n\n\nForks\nA fork is a new repository that shares code and visibility settings with the original repository37, that is to say, a copy of the repository where we can make any modifications we like. However, a fork is not just a copy because after you fork a repository, you can fetch updates from the original repository to keep your fork up to date, and you can also propose the changes from your fork to the main repository using pull requests.\nAnybody can fork a public repository and propose changes which may or may not be accepted by the repository owners. This is a frequent collaboration dynamic in open source projects. Deleting a fork will not delete or affect in any way the original repository."
  },
  {
    "objectID": "materials/2_intro_II/1_class_notes.html#footnotes",
    "href": "materials/2_intro_II/1_class_notes.html#footnotes",
    "title": "Introduction II",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIs data enough? In 2015, communications researcher Candice Lanius wrote a widely shared blog post, Fact Check: Your Demand for Statistical Proof is Racist in which she summarizes the ample research on how those in positions of power accept anecdotal evidence from those like themselves, but demand endless statistics from minoritized groups. In those cases, she argues, more data will never be enough.↩︎\n“The Library of Missing Datasets is a physical repository of those things that have been excluded in a society where so much is collected. The word missing is used to imply both a lack and an ought: something does not exist, but it should.”↩︎\nYou can find some solutions to frequent problems here.↩︎\nHere you can find some suggestions on how to write more readable code.↩︎\nIt is possible to change the default location of the panes.↩︎\nMore info on customizing RStudio.↩︎\nFor more keyboard shortcuts got to: Help &gt; Keyboard Shortcuts Help. Note that Mac and Windows users have different shortcuts.↩︎\nWhat happens when a project is opened?↩︎\nWhen we position ourselves between the quotation marks that define the location for searching or saving a file, pressing Tab in R will reveal the documents or folders residing within the folder we are actively working in. It’s always a good practice to search for files or folders in this manner instead of manually typing their names in order to prevent errors.↩︎\nThese class notes were written using an .Rmd file!↩︎\nResources on how to use RMarkdown.↩︎\nHow can I find the package I’m looking for? https://cran.r-project.org/web/views/↩︎\nFor a more exhaustive description of the R language, visit this site.↩︎\nRemember R is case-sensitive!↩︎\nThroughout this document, we will heavily rely on GitHub’s documentation files (GitHub, n.d.).↩︎\nWe will get back to this subject when discussing branches.↩︎\nOn the other hand, GitHub also functions as a social network, allowing users to showcase their work and contributions. People may explore your GitHub profile to gain insights into your professional portfolio and the projects you’ve been involved in.↩︎\nThere are, however, limits to the size of the files that can be uploaded to GitHub. Nonetheless, it’s a good practice to keep all the files we use in our workflow within the same working repository (including large files). While we may not upload everything to GitHub (as explained further below, this can be set up using the .gitignore file), we can share large files through other platforms, such as Dropbox, or provide clear indications of the source from which we obtained these files (so that anyone collaborating on our code can easily access the same materials).↩︎\nGitHub’s documentation to help you get started.↩︎\nIn this document, we will describe how to authenticate and clone repositories using HTTPS (as recommended by Git), you can find instructions to use SSH instead here.↩︎\nTrouble verifying your email?↩︎\nNeed help?(https://github.com/git-guides/install-git)↩︎\nYou can find a step by step guide to set up your personal access token here.↩︎\nWe should see the following output when trying to use a command:  Username: Your user name  Password: Your personal access token↩︎\nWe will use the command:  git config –global user.name “Your user name”↩︎\nWe will use the command:  git config –global user.email “Your email”↩︎\nIf you are using a Mac, you will also need to store your password.↩︎\nYou can find more commands in the Git cheat sheet or the Visual Git cheat sheet.↩︎\nFor more details, here is a step by step tutorial.↩︎\nA cheat sheet to help you out.↩︎\nMore details on how to integrate RStudio and Git. Something went wrong? (Bryan 2021)↩︎\nOther popular options are: visual studio code and github desktop.↩︎\nYou can find a step by step guide to create a branch here.↩︎\nOnly users with read access to a repository can create a branch, and only users with write access to a repository can push a branch.↩︎\nYou can find a step by step guide to open a pull request here.↩︎\nYou can find a step by step guide to delete a branch here.↩︎\nYou can find a step by step guide to fork a repository here.↩︎"
  },
  {
    "objectID": "materials/5_linear_regression/1_class_notes.html",
    "href": "materials/5_linear_regression/1_class_notes.html",
    "title": "Linear Regression",
    "section": "",
    "text": "In this session, we will delve into linear regression, renowned for its simplicity and popularity in statistical analysis. With its analytical solution, it serves as a cornerstone in understanding various statistical models, laying a solid foundation for comprehending more advanced techniques in data analysis and prediction."
  },
  {
    "objectID": "materials/5_linear_regression/1_class_notes.html#formula-and-coefficient-interpretation",
    "href": "materials/5_linear_regression/1_class_notes.html#formula-and-coefficient-interpretation",
    "title": "Linear Regression",
    "section": "Formula and coefficient interpretation",
    "text": "Formula and coefficient interpretation\nWhen building a simple linear regression model, we will fit a line to data where the relationship between two variables, \\(X\\) and \\(y\\), can be modeled as a straight line with some degree of error following the formula:\n\\[ y = \\beta_{0} + \\beta_{1}X + \\epsilon\\]\n\n\\(\\beta_{0}\\) represents the intercept. How can we interpret this value? It represents the average value of \\(y\\) when \\(X = 0\\).\n\\(\\beta_{1}\\) is the slope of the line, which indicates the average increases in \\(y\\) when \\(X\\) increases by one unit.\n\\(\\epsilon\\) represents the error or residual.\n\n\\(\\beta_{0}\\) and \\(\\beta_{1}\\) are the model’s coefficients or parameters. When fitting a line, we may ask ourselves, should we move the line slightly further up or further down? Should the line be steeper or less steep? The first question refers to estimating \\(\\beta_{0}\\). The second question refers to estimating \\(\\beta_{1}\\).\nHere’s a graphical representation of the role of \\(\\beta_{0}\\) and \\(\\beta_{1}\\):"
  },
  {
    "objectID": "materials/5_linear_regression/1_class_notes.html#numeric-vs.-analytic-calculation",
    "href": "materials/5_linear_regression/1_class_notes.html#numeric-vs.-analytic-calculation",
    "title": "Linear Regression",
    "section": "Numeric vs. analytic calculation",
    "text": "Numeric vs. analytic calculation\nWhen estimating the model’s parameters2, we will try to find a line that minimizes the residuals, that is to say, the difference between our estimation of \\(y\\) (which we will represent using \\(\\hat{y_{i}}\\)) and its observed value for each \\(X\\).\n\\[\\epsilon_{i} = y_{i} - \\hat{y_{i}}\\]\nIn order to do this, we might rely on numeric or analytic calculation methods. Numeric calculation methods will basically try different combinations of \\(\\hat{\\beta_{0}}\\) and \\(\\hat{\\beta_{1}}\\) and choose the best possible solution (in this case, the line that minimizes the residuals). On the other hand, analytic calculation methods will solve the mathematical expressions that describe the problem in order to obtain the solution.\n\nNumeric calculation\nIn order to explain what happens when using numeric methods, we will use an example3 based on the sim1 toy dataset of the modelr package. This is how the data looks like:\n\n\n\n\n\n\n\n\n\nTo adjust a linear regression, we can randomly generate several linear models and see what they look like:\n\n\n\n\n\n\n\n\n\nTo decide which model is the best one, we try to find a line that minimizes the residuals. We could design a function that describes the residuals and find the set of slope and intercept that minimizes our residuals.\nPlotting our best models:\n\n\n\n\n\n\n\n\n\nAnother way to present our models is to build a scatter plot with all possible combinations of \\(\\beta_{0}\\) and \\(\\beta_{1}\\), while coloring dots according to their residual measure:\n\n\n\n\n\n\n\n\n\nThe Newton-Raphson method and gradient descent are more refined numerical optimization techniques used to find the minimum of a function. In the context of linear regression, these methods can be used to iteratively adjust model parameters until the optimal values are reached.\nIn R, we can use the optim() function to perform these optimization methods. We need to provide the initial parameter values (for example \\(\\beta_{0} = 0\\) and \\(\\beta_{1} = 0\\)) and the objective function to be minimized (in this case, our function describing the residuals). The function will iteratively adjust the parameters, providing the optimized parameter estimates for the linear regression model.\n\nbest &lt;- optim(c(0, 0), measure_residuals, data = sim1)\n\nIn this case, the resulting model would be characterized by the parameters \\(\\beta_{0} =\\) 4.2 and \\(\\beta_{1} =\\) 2.1. Graphically:\n\n\n\n\n\n\n\n\n\n\n\nAnalytic calculation\nThe least squares approach is an analytic calculation method that chooses \\(\\hat{\\beta_{0}}\\) and \\(\\hat{\\beta_{1}}\\) to minimize the residual sum of squares4 (RSS), that is to say, the parameters that describe a line that provides the smallest possible value for this measure:\n\\[RSS = \\sum_{i=1}^n \\epsilon^2_{i} = \\sum_{i=1}^n (y_{i} - \\hat{y}_{i})^2\\]\nOnce we minimize the function describing the residual sum of squares, we will obtain the following formulas describing our statistics5:\n\\[\n\\hat{\\beta_{1}} = \\frac{\\sum_i^n (y_i -\\bar{y})(x_i -\\bar{x})}{\\sum_i^n (x_i- \\bar{x})}\n\\]\n\\[\n\\hat{\\beta_{0}} = \\bar{y} - \\hat{\\beta_{1}}\\bar{x}\n\\]"
  },
  {
    "objectID": "materials/5_linear_regression/1_class_notes.html#model-assumptions",
    "href": "materials/5_linear_regression/1_class_notes.html#model-assumptions",
    "title": "Linear Regression",
    "section": "Model assumptions",
    "text": "Model assumptions\nThe simple regression model makes several assumptions regarding the variables involved. It is therefore fundamental to assess these assumptions before drawing conclusions from a regression analysis.\nThe first assumption is that of linearity: there is a linear relationship between the outcome and predictor variable. Consequently, the effect on \\(y\\) of a variation in \\(X\\) is the same regardless \\(X\\)’s value.\nThe model also assumes that errors are normally distributed with a mean equal to 0 and a constant variance6 (across all levels of the independent variables). On the other hand, it is assumed that errors are independent of each other, and that their distribution is independent of the predictor variables."
  },
  {
    "objectID": "materials/5_linear_regression/1_class_notes.html#formula.-coefficient-interpretation",
    "href": "materials/5_linear_regression/1_class_notes.html#formula.-coefficient-interpretation",
    "title": "Linear Regression",
    "section": "Formula. Coefficient interpretation",
    "text": "Formula. Coefficient interpretation\nWhen building a multiple linear regression model, we will estimate a series of parameters describing the linear relationship between a set of \\(n\\) predictor variables, \\(X_{1}\\), \\(X_{2}\\),…, \\(X_{n}\\) and \\(y\\), which can be modeled following the formula:\n\\[ y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{n}X_{n} + \\epsilon\\]\n\n\\(\\beta_{0}\\) represents the average value of \\(y\\) when \\(X_{1} = X_{2} = ... = X_{n} = 0\\).\n\\(\\beta_{i}\\) indicates the average increases in \\(y\\) when \\(X_{i}\\) increases by one unit holding all other predictors (\\(X_{1}\\), \\(X_{2}\\),…, \\(X_{i-1}\\),\\(X_{i+1}\\),…, \\(X_{n}\\)) fixed.\n\\(\\epsilon\\) represents the error or residual."
  },
  {
    "objectID": "materials/5_linear_regression/1_class_notes.html#model-assumptions-1",
    "href": "materials/5_linear_regression/1_class_notes.html#model-assumptions-1",
    "title": "Linear Regression",
    "section": "Model assumptions",
    "text": "Model assumptions\nBesides the model assumptions discussed for the simple linear regression, multiple linear regression models must assume the absence of multicollinearity. Multicollinearity refers to the presence of high correlation or linear dependence among the predictors in a regression model.\nMulticollinearity poses a problem because when predictors are correlated, it becomes difficult to distinguish and isolate the individual effects of each predictor on the outcome variable. As a result, the coefficients of the model become less precise and can lead us to draw unreliable conclusions."
  },
  {
    "objectID": "materials/5_linear_regression/1_class_notes.html#categorical-predictors",
    "href": "materials/5_linear_regression/1_class_notes.html#categorical-predictors",
    "title": "Linear Regression",
    "section": "Categorical predictors",
    "text": "Categorical predictors\nCategorical predictors are built transforming qualitative variables. That is to say, variables that can take one of a limited number of possible values, assigning each observation to a class or category on the basis of some qualitative property. In order to incorporate qualitative variables into regression analysis, we must first transform them into numerical values.\nLet’s start with the simplest case: binary qualitative variables (also called dummy variables) can only take one value of two categories (e.g., having received or not certain medical treatment). In this case, we will assign a value of 1 to indicate the presence of the attribute represented by one of the variable’s possible values (for example: having received the medical treatment), and 0 to represent the absence of the attribute (for example: not having received the medical treatment). The group identified with a 0 is referred to as the base or reference category (in this case, the patients that did not received the treatment).\n\nNote that we do not create a dummy variable for each category of our qualitative variable, since that would leave us with redundant information in the model. For example, if we create a dummy variable for “holds a higher degree” (with value 1 to indicate TRUE and 0 for FALSE), and another one for “does not hold a higher degree” (with value 1 to indicate TRUE and 0 for FALSE), they are evidently redundant and each can be predicted from the other one. This relates to the multicollinearity problem mentioned above, and it is called the dummy variable trap.\n\nLet’s consider an example in order to understand how a binary variable contributes to the model. Suppose we have a regression equation given by:\n\\[ Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\epsilon\\]\nWhere \\(X_1\\) is quantitative (e.g., income) and \\(X_2\\) is qualitative (e.g., having or not having a higher education degree).\nLet’s see what happens for each possible value of \\(X_2\\):\n\nIf \\(X_{2}\\) = 1:\n\n\\[ Y \\approx \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}\\] \\[ Y \\approx (\\beta_{0} + \\beta_{2}) + \\beta_{1}X_{1}\\]\n\nIf \\(X_{2}\\) = 0:\n\n\\[ Y \\approx \\beta_{0} + \\beta_{1}X_{1}\\]\nSo, what is the effect of the inclusion of a qualitative variable in the model? It leads to a shift in the intercept. This can be visualized through a graphical representation9:\n\n\n\n\n\n\n\n\n\nWhen incorporating a qualitative variable, the coefficient \\(\\beta_0\\) can be interpreted as the intercept for cases where the qualitative variable has a value of 0 (in our example, people who do not have a higher degree), while the coefficient \\(\\beta_2\\) represents the shift in the intercept for cases where the qualitative variable has a value of 1 (in our example, people holding a higher degree).\n\nWhat approach should be taken when dealing with a qualitative variable that has more than two categories?\nFor a qualitative variable with \\(n\\) categories, we will need to create \\(n - 1\\) dummy variables that take on values of 0 or 1. The reference category will be the category for which \\(\\beta_{1} = \\beta_{2} = ... = \\beta_{n-1} = 0\\) 10.\nConsider for example a non-ordinal variable on scientific domain, that can take one of three values: Natural sciences, Social sciences and humanities, and Health sciences. In order to transform this variable, we would need to create \\(2\\) (\\(= 3 - 1\\)) dummy variables, \\(X_{1}\\) and \\(X_{2}\\).\n\n\\(X_{1}\\) identifies individuals who are part of the Social sciences and humanities community. If \\(X_{1} = 1\\), the person is part of that community; if \\(X_{1} = 0\\), they are not. The intercept for people from Social sciences and humanities is \\(\\beta_{0} + \\beta_{1}\\).\n\\(X_{2}\\) identifies individuals who are part of the Health sciences community. If \\(X_{2} = 1\\), the person is part of that community; if \\(X_{2} = 0\\), they are not. The intercept for people from Health sciences is \\(\\beta_{0} + \\beta_{2}\\).\nThe remaining category (Natural sciences) serves as the reference category, and its intercept corresponds to \\(\\beta_{0}\\) since observations in this group have \\(X_{1} = 0\\) and \\(X_{2} = 0\\)."
  },
  {
    "objectID": "materials/5_linear_regression/1_class_notes.html#footnotes",
    "href": "materials/5_linear_regression/1_class_notes.html#footnotes",
    "title": "Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that \\(\\beta_2\\) could take negative values. In that case, we would observe a downward shift in the intercept.↩︎\nThis transformation is known as one-hot encoding (kuhnTidyModelingFramework2022a?).↩︎\nThis section is based on this book and this tutorial↩︎\nWhy do we square the residuals? The main reason is to eliminate the sign of the differences. By squaring the residuals, we ensure that all values are positive and prevent positive and negative differences from canceling each other out. This allows us to focus on the magnitude of the differences rather than their direction when fitting the model.↩︎\nRecall a statistic is a function of the values of the variables to be gathered from the sample.↩︎\nThe assumption of constant variance is usually named homoscedasticity.↩︎\nRemember hypothesis tests are decision rules that use information from the sample to evaluate whether a property exists in the population.↩︎\nRecall the null hypothesis typically describes the current state of knowledge.↩︎\nNote that \\(\\beta_2\\) could take negative values. In that case, we would observe a downward shift in the intercept.↩︎\nThis transformation is known as one-hot encoding (kuhnTidyModelingFramework2022a?).↩︎\nA famous example is the correlation between the number of people who drowned by falling into a pool and the films Nicolas Cage appeared in for a given period of time. Some more examples.↩︎\nRecall an outlier is an observation that appears extreme relative to the rest of the data.↩︎\nFor example, imagine a linear model describing the relationship between height (\\(y\\)) and age (\\(X\\)) in humans. If we build this model using data corresponding to humans between the ages of 5 and 20, would it be correct to assume the same relationship between the variables holds for a 40 year old human?↩︎"
  },
  {
    "objectID": "materials/8_tidyverse/1_class_notes.html",
    "href": "materials/8_tidyverse/1_class_notes.html",
    "title": "Tidyverse",
    "section": "",
    "text": "One of the main advantages that R has as a software for statistic analysis is its incremental syntax. This means that the things you can do in R are constantly updated and expanded through the creation of new packages, developed by researchers, users or private companies.\n\n\n\nSource: Gergely Daróczi\n\n\nThese packages contain code, data, and documentation in a standardized collection that can be installed by users of R. Most of the time, we will install them in order to use functions that will do certain tasks that help us work with our data. So far, we were using functions contained in R base: such as mean(), median(), quantile(), etc. But as we dive deep into the data science life cycle, we might address certain challenges that require more complex, or specific, functions. We will need to import the data, tidy the data into a format that is easy to work with, explore the data, generate visualizations, carry out the analysis and communicate the insights. The tidyverse ecosystem provides a powerful tool for streamlining the workflow in a coherent manner that can be easily connected with other data science tools."
  },
  {
    "objectID": "materials/8_tidyverse/1_class_notes.html#packages",
    "href": "materials/8_tidyverse/1_class_notes.html#packages",
    "title": "Tidyverse",
    "section": "",
    "text": "One of the main advantages that R has as a software for statistic analysis is its incremental syntax. This means that the things you can do in R are constantly updated and expanded through the creation of new packages, developed by researchers, users or private companies.\n\n\n\nSource: Gergely Daróczi\n\n\nThese packages contain code, data, and documentation in a standardized collection that can be installed by users of R. Most of the time, we will install them in order to use functions that will do certain tasks that help us work with our data. So far, we were using functions contained in R base: such as mean(), median(), quantile(), etc. But as we dive deep into the data science life cycle, we might address certain challenges that require more complex, or specific, functions. We will need to import the data, tidy the data into a format that is easy to work with, explore the data, generate visualizations, carry out the analysis and communicate the insights. The tidyverse ecosystem provides a powerful tool for streamlining the workflow in a coherent manner that can be easily connected with other data science tools."
  },
  {
    "objectID": "materials/8_tidyverse/1_class_notes.html#tidy-data",
    "href": "materials/8_tidyverse/1_class_notes.html#tidy-data",
    "title": "Tidyverse",
    "section": "Tidy data",
    "text": "Tidy data\nAll packages share an underlying design philosophy, grammar, and data structures. These structures refer to the format of tidy datasets. But, what is tidy data? It is a way to describe data that’s organized with a particular structure: a rectangular structure, where each variable has its own column and each observation has its own row (Wickham 2014a; Peng n.d.).\n\n\n\nJulie Lowndes and Allison Horst\n\n\nAccording to (Wickham 2014b),\n\n“Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.”\n\nWorking with tidy data means that we work with information that has a consistent data structure. The main benefit behind this is that we will have to spend less time thinking how to process and clean data, because we can use existing tools instead of starting from scratch each time we work with a new dataset. In other words, we only require a small set of tools to be learned, since we can reuse them from one project to the other.\n\n\n\nJulie Lowndes and Allison Horst"
  },
  {
    "objectID": "materials/8_tidyverse/1_class_notes.html#tidyverse-ecosystem",
    "href": "materials/8_tidyverse/1_class_notes.html#tidyverse-ecosystem",
    "title": "Tidyverse",
    "section": "tidyverse ecosystem",
    "text": "tidyverse ecosystem\nWe can think of the tidyverse ecosystem (Grolemund n.d.) as the set of tools we can reuse in our tidy data. It is an ecosystem because consists in a set of various packages that can be installed in one line of code (install.packages(tidyverse)), and each package fits into a part of the data science life cycle. This is the main reason why we prefer to use tidyverse instead of R base. It provides us with a consistent set of tools we can use for many different datasets, it is designed to keep our data tidy, and it contains all the specific tools we might need in our data science workflow.\n\n\n\nTidyverse\n\n\nThere is a set of core tidyverse packages that are installed with the main ecosystem, which are ones you are likely to use in everyday data analyses.\n\ntibble: it is a package that re-imagines the familiar R data.frame. It is a way to store information in columns and rows, but does so in a way that addresses problems earlier in the pipeline. That is to say, it stores it in the tidy data format. The official documentation calls tibbles ‘lazy and slurly’, since they do less (they don’t change variable names or types, and don’t do partial matching) and complain more (e.g. when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code.\nreadr: this is a package we will use every time we start a new project. It helps read rectangular data into R, and it includes data in .csv and .tsv format. It is designed to flexibly parse many types of data found.\n\nto read data in .xlsx or .xlx format, you should install the tidyverse-adjacent package readxl.\n\ndplyr: designed for data wrangling. It is built around five primary verbs (mutate, select, filter, summarize, and arrange) that help make the data wrangling process simpler.\ntidyr: it is quite similar to dplyr, but its main goal is to provide a set of functions to help us convert dataframes into tidy data.\npurr: enhances R’s functional programming toolkit by providing a complete and consistent set of tools for working with functions and vectors. It makes easier for us to work with loops inside a dataframe.\nstringr: it is designed to help us work with data in string format.\nforcats: it provides functions to help us work with data in the factor format.\nggplot: the main package for data visualization in the R community. It is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.\n\nWhile this is the main structure of the tidyverse ecosystem, there are multiple adjacent packages that we can install which fit into the tidy syntax and work well with the tidy data format. Some of them are readxl to read data in .xlsx or .xl format, janitor for cleaning data, patchwork to paste ggplot graphs together, tidymodels for machine learning… and many more."
  },
  {
    "objectID": "materials/8_tidyverse/1_class_notes.html#tidy-data-functions",
    "href": "materials/8_tidyverse/1_class_notes.html#tidy-data-functions",
    "title": "Tidyverse",
    "section": "Tidy data functions",
    "text": "Tidy data functions\nAs you had a glimpse in the previous classes, there are some basic functions that we use to tidy data:\n\nmutate: to transform columns\nselect: to select certain columns\nfilter: to select certain rows (we can think of filter as the row-wise version of select, and select as the column-wise version of filter)\narrange: to reorder values of rows\n\nNow, we will go over the basics of more complex data transformation functions to reshape the data: joins, pivots, summarizes and date processing.\n\nMerges of dataframes\nIf you ever worked with SQL or multiple databases in the past, you might already be familiar with the concept of joining data. A tidy dataset should contain one type of observational unit per table. For example, if we have done a survey regarding labor conditions to individuals, but we also have sociodemographical information regarding their household, we should have each information in a different dataset. However, we will probably be interested in crossing the information regarding the individuals and their household conditions. So we need to have key IDs to be able to combine these two datasets: for example, in this case, the ID for the household.\n\nNote that the household ID is not the same as the person ID. Each person has a unique identifier, but so does each household. The information is consistent: when we have two individuals who share a household ID, we see in the household information that there are two people living there.\nNow, how do dataframe joins work?\n\n\n\nNiloy Biswas\n\n\n\nInner join: this creates a new dataset that only contains the information of rows where the IDs match. For example, in our case, the data of the household 5 wouldn’t appear since it doesn’t join any individual data.\n\n\n\n# A tibble: 7 × 7\n  person_ID household_ID gender   age income neighborhood_income\n      &lt;int&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;              \n1         1            1 f         31   1039 low income         \n2         2            1 f         30   1677 low income         \n3         3            2 m         45   4971 low income         \n4         4            3 m         42   3274 high income        \n5         5            4 m         32   3937 high income        \n6         6            6 f         42   2031 high income        \n7         7            7 f         46   3463 high income        \n# ℹ 1 more variable: n_people_household &lt;dbl&gt;\n\n\n\nLeft join: it keeps all the rows of the data on the “left” and adds the columns that match the dataframe on the right. The decision of which dataframe goes where (left or right) is arbitrary and up to us, but we must keep in mind that it will be our main dataframe in the join. In our example, if we chose the individual dataset as the left, we would have the same table as the result of the inner join. But if we chose the household dataset, we would have a dataset with empty values for the ID’s that don’t match.\n\n\n\n# A tibble: 8 × 7\n  household_ID neighborhood_income n_people_household person_ID gender   age\n         &lt;dbl&gt; &lt;chr&gt;                            &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n1            1 low income                           2         1 f         31\n2            1 low income                           2         2 f         30\n3            2 low income                           1         3 m         45\n4            3 high income                          1         4 m         42\n5            4 high income                          1         5 m         32\n6            5 low income                           5        NA &lt;NA&gt;      NA\n7            6 high income                          4         6 f         42\n8            7 high income                          3         7 f         46\n# ℹ 1 more variable: income &lt;dbl&gt;\n\n\n\nFull join: it keeps all the columns and all the rows in both dataframes.\n\n\n\n# A tibble: 8 × 7\n  household_ID neighborhood_income n_people_household person_ID gender   age\n         &lt;dbl&gt; &lt;chr&gt;                            &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n1            1 low income                           2         1 f         31\n2            1 low income                           2         2 f         30\n3            2 low income                           1         3 m         45\n4            3 high income                          1         4 m         42\n5            4 high income                          1         5 m         32\n6            5 low income                           5        NA &lt;NA&gt;      NA\n7            6 high income                          4         6 f         42\n8            7 high income                          3         7 f         46\n# ℹ 1 more variable: income &lt;dbl&gt;\n\n\n\n\nSummarizing information\nMany times, we will want to summarize the information in our dataset. We can catch a glimpse of the summarized dataset with functions such as summary() or str(). However, more often, we will want to see specific information regarding groups in our dataset. For example, how many households we have in the different types of neighborhoods. To do this, it is necessary to group our data. We will not look into the total number of households, but we will group the data by the neighborhood of the households. Then, we can count each group.\n\n\n# A tibble: 2 × 2\n  neighborhood_income     n\n  &lt;chr&gt;               &lt;int&gt;\n1 high income             4\n2 low income              3\n\n\nOther times, we might be interested in getting descriptive statistics per group. For example, the median age for men and women in the individuals dataset.\n\n\n# A tibble: 2 × 2\n  gender median_age\n  &lt;chr&gt;       &lt;dbl&gt;\n1 f            36.5\n2 m            42  \n\n\nOr even combine groups and subgroups. For example, the median income for women and men who live in low income or high income neighborhoods\n\n\n# A tibble: 4 × 3\n# Groups:   neighborhood_income [2]\n  neighborhood_income gender income\n  &lt;chr&gt;               &lt;chr&gt;   &lt;dbl&gt;\n1 high income         f       2747 \n2 high income         m       3606.\n3 low income          f       1358 \n4 low income          m       4971 \n\n\n\n\nReshaping data\nConverting your data from wide-to-long or from long-to-wide data formats is referred to as reshaping your data.\nFor example, take this subset of columns from our individuals dataset.\n\n\n# A tibble: 7 × 4\n  person_ID gender   age income\n      &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1         1 f         31   1039\n2         2 f         30   1677\n3         3 m         45   4971\n4         4 m         42   3274\n5         5 m         32   3937\n6         6 f         42   2031\n7         7 f         46   3463\n\n\nThis is what we call a wide format: each variable has its own column, and each row represents a single observation. Long data, on the other hand, refers to a dataset where each variable is contained in its own column. This format is often used when working with large datasets or when performing statistical analyses that require data to be presented in a more condensed format.\nThis is the same dataset we had previously but reshaped as long data.\n\n\n# A tibble: 14 × 4\n   person_ID gender variable value\n       &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n 1         1 f      age         31\n 2         1 f      income    1039\n 3         2 f      age         30\n 4         2 f      income    1677\n 5         3 m      age         45\n 6         3 m      income    4971\n 7         4 m      age         42\n 8         4 m      income    3274\n 9         5 m      age         32\n10         5 m      income    3937\n11         6 f      age         42\n12         6 f      income    2031\n13         7 f      age         46\n14         7 f      income    3463\n\n\nAs you can see, the wide data format has a separate column for each variable, whereas the long data format has a single “variable” column that indicates whether the value refers to age or income."
  },
  {
    "objectID": "materials/8_tidyverse/1_class_notes.html#dates-in-r",
    "href": "materials/8_tidyverse/1_class_notes.html#dates-in-r",
    "title": "Tidyverse",
    "section": "Dates in R",
    "text": "Dates in R\nDates are a special type of object in R. There are three types of data that refer to an instant in time:\n\nA date. Tibbles will print it as date.\nA time within a day. Tibbles will print it as time.\nA date-time, which refer to dates plus times, specifying an exact moment in time. Tibbles print this as &lt;dttm&gt;. In R base, these are called POSIXct.\n\nIt is a good practice to keep our data simple, so if we don’t necessarily need the time information to work in our project, we will usually prefer to work with date formats.\nWhen working with dates in R, we should also be careful with time zones. Time zones are an enormously complicated topic because of their interaction with geopolitical entities. R uses the international standard IANA time zones. You can find out what R thinks your time zone is using the following command:\n\nSys.timezone()\n\n[1] \"America/Toronto\"\n\n\nlubridate uses UTC (Coordinated Universal Time), which is the standardized time zone used by the scientific community."
  },
  {
    "objectID": "materials/4_descriptive_stats/1_class_notes.html",
    "href": "materials/4_descriptive_stats/1_class_notes.html",
    "title": "Descriptive statistics",
    "section": "",
    "text": "In this class, we will dive into the basics of statistics and probability. We will first explore the most frequently employed indicators for describing one or more variables and understanding the relationships between them. Secondly, we will delve into a series of techniques for making population inferences based on a sample.\nA solid understanding of the fundamental concepts of statistics and probability is essential to comprehend R’s operations, as well as to make informed choices when selecting the most suitable indicators to explore and interpret the information contained in a data set."
  },
  {
    "objectID": "materials/4_descriptive_stats/1_class_notes.html#conditional-probability-and-independence",
    "href": "materials/4_descriptive_stats/1_class_notes.html#conditional-probability-and-independence",
    "title": "Descriptive statistics",
    "section": "Conditional probability and independence",
    "text": "Conditional probability and independence\nConditional probability is a measure of the probability of an event occurring given the occurrence of another event. We will represent the probability of event A happening given the event B using \\(P(A/B)\\)2.\nTwo events A and B are said to be independent if there is no relationship between them, that is to say that the probability of event A occurring does not depend on the occurrence of event B: \\(P(A/B) = P(A)\\)."
  },
  {
    "objectID": "materials/4_descriptive_stats/1_class_notes.html#sample-vs-population",
    "href": "materials/4_descriptive_stats/1_class_notes.html#sample-vs-population",
    "title": "Descriptive statistics",
    "section": "Sample vs population",
    "text": "Sample vs population\nA population is a set or universe of items (objects, households, people, etc.) of interest.\nA sample is a subset of observations drawn from a population3."
  },
  {
    "objectID": "materials/4_descriptive_stats/1_class_notes.html#probability-distributions",
    "href": "materials/4_descriptive_stats/1_class_notes.html#probability-distributions",
    "title": "Descriptive statistics",
    "section": "Probability distributions",
    "text": "Probability distributions\nA variable is any characteristic whose value can vary from one object to another within the population. Events will be defined as a set of outcome values for the variable or variables observed 4.\nA probability distribution is a function describing the probability of each possible outcome within the sample space.\nThere are two types of distributions:\n\nA discrete probability distribution is the probability distribution of a random variable that can take on only a countable number of possible values.5\n\n\n\nA continuous probability distribution is the probability distribution of a random variable that can take on an uncountable number of possible values.6\n\nProbability distributions are characterized by their parameters, which refer to the population’s characteristics.\nFor example, the normal distribution is characterized by its mean and variance (or standard deviation)7.\n\nThe Central Limit Theorem8 states that even when individual variables are not normally distributed, the sums and averages of the variables will approximately follow a normal distribution (Devore 2011).\nOn the other hand, for example, the Chi-Squared distribution is characterized by its degrees of freedom.\n\nNote that, while distributions may be symmetrical (e.g.: the normal distribution), they might also be positively skewed (e.g.: the chi-squared distribution), or negatively skewed when the tail of the distribution extends towards the left side of the curve, indicating a higher frequency of values at the higher end of the distribution."
  },
  {
    "objectID": "materials/4_descriptive_stats/1_class_notes.html#statistical-inference-basics",
    "href": "materials/4_descriptive_stats/1_class_notes.html#statistical-inference-basics",
    "title": "Descriptive statistics",
    "section": "Statistical inference basics",
    "text": "Statistical inference basics\nPoint estimation involves the use of sample data to calculate a single value which is to serve as a “best guess” of an unknown population parameter. We call this value an estimator, where estimators are statistics, that is to say, functions of the values of the variables to be gathered from the sample.\nIt is also possible to estimate an interval within which we believe a population parameter lies. The advantage of this approach is that we can determine the probability that the population parameter truly lies within this interval. This statistical technique is known as confidence intervals.\nFinally, we can calculate the probability that the population parameter is greater than, less than, or equal to a certain value. This is known as hypothesis testing.\nWhen applying these techniques, functions are constructed based on the sample data, which are then compared to known theoretical distributions. We will further develop the key concepts to understand how to apply them in the upcoming sections."
  },
  {
    "objectID": "materials/4_descriptive_stats/1_class_notes.html#measures-of-centrality",
    "href": "materials/4_descriptive_stats/1_class_notes.html#measures-of-centrality",
    "title": "Descriptive statistics",
    "section": "Measures of centrality",
    "text": "Measures of centrality\nMeasures of centrality will provide information regarding the location of a set of numbers, particularly its center. We will rely on them for the analysis of a single variable.\n\nMean\nThe arithmetic mean (or average) is obtained by adding the observation values and dividing the sum by the total number of values9.\n\n\n\n\n\n\n\n\n\nThe weighted mean follows a similar methodology, but each observation does not contribute equally to the final average. The contribution of each observation to the mean is determined by it’s weight10. This measure becomes particularly useful when each observation in our sample corresponds to varying proportions of the population (which is usually the case when working with household surveys).\nThe trimmed mean is a calculation of the arithmetic mean that removes a certain percentage of extreme values from each end of the distribution, reducing the influence of outliers11 in the result.\n\n\nMode\nThe mode is the most frequently occurring value of the distribution.\n\n\n\n\n\n\n\n\n\n\n\nMedian\nThe median is the middle value in an ordered list of values. It separates the distribution in such a way that 50% of the values are on its left and 50% are on its right.\n\n\n\n\n\n\n\n\n\nNote that the mean, mode and the median may or may not coincide in a distribution.\n\n\nQuantiles\nAs we mentioned, the median represents the value that divides the data leaving 50% on one side and 50% on the other. This concept can be generalized to any X% using quantiles. The Xth quantile is the value that divides the distribution, with X% of the data to its left and 1-X% to its right.\nSome of the most commonly used quantiles are the 25th percentile, also known as \\(Q_{1}\\) (the first quartile), \\(Q_{2}\\) (the median), and \\(Q_{3}\\) (the third quartile). \\(Q_{1}\\) separates the lower 25% of the data, \\(Q_{2}\\) represents the median, and \\(Q_{3}\\) separates the lower 75% of the data, leaving the upper 25% to the right."
  },
  {
    "objectID": "materials/4_descriptive_stats/1_class_notes.html#measures-of-dispersion",
    "href": "materials/4_descriptive_stats/1_class_notes.html#measures-of-dispersion",
    "title": "Descriptive statistics",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\nMeasures of dispersion will help us interpret the variability of data or, in other words, the spread of a variable’s distribution. We will also rely on them for the analysis of a single variable.\n\nVariance\nVariance quantifies the dispersion or spread of a set of data points providing information about how far each value in the dataset deviates from the mean (average) of the data. Specifically, it measures the average squared12 difference between each data point and the mean13.\n\n\nStandard deviation\nThe standard deviation is the square root of the variance and provides similar information in a more easily interpretable unit. By computing the square root of the variance, the value is expressed in the same units as the original set of values14."
  },
  {
    "objectID": "materials/4_descriptive_stats/1_class_notes.html#relationships-between-variables",
    "href": "materials/4_descriptive_stats/1_class_notes.html#relationships-between-variables",
    "title": "Descriptive statistics",
    "section": "Relationships between variables",
    "text": "Relationships between variables\n\nLinear and non-linear relationships\nWhen analyzing the relationship between two quantitative variables, linear relationships between two \\(X\\) and \\(Y\\) variables can be described by a linear equation such as:\n\\[ Y = b + mX\\]\nThat is to say, for any value of \\(X\\) and \\(Y\\) an increase (decrease) in \\(X\\) will imply a proportional increase (decrease) in \\(Y\\).\n\n\nCorrelation\nCorrelation quantifies the strength and direction of the linear relationship between two variables15. It can take values between -1 (perfect negative relationship) and 1 (perfect positive relationship), with 0 indicating no linear relationship. The correlation value has no units and will not be affected by a linear transformation in the variables (Çetinkaya-Rundel and Hardin 2021). It is important to remember that correlation does not imply causation.\nTwo of the main correlation calculation methods are Pearson’s and Spearman’s correlation.\n\nPearson’s correlation is useful to evaluate linear relationships between variables (and will yield 0 if no linear relationship can be found), but is very sensitive to outliers.\nSpearman’s correlation is not as sensible to outliers and can identify monotonic (non-linear) relationships between variables, that is to say, scenarios where when the value of one variable increases, the value of the other variable either consistently increases or consistently decreases, even if these transformations are not proportional.\n\nThe choice of our method will therefore depend on whether the relationship between variables is linear or monotonic, and whether our data includes outliers or not."
  },
  {
    "objectID": "materials/4_descriptive_stats/1_class_notes.html#footnotes",
    "href": "materials/4_descriptive_stats/1_class_notes.html#footnotes",
    "title": "Descriptive statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn example tossing a coin.↩︎\nBayes’ theorem provides us with a method to reverse or convert a conditional probability (That is to say, to obtain \\(P(A/B)\\) knowing \\(P(B/A)\\)): \\[ P(A/B) = \\frac{P(B/A)P(A)}{P(B)} \\]↩︎\nConventionally, uppercase (N) is often used to represent the population, while lowercase (n) is used to denote samples.↩︎\nFor instance, consider a population in which each object represents an ice cream cone and we are observing the variable “taste”, with the following possible outcome values: chocolate, strawberry, and vanilla.↩︎\nConsider, for example, our ice cream “taste” variable: we have three possible values.↩︎\nFor example, a person’s height. Since this is a continuous set of numbers, the possible values it can take are uncountable, as there are always intermediate values between any two numbers. For example, between 170 and 171, you have 170.5, between 170 and 170.5, there’s 170.25, and so on.↩︎\nWe will elaborate on the information contained within these parameters in the following sections.↩︎\nTry it yourselves↩︎\nFor a series of values \\(x_{1}, x_{2}, x_{3},...,x_{n}\\) we can obtain the mean (\\(\\overline{x}\\)) using: \\[ \\overline{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} \\]↩︎\nFor a series of values \\(x_{1}, x_{2}, x_{3},...,x_{n}\\) and weights \\(w_{1}, w_{2}, w_{3},...,w_{n}\\) we can obtain the weighted mean using: \\[  \\frac{\\sum_{i=1}^{n} w_{i}x_{i}}{\\sum_{i=1}^{n} w_{i}}  \\]↩︎\nAn outlier is an observation that appears extreme relative to the rest of the data. Outliers might result from data collection or data entry errors, or might provide interesting unexpected information about the data (Çetinkaya-Rundel and Hardin 2021). Therefore, removing outliers from our calculations might or might not constitute a good practice in each particular context.↩︎\nMake sure you understood the fundamental reasoning behind this concept: what’s the rationale for employing the squared difference?  (Here’s a hint: consider positive and negative differences may cancel each other out).↩︎\nFor a series of values \\(x_{1}, x_{2}, x_{3},...,x_{n}\\) we can obtain the variance using: \\[ \\frac{1}{n} \\sum_{i=1}^{n} (x_{i} - \\overline{x})^2 \\]↩︎\nWe can obtain it using: \\[ \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_{i} - \\overline{x})^2} \\]↩︎\nVisit this site for an intuitive visualization on correlation.↩︎\nVisit this site for a visualization on the interpretation of confidence intervals↩︎\nHow small should our p-value be? Conventionally, a p-value smaller than 0.05 is considered statistically significant, suggesting strong evidence against the null hypothesis. However, the benchmark value for the significance level may vary depending on the specific problem and the preferences of the researcher or decision-maker.↩︎\nA common approach to address this distortion is to conduct our tests on random subsamples of our data.↩︎"
  },
  {
    "objectID": "materials/7_shiny_apps/1_class_notes.html#user-interface",
    "href": "materials/7_shiny_apps/1_class_notes.html#user-interface",
    "title": "Shiny Apps",
    "section": "User Interface",
    "text": "User Interface\nThe user interface will include the inputs, outputs and layout functions that will define the general appearance of the app12. For example:\n\nui &lt;- fluidPage(\n  titlePanel(\"The app's title\"),\n  sidebarLayout(\n    sidebarPanel(\n      numericInput(inputId = \"my_first_input\",\n                   label = \"A clear description for the user to see\"\n                   #,... other parameters\n                   ),\n      textInput(inputId = \"my_second_input\",\n                label = \"A clear description for the user to see\"\n                #,... other parameters\n                ),\n      actionButton(inputId = \"go\",\n                   label = \"Update!\"\n                   #,... other parameters\n                   )\n      ),\n    mainPanel(\n      plotOutput(outputId = \"my_first_output\")\n    )\n  )\n)\n\n\nLayout\nLayout functions can be combined or nested in order to design our desired layout. In our example, the fluidPage() function defines the general layout of the site: a fluid page layout consists of rows which in turn include columns13. However, note that after defining our titlePanel(), nested inside our fluid page you can find another layout function: sidebarLayout(). This is a predefined layout featuring a sidebar14 (typically used for inputs) and a main panel15 (where you will usually find the outputs).\n\n\n\nStructure of a basic app with sidebar (Wickham 2021)\n\n\nUsing the shinythemes package16, we can also change the theme of the app using theme = shinytheme(\"the_theme_we_want\")17 inside our general layout function18.\nConsidering a Shiny app’s ui is an HTML document, it is possible to include tags such as h1(\"Header 1\") for a level 1 header, br() to insert a line break or a(href=\"\", \"link\") to define a hyperlink19.\n\n\nInputs and outputs\nMultiple input functions exist, including numericInput(), textInput(), selectInput(), and checkboxInput(), among others: each one corresponds to a different type of input to be collected from the user (a numeric value, a string, etc.). All input functions share two core parameters: inputId for specifying the unique identifier and label to inform the user the expected input content. Additionally, each input type may have further parameters tailored to its specific characteristics, such as potential values (a vector of options for the input selection, maximum or minimum values for numeric inputs, etc.) and default values.\nSimilarly, output functions such as plotOutput(), dataTableOutput(), imageOutput() or textOutput() will specify the type of output. In this case, the main parameter will be outputId, used to specify the unique identifier.\n\nWhen designing visualizations in a Shiny app, it is usually a good idea to create interactive visualizations using the plotly package."
  },
  {
    "objectID": "materials/7_shiny_apps/1_class_notes.html#server",
    "href": "materials/7_shiny_apps/1_class_notes.html#server",
    "title": "Shiny Apps",
    "section": "Server",
    "text": "Server\nThe server object will be defined as a function with two main arguments20:\n\ninput: It is a list of elements that we receive from the ui. In this case it contains my_first_input, my_second_input and go (the inputIds).\noutput: It is a list that we generate inside the server. In this case we only define the element my_first_output.\n\nFor example:\n\nserver &lt;- function(input, output) {\n  \n  output$my_first_output &lt;- renderPlot({\n    hist(c(1:input$my_first_input), main = input$my_second_input)\n  })\n}\n\nIn order to create our outputs, we will resort to render functions within the server. These functions are responsible for dynamically generating the content of each output element based on the inputs (in order to do so, we will refer to each input using the syntax input$input_id). Note that there must be a correspondence between the type of output defined in the ui and the render function.21\nIn our example, renderPlot() is a reactive function, which is triggered every time the input changes and re-generates the output (in our example, a plot)22. In other words, everything that is wrapped between the braces is going to be run each time the input changes.\nWe may not want everything to re-run and update every single time any input changes but only in response to specific events (for example, update only if the user clicks on our “Update!” actionButton)23. In this case, we may resort to functions such as eventReactive(), which will create the reactive expression with code in its second argument (that is to say, the expression between curly brackets) when reactive values in the first argument change (in this case, input$go)24. For example:\n\nserver &lt;- function(input, output) {\n     \n    plot &lt;- eventReactive(input$go,\n                          {hist(c(1:input$my_first_input), \n                                main = input$my_second_input)\n                            })\n    \n    output$my_first_output &lt;- renderPlot({\n      plot()\n      })\n  \n}\n\nNotice that plot is now a reactive expression, and we therefore have to call it like a function (that is to say, plot()) inside renderPlot(). However, while it looks like we are calling a function, a reactive expression has an important difference: it only runs the first time it is called and then it stores its result until it needs to be updated (Wickham 2021)."
  },
  {
    "objectID": "materials/7_shiny_apps/1_class_notes.html#data-preprocessing",
    "href": "materials/7_shiny_apps/1_class_notes.html#data-preprocessing",
    "title": "Shiny Apps",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nUsually, our app won’t solely depend on user inputs but will also make use of additional databases25 that we load ourselves (the developers). To prevent excessive processing times (and ensure smooth app performance), we can preprocess much of this data externally and then simply load the prepared databases for the app server to operate on.\nThus, in our working directory we will typically have a file named prepare_data.R where we will handle all of these preprocessing tasks."
  },
  {
    "objectID": "materials/7_shiny_apps/1_class_notes.html#taking-our-apps-to-the-next-level",
    "href": "materials/7_shiny_apps/1_class_notes.html#taking-our-apps-to-the-next-level",
    "title": "Shiny Apps",
    "section": "Taking our apps to the next level",
    "text": "Taking our apps to the next level\nAs you aim to create more complex apps, you’ll discover that this basic script design can become cumbersome. It may result in excessively lengthy and intricate scripts, riddled with multiple nested functions (and long lists of ids26), which can increase the likelihood of errors when attempting to make modifications to the app. The solution: modules27.\nA module is basically a pair of ui and server functions, but these functions are constructed in a special way that creates a “namespace” (“spaces” of “names” that are isolated from the rest of the app). Namespacing makes it easier to understand how your app works because you can write, analyse, and test individual components in isolation (Wickham 2021).\nIn order to get your app to work, in your main app.R script you will summon all your module’s ui and server functions:\n\nlibrary(shiny)\n\n  ui &lt;- fluidPage(\n    module1_ui(\"id1\"),\n    module2_ui(\"id2\")\n    #,.....\n  )\n  \n  server &lt;- function(input, output, session) {\n    module1_server(\"id1\")\n    module2_server(\"id2\")\n    #.....\n  }\n  \n  shinyApp(ui, server)  \n\nNote that you should use the same id in both the module’s ui and server, otherwise the two pieces will not be connected. However, the ids you use to identify inputs and outputs inside each module don’t need to be globally unique, they just have to be unique inside each module.\n\nRecap: Directory structure when working with modules\napp.R\nprepare_data.R\nR /\n   module1.R\n   module2.R\n    …."
  },
  {
    "objectID": "materials/7_shiny_apps/1_class_notes.html#sharing-your-app",
    "href": "materials/7_shiny_apps/1_class_notes.html#sharing-your-app",
    "title": "Shiny Apps",
    "section": "Sharing your app",
    "text": "Sharing your app\nHow can we share our app? There are two main options, and the choice between them will depend on the specific circumstances:\n\nUsing a shiny server28\nShiny Server is an open-source backend program that enables you to host your apps within a controlled environment, such as within your organization. It will host each app at its own web address and automatically start the app when a user visits the address (when the user leaves, Shiny Server will automatically stop the app).\n\n\n\nPublishing your app to https://www.shinyapps.io/\nThis option is easier, it only requires setting up an account in the site and deploying your app. However, the free option comes with certain limitations, such as restrictions on the number of apps that can be hosted and the monthly active hours of the app, which refer to the amount of time when a user is interacting with the app."
  },
  {
    "objectID": "materials/7_shiny_apps/1_class_notes.html#footnotes",
    "href": "materials/7_shiny_apps/1_class_notes.html#footnotes",
    "title": "Shiny Apps",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor example, this app is a supporting material of this article, allowing readers to independently explore the researcher’s findings.↩︎\nSuch as importing and exporting data sets, creating new objects, or applying transformations according to a specific set of instructions.↩︎\nYou can find some examples here.↩︎\nA widget is a web element that allows the user to send a message.↩︎\nBesides being unique, ids must follow the same naming conventions as variables (no spaces, special characters, etc.)↩︎\nConsider, for instance, a simple app where the user is prompted to provide their name and date of birth: we would assign each input an id, for example “name” and “date_birth”. We would later refer to these user inputs in our functions using this ids (for example, in order to display a personalized greeting message by addressing each user by their name or wishing them a happy birthday on the right date).↩︎\nThroughout this section, we’ll be drawing on R’s official documentation on the subject.↩︎\nWe will usually save this script with a name such as app.R↩︎\nNote that you may use any names you like for this objects, you will simply need to make sure to refer to the object in the same way when calling the shinyApp() function. For example: shinyApp(ui = my_original_ui_name, server = my_original_server_name)↩︎\nFor comprehensive information on various Shiny app design options, please refer to the Shiny Cheatsheet.↩︎\nYou can also run shiny::runApp().↩︎\nNote that elements within the ui will be separated by commas!↩︎\nThis is the most commonly used layout (although there are other options to explore!)↩︎\nWe’ll include its contents within the sidebarPanel() function.↩︎\nWe’ll include its contents within the mainPanel() function.↩︎\nThe bslib package also offers a function to change the app’s layout. In this case, the syntaxis is theme = bs_theme(…).↩︎\nYou can explore the available themes here.↩︎\nIn our example, fluidPage(..., theme = shinytheme(\"the_theme_we_want\")).↩︎\nTry running names(tags) to get a complete list of all possible tags.↩︎\nIt can optionally include session as a parameter: an environment that can be used to access information and functionality relating to the session. For more details, visit this site.↩︎\nFor example: a plotOutput will be rendered with a renderPlot() function, an imageOutput will be rendered with a renderImage() function, etc.↩︎\nIn addition to generating output based on the inputs provided, certain render functions, like renderDataTable(), offer additional features, such as the possibility to download the rendered table in multiple formats.↩︎\nSee for example this app’s “Change” button.↩︎\nSimilarly, a frequent use of the observe() function is to update a certain input’s options considering a different input. For instance, consider a restaurant reservation app featuring two inputs: “restaurant” and “available_time_slots”. Since each restaurant has its unique set of available time slots, it’s essential to ensure that the time slot options align with the selected restaurant. Therefore, we will want to update the time slot options every time the user chooses a different restaurant.↩︎\nFor example, the findings of a scientific article we authored, which we intend to share with the wider scientific community.↩︎\nKeep in mind that ids must be unique, and any accidental repetition of an id will cause the app to fail!↩︎\nIn this class, we won’t extensively cover the implementation of modules, but you can refer to this text for more comprehensive instructions.↩︎\nFor more details, visit this site.↩︎"
  },
  {
    "objectID": "contributors.html",
    "href": "contributors.html",
    "title": "Contributors",
    "section": "",
    "text": "The team that developed the materials for this website is composed of:\n\nDiego Kozlowski:\n\nÉcole de bibliothéconomie et des sciences de l’information, Université de Montréal.\nORCID\n\nLaia Domenech Burin:\n\nORCID\n\nCarolina Pradier:\n\nÉcole de bibliothéconomie et des sciences de l’information, Université de Montréal.\nORCID\n\nGermán Rosati:\n\nCONICET / IDAES-UNSAM\nORCID\n\nNatsumi S. Shokida:\n\nÉcole de bibliothéconomie et des sciences de l’information, Université de Montréal.\nORCID\n\n\nWe are grateful for their dedication and contributions to this project.\n\nThis project was founded by\nProfessor Thema Monroe-White\n\nSchar School of Policy and Government\nORCID",
    "crumbs": [
      "Contributors"
    ]
  }
]