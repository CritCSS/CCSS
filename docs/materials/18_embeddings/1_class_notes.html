<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="">

<title>Word embeddings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Word embeddings</li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contributors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributors</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../materials.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Materials</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#bag-of-words-model" id="toc-bag-of-words-model" class="nav-link" data-scroll-target="#bag-of-words-model">Bag of Words model</a></li>
  </ul></li>
  <li><a href="#word-embeddings" id="toc-word-embeddings" class="nav-link" data-scroll-target="#word-embeddings">Word embeddings</a></li>
  <li><a href="#classical-word-embeddings-techniques" id="toc-classical-word-embeddings-techniques" class="nav-link" data-scroll-target="#classical-word-embeddings-techniques">Classical word embeddings techniques</a>
  <ul class="collapse">
  <li><a href="#word2vec" id="toc-word2vec" class="nav-link" data-scroll-target="#word2vec">Word2Vec</a></li>
  <li><a href="#glove" id="toc-glove" class="nav-link" data-scroll-target="#glove">GloVe</a></li>
  <li><a href="#fasttext" id="toc-fasttext" class="nav-link" data-scroll-target="#fasttext">FastText</a></li>
  </ul></li>
  <li><a href="#how-to-access-word-embeddings" id="toc-how-to-access-word-embeddings" class="nav-link" data-scroll-target="#how-to-access-word-embeddings">How to access word embeddings</a></li>
  <li><a href="#word-embeddings-document-embeddings-and-llms" id="toc-word-embeddings-document-embeddings-and-llms" class="nav-link" data-scroll-target="#word-embeddings-document-embeddings-and-llms">Word embeddings, document embeddings and LLM‚Äôs</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Word embeddings</h1>
<p class="subtitle lead">class notes</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Textual information is a type of <em>unstructured data</em>. This means that we must give it some kind of structure to work with it.</p>
<section id="bag-of-words-model" class="level2">
<h2 class="anchored" data-anchor-id="bag-of-words-model">Bag of Words model</h2>
<p>A classic approach to structure textual information is building a Bag of Words (BoW). For this we build a Term-Frequency Matrix (TFM), where:</p>
<ul>
<li>each row is a document from the corpus and each column is a term from the vocabulary</li>
<li>each cell denotes the appearance of a word in a document. There are different types of counting: from a binary (it appears or not), the frequency, to the normalized frequency, and the Term-Frequency Inverse Document Frequency (TF-IDF), among others.</li>
<li>We can now think of each document as a vector over the vocabulary, and vice-versa each term is a vector over the documents of the corpus.</li>
</ul>
<p>This classic approach for text mining allowed to use formal methods to automatically study large corpuses, but it has some limitations:</p>
<ul>
<li>It does not preserve the <em>order</em> of language, the structure and sequential order of words and n-grams are lost</li>
<li>it suffers from <em>high dimensionality</em> (the number of columns is the number of unique words in the vocabulary, a very high number, around ~10,000 for a medium sized corpus), and</li>
<li>since each text will only uses a subset of words, most cells in TFM are zero; this is called <em>sparsity</em></li>
<li>High dimensionality and sparsity have a high computational burden and reduce the quality of results for many automatic techniques.</li>
</ul>
</section>
</section>
<section id="word-embeddings" class="level1">
<h1>Word embeddings</h1>
<p>The central idea of word embeddings can be summarized in John Firth‚Äôs quote:</p>
<blockquote class="blockquote">
<p>‚Äúyou shall know a word by the company it keeps‚Äù.</p>
</blockquote>
<ul>
<li>The hypothesis behind word embeddings is that, in some way, word contexts are strongly correlated with their meaning.</li>
<li>Words that appear ‚Äúclose‚Äù probably have similar meanings.</li>
<li>Word embedding also seek to represent textual data in a vectorial form;</li>
<li>this representation aims to be smaller than that of a TFM, generally between 100 and 300 dimensions</li>
</ul>
<p>Word embeddings are learned from a corpus (usually large) of text. Using word contexts, word embeddings models seek to capture the semantics of such words.</p>
<p>We could represent the problem that word embedding models seek to solve following <span class="citation" data-cites="kozlowski_2019">(<a href="#ref-kozlowski_2019" role="doc-biblioref">Kozlowski, Taddy, and Evans 2019</a>)</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/kozlowski_embedding.jpeg" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption><strong>Source:</strong> <span class="citation" data-cites="kozlowski_2019">(<a href="#ref-kozlowski_2019" role="doc-biblioref">Kozlowski, Taddy, and Evans 2019</a>)</span></figcaption>
</figure>
</div>
<p>The main question is:</p>
<ul>
<li><p>how to represent all the words of a corpus in a <span class="math inline">\(k\)</span>-dimensional space that also preserves the distances between the <span class="math inline">\(n\)</span> words across the <span class="math inline">\(m\)</span> contexts?.</p></li>
<li><p>One of the differences between the many techniques for training a word embedding will be how they define those ‚Äúcontexts‚Äù and how they operate with them.</p>
<h2 id="assumptions" class="anchored" data-anchor-id="word-embeddings">Assumptions</h2></li>
<li><p>The traditional BoW worked on the assumption that words were <strong>independent</strong> from each other<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> (they were all equally represented as columns). Since an embedding is trained based on its context, the assumption of independence between words is greatly attenuated.</p></li>
<li><p>The use of context allows for interaction between words, so a lower dimensionality than that in BoW is expected.</p>
<h2 id="geometrical-representation-of-meaning" class="anchored">Geometrical representation of meaning</h2></li>
<li><p>One of great advantages of word embeddings is that it brings a geometrical representation of words. The resulting <span class="math inline">\(k\)</span>-dimensional space in which we codify words is a representation of their meaning.</p></li>
<li><p>Semantically similar words will be close in space and therefore will have a high similarity; conversely, words with very different meanings will be far apart in space and thus will show low values in the similarity metric. In fact, it‚Äôs expected that synonyms will be <em>very</em> close.</p></li>
</ul>
<p>The following figure illustrates this idea:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/chollet.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption><strong>Source:</strong> <span class="citation" data-cites="chollet_2017">(<a href="#ref-chollet_2017" role="doc-biblioref">Chollet and Allaire 2017</a>)</span></figcaption>
</figure>
</div>
<ul>
<li><p>There are four words embedded in this model of <span class="math inline">\(k=2\)</span> (two dimensions): ‚Äúcat‚Äù, ‚Äúdog‚Äù, ‚Äúwolf‚Äù, and ‚Äútiger‚Äù.</p></li>
<li><p>We can think of the semantic relationships between these words as geometric transformations and vector operations in this <span class="math inline">\(k=2\)</span> space.</p></li>
<li><p>The same vector allows us to move from ‚Äúcat‚Äù to ‚Äútiger‚Äù and from ‚Äúdog‚Äù to ‚Äúwolf‚Äù.</p></li>
<li><p>In that sense, we could try to interpret that vector as a dimension that tells us about the ‚Äúwild‚Äù or ‚Äúdomestic‚Äù nature of an animal.</p></li>
<li><p>In real-world applications of word embeddings, there are many significant geometric transformations, such as ‚Äúgender‚Äù vectors or vectors that tell us about the number (plural or singular) of a word.</p></li>
</ul>
<!-- -->
<ul>
<li>Although it was also used with traditional methods, the metric of <em>cosine similarity</em> gained popularity with embeddings as a measure of how close two words are in the embedding space: the <em>cosine similarity</em> is defined as the cosine of the angle between two vectors; that is, the dot product of the vectors divided by the product of their lengths:</li>
</ul>
<p><span class="math display">\[cos(A,B) = \frac{A \times B}{||A|| \times  ||B||}\]</span></p>
<ul>
<li><p>Varies between -1 and 1</p>
<ul>
<li>-1 means that the two vectors are opposite</li>
<li>+1 that the vectors are proportional, and</li>
<li>0 that the vectors are orthogonal.</li>
</ul></li>
</ul>
<!-- -->
<ul>
<li>In the <em>Discussion</em> section, we will see two papers <span class="citation" data-cites="kozlowski_2019">(<a href="#ref-kozlowski_2019" role="doc-biblioref">Kozlowski, Taddy, and Evans 2019</a>)</span>, <span class="citation" data-cites="garg_2018">(<a href="#ref-garg_2018" role="doc-biblioref">Garg et al. 2018</a>)</span> that make use of these types of transformations with sociologically relevant concepts (class, wealth, etc.).</li>
</ul>
<p>To conclude this general approach, <a href="http://jalammar.github.io/illustrated-word2vec/">let‚Äôs see a slightly more realistic example</a>:</p>
<ul>
<li>Let‚Äôs imagine we have the following vector representing the word ‚Äúking‚Äù. It‚Äôs a strip of 50 numbers.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/king-colored-embedding.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption><strong>Source:</strong> http://jalammar.github.io/illustrated-word2vec/</figcaption>
</figure>
</div>
<ul>
<li>On its own, it‚Äôs not very useful, so we can contrast it with other words in the embedding.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/king-man-woman-embedding.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption><strong>Source:</strong> http://jalammar.github.io/illustrated-word2vec/</figcaption>
</figure>
</div>
<ul>
<li>Intuitively, from the color-coded chart, we can see that the vectors for ‚Äúman‚Äù and ‚Äúwoman‚Äù appear to be more similar to each other.</li>
</ul>
<p>Let‚Äôs extend the list of words in the embedding a bit:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/queen-woman-girl-embeddings.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption><strong>Source:</strong> http://jalammar.github.io/illustrated-word2vec/</figcaption>
</figure>
</div>
<ul>
<li><p>‚ÄúWoman‚Äù and ‚Äúgirl‚Äù are similar to each other in several places in the vector; the same goes for ‚Äúman‚Äù and ‚Äúboy‚Äù</p></li>
<li><p>‚Äúboy‚Äù and ‚Äúgirl‚Äù also show some similarities in certain elements of the vector, which are different from ‚Äúman‚Äù or ‚Äúwoman‚Äù. The question here is whether these elements of the vector manage to somehow encode a sense of ‚Äúyouth‚Äù</p></li>
<li><p>the last word (‚Äúwater‚Äù) is the only one representing an object and not a person: it can be seen that the vector for this word is quite contrasting to the rest of the words in the figure</p></li>
<li><p>there are several places where ‚Äúking‚Äù and ‚Äúqueen‚Äù resemble each other and differ from all the others. What could they be representing?</p></li>
</ul>
<p>Earlier we talked about the possibility of word embeddings encoding some semantic dimensions.</p>
<ul>
<li><p>Let‚Äôs see the following example (quite classic) based on an analogy: what is to ‚Äúwoman‚Äù what ‚Äúking‚Äù is to ‚Äúman‚Äù? This is a typical analogy question: the answer is clear from semantics: ‚Äúqueen‚Äù.</p></li>
<li><p>We can pose this type of analogy problem based on vector operations in a word embedding</p></li>
<li><p>We can take the vector ‚Äúking‚Äù, subtract the vector ‚Äúman‚Äù from it, and add the vector ‚Äúwoman‚Äù to it. That gives us an ‚Äúartificial‚Äù vector.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/king-analogy-viz.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption><strong>Source:</strong> http://jalammar.github.io/illustrated-word2vec/</figcaption>
</figure>
</div>
<ul>
<li><p>Then, we can search among all the vectors in the embedding which one is the closest (using cosine similarity metric) to this generated vector.</p></li>
<li><p>It seems like magic, but the one that is closest is the vector for ‚Äúqueen‚Äù<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p></li>
</ul>
</section>
<section id="classical-word-embeddings-techniques" class="level1">
<h1>Classical word embeddings techniques</h1>
<p>So far, we‚Äôve spoken in very general terms about word embedding models. While we won‚Äôt delve into the specific technical details of each model (for that, you can refer to each model‚Äôs references), we will give a general overview of some of the first approaches.</p>
<section id="word2vec" class="level2">
<h2 class="anchored" data-anchor-id="word2vec">Word2Vec</h2>
<p>W2V is one of the first methods for word embeddings. It was developed by Google in 2013<span class="citation" data-cites="mikolov2013efficient">(<a href="#ref-mikolov2013efficient" role="doc-biblioref">Mikolov, Chen, et al. 2013</a>)</span> <span class="citation" data-cites="mikolov2013distributed">(<a href="#ref-mikolov2013distributed" role="doc-biblioref">Mikolov, Sutskever, et al. 2013</a>)</span>.</p>
<p>How does word2vec works?</p>
<ul>
<li><p>Each word in the vocabulary is represented as a one-hot encoded vector<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p></li>
<li><p>It uses a shallow neural network with either a continuous bag of words (CBOW) or skip-gram architecture.</p>
<ul>
<li>In CBOW, the model predicts the target word based on the context words within a window around the target word.</li>
<li>In skip-gram, the model predicts context words given a target word.</li>
</ul></li>
<li><p>Once the model is trained, the weights of the hidden layer (between the input and output layers) are used as word embeddings.</p></li>
</ul>
</section>
<section id="glove" class="level2">
<h2 class="anchored" data-anchor-id="glove">GloVe</h2>
<p>GloVe<span class="citation" data-cites="pennington2014glove">(<a href="#ref-pennington2014glove" role="doc-biblioref">Pennington, Socher, and Manning 2014</a>)</span> differs from Word2Vec in its approach to learning word embeddings.</p>
<ul>
<li>The main difference with word2vec is that it starts by constructing a co-occurrence matrix from the corpus.</li>
<li>This matrix captures how frequently words co-occur within a context window.</li>
<li>The context can be defined based on words within a fixed window size or using other contextual information like sentence boundaries.</li>
</ul>
</section>
<section id="fasttext" class="level2">
<h2 class="anchored" data-anchor-id="fasttext">FastText</h2>
<p>FastText <span class="citation" data-cites="bojanowski2017enriching">(<a href="#ref-bojanowski2017enriching" role="doc-biblioref">Bojanowski et al. 2017</a>)</span> <span class="citation" data-cites="joulin2016bag">(<a href="#ref-joulin2016bag" role="doc-biblioref">Joulin et al. 2016</a>)</span> is an extension of the Word2Vec model developed by Facebook AI Research. It shares similarities with Word2Vec but introduces several innovations, particularly in how it represents words and handles out-of-vocabulary words. In summary:</p>
<ul>
<li>FastText represents words as a bag of character n-grams (subword units). This allows FastText to capture morphological information and handle out-of-vocabulary words efficiently.</li>
<li>That is why it can generate embeddings for out-of-vocabulary words by summing the embeddings of their constituent character n-grams. This makes FastText more robust to rare and unseen words compared to traditional word embedding models.</li>
</ul>
</section>
</section>
<section id="how-to-access-word-embeddings" class="level1">
<h1>How to access word embeddings</h1>
<p>To work with word embedding models, there are two basic approaches.</p>
<p><strong>Train an embedding from scratch</strong> based on a specific task: if we were to train a classification model for literary works, we could generate a vector representation of the vocabulary from the corpus. Then, we could use that representation as input to construct features for the classification model.</p>
<ul>
<li>The <em>advantage</em> of this approach is that the embedding is likely to be a highly efficient representation of our corpus.</li>
<li>The <em>disadvantage</em> is that large corpora are required to get a good representations of words<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</li>
<li>Many times we don‚Äôt have a corpus that large.</li>
</ul>
<p>So, a very common option is to <strong>use a pre-trained model</strong>.</p>
<ul>
<li><p>This approach is based on the principle of <em>transfer learning</em>: the idea that it‚Äôs possible to apply a learned model to solve a similar or related task without having to train another model from scratch.</p></li>
<li><p>If the corpus on which a word embedding is trained is large and representative enough of a language, we can consider that model as a first general representation of the language, or at least of the semantics of that language.</p></li>
<li><p>If this is true, then the embedding managed to capture certain aspects of the word meanings that are general and not restricted to a specific corpus.</p></li>
<li><p>We can use a pre-trained embedding and apply it to a specific task for our specific corpus. In thisc ase, we use the embedding as input to generate the features of our classification model.</p></li>
<li><p>The difference is that we don‚Äôt train the embedding ourselves but rather use one that is already pre-trained.</p></li>
<li><p>Pre-trained word embeddings were a first milestone in the paradigm shift in NLP that we are currently experiencing. With the appearance of Large Language Models (LLMs), we stopped thinking about training a model for every task and every corpus we want to solve. Currently, it‚Äôs more common, for specific tasks such as hate speech detection <span class="citation" data-cites="perez_2023">(<a href="#ref-perez_2023" role="doc-biblioref">P√©rez et al. 2023</a>)</span>, to use some pre-trained model and perform a fine-tuning process. <strong>Fine-tuning</strong> means taking the parameters of a pre-trained model and modifying only some of its parameters to adapt it to a new domain.</p></li>
</ul>
</section>
<section id="word-embeddings-document-embeddings-and-llms" class="level1">
<h1>Word embeddings, document embeddings and LLM‚Äôs</h1>
<ul>
<li><p>Another extremely useful feature of word embeddings is that it‚Äôs possible to generate embeddings of documents, sentences, or paragraphs.</p></li>
<li><p>In fact, we can create an embedding of a given document by obtaining all the vectors of each of the words that compose it and adding them along each of their dimensions. We will see how to implement this in the guided practice.</p></li>
<li><p><strong>Transformer</strong> models and LLMs use different forms of embeddings in some of their layers. While it exceeds the scope of this class, it‚Äôs important to mention that the embeddings we work with here are ‚Äústatic‚Äù, meaning there is only one embedding per word. An innovation of transformer models is the introduction of of dynamic embeddings.</p></li>
<li><p>For instance,</p>
<pre><code>"I sat on the bank of the river" and 
"I deposited money in the bank" </code></pre></li>
<li><p>Both sentences use the term ‚Äúbank‚Äù in two different senses (a seat or a financial institution).</p></li>
<li><p>Models like word2vec or GloVe or FastText generate the same embedding for both sentences, so they would not be able to capture these two different meanings.</p></li>
<li><p>Contextual embeddings solve this problem by generating different vectors depending on the context in which the word appears.</p></li>
<li><p>ELMO <span class="citation" data-cites="peters_elmo_2019">(<a href="#ref-peters_elmo_2019" role="doc-biblioref">Peters et al. 2018</a>)</span> was one of the first models to use this form of contextualized embeddings.</p></li>
<li><p>This, along with other innovations such as self-attention mechanisms <span class="citation" data-cites="vaswani2023attention">(<a href="#ref-vaswani2023attention" role="doc-biblioref">Vaswani et al. 2023</a>)</span>, opened the possibility of training larger and more general language models (LLMs).</p></li>
</ul>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<ul>
<li>The utility of these models from a computer science perspective is clear. Indeed, word embeddings marked a qualitative leap in the evolution of NLP. The application of transfer learning techniques to this domain resulted in improvements in many tasks such as automatic translation, text classification, etc.</li>
<li>A big warning that needs to be made on these models (and language models in general) is that they tend to <strong>implicitly encode cultural biases</strong>. The corpora use to train this models is not exempt from the bias of their times. The model will learn the implicit associations between words. This can be dangerous when the models are used for sensitive tasks, like text classification and automatic translation. <span class="citation" data-cites="bender_2021">(<a href="#ref-bender_2021" role="doc-biblioref">Bender et al. 2021</a>)</span> develops a systematic exposition of these problems.</li>
<li>However, for social sciences, this feature opens an extremely interesting possibility: that of analyzing and quantifying such cultural biases and stereotypes. By analyzing a large and representative corpus of a language or a specific society, word embeddings can be used to identify how <em>certain forms of inequality</em> are expressed and reproduced in language.</li>
</ul>
<p>Here, we will present two examples in which social scientists used this characteristic of language models as a tool to study biases.</p>
<p><strong>First example</strong></p>
<p><span class="citation" data-cites="kozlowski_2019">(<a href="#ref-kozlowski_2019" role="doc-biblioref">Kozlowski, Taddy, and Evans 2019</a>)</span> uses word2vec and other models based on various corpora<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, to generate word embeddings over the period 1900-2000. - They construct a series of independent dimensions (class, wealth, gender, status, etc.) and compare them with different cultural dimensions. The following figure illustrates this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/kozlowski_embedding2.jpeg" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption><strong>Source:</strong> <span class="citation" data-cites="kozlowski_2019">(<a href="#ref-kozlowski_2019" role="doc-biblioref">Kozlowski, Taddy, and Evans 2019</a>)</span></figcaption>
</figure>
</div>
<ul>
<li>The first panel (A) shows the construction of the ‚Äúaffluence‚Äù dimension. It is constructed by calculating the difference between vectors of word pairs: <span class="math inline">\(rich - poor\)</span>, <span class="math inline">\(priceless-worthless\)</span>, etc. These vectors are then averaged, which results in the ‚Äúaffluence‚Äù dimension.</li>
<li>Next, the cosine similarity is calculated between this ‚Äúaffluence‚Äù dimension and different terms related, for example, to cultural goods.</li>
<li>This allows them to ‚Äúproject‚Äù them onto the ‚Äúaffluence‚Äù dimension. Panel B shows the position of different sports.</li>
<li>Thus, ‚Äúcamping‚Äù and ‚Äúboxing‚Äù appear strongly associated with the poverty pole, while ‚Äúgolf‚Äù and ‚Äútennis‚Äù are associated with the rich pole of the ‚Äúaffluence‚Äù dimension.</li>
</ul>
<hr>
<p><strong>Discuss</strong> Panel C shows the position of sports in two dimensions: ‚Äúaffluence‚Äù and ‚Äúgender‚Äù. What can you observe in this figure?</p>
<hr>
<ul>
<li>They also perform different comparisons with other cultural dimensions over time and several validations with external sources (such as a survey of cultural consumption).</li>
</ul>
<p><strong>Second example</strong></p>
<p><span class="citation" data-cites="garg_2018">(<a href="#ref-garg_2018" role="doc-biblioref">Garg et al. 2018</a>)</span> aims to show how word embeddings can illustrate the historical evolution of gender and ethnic stereotypes.</p>
<ul>
<li>They trained embeddings using word2vec and GloVe from different datasets: for the present analysis, the Google News dataset, and for historical analysis, they use models trained previously on Google Books/Corpus of Historical American English (COHA).</li>
<li>They find clear correlation between ethnic and gender biases, in one hand, and occupation participation rates, in the other.</li>
<li>They show how adjective associations in the embeddings offer insights into the evolving perceptions of various demographic groups.</li>
</ul>
<hr>
</section>
<section id="references" class="level1">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bender_2021" class="csl-entry" role="listitem">
Bender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. <span>‚ÄúOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú.‚Äù</span> In <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 610‚Äì23. FAccT ‚Äô21. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a>.
</div>
<div id="ref-bojanowski2017enriching" class="csl-entry" role="listitem">
Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. <span>‚ÄúEnriching Word Vectors with Subword Information.‚Äù</span> <a href="https://arxiv.org/abs/1607.04606">https://arxiv.org/abs/1607.04606</a>.
</div>
<div id="ref-chollet_2017" class="csl-entry" role="listitem">
Chollet, Fran√ßois, and J. J. Allaire. 2017. <em>Deep Learning with r</em>. USA: Manning Publisher.
</div>
<div id="ref-garg_2018" class="csl-entry" role="listitem">
Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018. <span>‚ÄúWord Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes.‚Äù</span> <em>Proceedings of the National Academy of Sciences</em> 115 (16): E3635‚Äì44. <a href="https://doi.org/10.1073/pnas.1720347115">https://doi.org/10.1073/pnas.1720347115</a>.
</div>
<div id="ref-joulin2016bag" class="csl-entry" role="listitem">
Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. <span>‚ÄúBag of Tricks for Efficient Text Classification.‚Äù</span> <a href="https://arxiv.org/abs/1607.01759">https://arxiv.org/abs/1607.01759</a>.
</div>
<div id="ref-kozlowski_2019" class="csl-entry" role="listitem">
Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. <span>‚ÄúThe Geometry of Culture: Analyzing the Meanings of Class Through Word Embeddings.‚Äù</span> <em>American Sociological Review</em> 84 (5): 905‚Äì49. <a href="https://doi.org/10.1177/0003122419877135">https://doi.org/10.1177/0003122419877135</a>.
</div>
<div id="ref-mikolov2013efficient" class="csl-entry" role="listitem">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>‚ÄúEfficient Estimation of Word Representations in Vector Space.‚Äù</span> <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>.
</div>
<div id="ref-mikolov2013distributed" class="csl-entry" role="listitem">
Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>‚ÄúDistributed Representations of Words and Phrases and Their Compositionality.‚Äù</span> <a href="https://arxiv.org/abs/1310.4546">https://arxiv.org/abs/1310.4546</a>.
</div>
<div id="ref-pennington2014glove" class="csl-entry" role="listitem">
Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. 2014. <span>‚ÄúGloVe: Global Vectors for Word Representation.‚Äù</span> In <em>Empirical Methods in Natural Language Processing (EMNLP)</em>, 1532‚Äì43. <a href="http://www.aclweb.org/anthology/D14-1162">http://www.aclweb.org/anthology/D14-1162</a>.
</div>
<div id="ref-perez_2023" class="csl-entry" role="listitem">
P√©rez, Juan Manuel, Franco M. Luque, Demian Zayat, Mart√≠n Kondratzky, Agust√≠n Moro, Pablo Santiago Serrati, Joaqu√≠n Zajac, et al. 2023. <span>‚ÄúAssessing the Impact of Contextual Information in Hate Speech Detection.‚Äù</span> <em>IEEE Access</em> 11: 30575‚Äì90. <a href="https://doi.org/10.1109/ACCESS.2023.3258973">https://doi.org/10.1109/ACCESS.2023.3258973</a>.
</div>
<div id="ref-peters_elmo_2019" class="csl-entry" role="listitem">
Peters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. <span>‚ÄúDeep Contextualized Word Representations.‚Äù</span> In <em>Proceedings of the 2018 Conference of the North <span>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>, edited by Marilyn Walker, Heng Ji, and Amanda Stent, 2227‚Äì37. New Orleans, Louisiana: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N18-1202">https://doi.org/10.18653/v1/N18-1202</a>.
</div>
<div id="ref-vaswani2023attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>‚ÄúAttention Is All You Need.‚Äù</span> <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Interestingly, the assumption of independence was once a big jump forward for computational linguistics, and allows for the implementation of models such as Naive Bayes. Lifting this assumption with word embeddings is what more recently allowed for another big step in the field.<br>
<a href="#fnref1" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p>These types of analogy tasks are often used to evaluate the performance of word embeddings. There is a series of relatively common questions used to calculate the accuracy of each embedding model. Thus, for example, questions like ‚ÄúWhat is to Greece what Paris is to France?‚Äù are used to compare the embedding‚Äôs ‚Äúresponse‚Äù to the real response. <span class="citation" data-cites="mikolov2013efficient">(<a href="#ref-mikolov2013efficient" role="doc-biblioref">Mikolov, Chen, et al. 2013</a>)</span> used about 8,869 semantic questions and about 10,675 syntactic questions.<a href="#fnref2" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn3"><p>This means that each word is represented as a vector where all elements are zero except for one element which represents the index of that word in the vocabulary<a href="#fnref3" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn4"><p>For example, in <span class="citation" data-cites="pennington2014glove">(<a href="#ref-pennington2014glove" role="doc-biblioref">Pennington, Socher, and Manning 2014</a>)</span>, the original paper of <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a>, one of the most commonly used embedding models, different trial models were trained on the following corpora: 2010 Wikipedia dump (1 billion tokens), 2014 Wikipedia dump (1.6 billion tokens), Gigaword 5 (4.3 billion tokens). Then, they combine Gigaword 5 and Wikipedia 2014. This often entails relatively long training times.<a href="#fnref4" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn5"><p>They use the Google NGram dataset, containing around a million books, and Google News, which contains a large number of news articles<a href="#fnref5" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>